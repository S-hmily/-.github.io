<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="1.前言
众所周知，我们生活中遇到的很多问题都是非线性的问题，例如预测花的生长情况。那么如果我们遇到了一些非线性问题，我们想要对这个非线性问题进行预测估计的时候我们需要怎么办呢？
这里就需要引入神经网络的概念了。
2.神经网络的概述
我们知..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://s-hmily.github.io/media/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://s-hmily.github.io/styles/main.css">
    
    <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/styles/default.min.css">
              
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/highlight.min.js"></script>
    <script src="https://s-hmily.github.io/media/js/clipboard.min.js"></script>
    <link rel="stylesheet" href="https://s-hmily.github.io/media/css/live2d.css">
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>
    <!-- 最新版本的 Bootstrap 核心 CSS 文件 -->
    <link href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.13.1/css/all.min.css" rel="stylesheet">
    <!-- 数学公式 -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"
        integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js"
        integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm" crossorigin="anonymous">
    </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js"
        integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH" crossorigin="anonymous">
    </script>
    <script>
        renderMathInElement(document.body, {
            delimiters: [{
                    left: "$$",
                    right: "$$",
                    display: true
                },
                {
                    left: "$",
                    right: "$",
                    display: false
                }
            ]
        });
    </script>

    
    <title>叮叮当</title>
    
    <style>
        .markdownIt-TOC {
            padding-left: 2px;
            width: 100%;
        }
        .markdownIt-TOC li{
            padding-left: 2%;
        }
    </style>
    
</head>

<body>
    <!-- 响应式布局，针对PC端内容显示 -->
    <div id="content">
        <div class="nav-large">
            <div class="row">
                <div class="side"><head>
    <meta name="description" content="“你买的什么书？”
“《边城》”
“C++还是python？”
“沈从文”" />
    <link rel="stylesheet" href="https://s-hmily.github.io/media/css/bootstrap.min.css">
</head>


<body>
    



    
    <nav class="navbar navbar-inverse navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <a class="navbar-brand" href="https://s-hmily.github.io"
                    style="font-size:21px">叮叮当&nbsp;&nbsp;|&nbsp;&nbsp;</a>
                <a class="navbar-brand" href=""
                    style="font-size:15px;font-family:kaiti">“你买的什么书？”
“《边城》”
“C++还是python？”
“沈从文”</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse">
                
                <div class="search nav navbar-nav" style="margin-top:8px">
                    <!-- <input type="text" class="search-input" placeholder="标题搜索(●'◡'●)" /> -->
                    <input type="text" class="search-input" placeholder="标题搜索 ⚆_⚆ つ♡">
                    <div class="search-results"></div>
                </div>
                
                <div class="search nav navbar-nav">
                <a title="text" onclick="document.getElementById('socialMenu').style.display='block'"><i><img class="social"
                    src="https://s-hmily.github.io/media/images/social.png" alt=""></i></a>
            </div>
            <ul class="nav navbar-nav" style="float: right;margin-right:5%">
                
                
                <li>
                    <a href="https://s-hmily.github.io" style="color:white">
                        首页
                    </a>
                </li>
                
                
                
                <li>
                    <a href="/archives" style="color:white">
                        归档
                    </a>
                </li>
                
                
                
                <li>
                    <a href="https://s-hmily.github.io/tags" style="color:white">
                        标签
                    </a>
                </li>
                
                
                
                <li><a href="https://s-hmily.github.io/talk" style="color:white;">说说</a></li>
                
                  
                <li><a href="https://s-hmily.github.io/friends" style="color:white">友链</a></li>
                
                  <li><img src="https://s-hmily.github.io/images/avatar.png?v=1595295888925" alt=""
                class="menutopavatar"></li>
            </ul>
        </div><!-- /.navbar-collapse -->
        </div><!-- /.container-fluid -->
    </nav>
    <div id="socialMenu" class="modal">
        <div class="animate">
            <div class="socialContainer">
                
                
                <a onclick="showqq()" style="cursor:pointer"><i><img class="icon" src="https://s-hmily.github.io/media/images/QQ.png"
                            alt=""></i></a>
                
                
                
                
                <a href="LB180928" target="_blank"><i><img class="icon"
                            src="https://s-hmily.github.io/media/images/wechat.png" alt=""></i></a>
                
                
            </div>
            <div id="qq" style="display:none">897438019</div>
        </div>
    </div>
    <!-- 引入jQuery核心js文件 -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>
    <script>
        var social = document.getElementById('socialMenu');
        // 鼠标点击模型外区域关闭登录框
        window.onclick = function (event) {
            if (event.target == social) {
                social.style.display = "none";
            }
        }
    </script>
    
</body>
<script>
    //-------------------------------------------------搜索
    // 获取搜索框、搜索按钮、清空搜索、结果输出对应的元素
    var searchInput = document.querySelector('.search-input');
    var searchResults = document.querySelector('.search-results');

    // 申明保存文章的标题、链接、内容的数组变量
    var searchValue = '',
        arrItems = [],
        arrLinks = [],
        arrTitles = [],
        arrResults = [],
        indexItem = [],
        itemLength = 0;
    var tmpDiv = document.createElement('div');
    tmpDiv.className = 'result-item';

    // ajax 的兼容写法
    var xhr = new XMLHttpRequest() || new ActiveXObject('Microsoft.XMLHTTP');
    xhr.onreadystatechange = function () {
        if (xhr.readyState == 4 && xhr.status == 200) {
            xml = xhr.responseXML;
            arrItems = xml.getElementsByTagName('entry');
            itemLength = arrItems.length;
            // 遍历并保存所有文章对应的标题、链接、内容到对应的数组中
            // 同时过滤掉 HTML 标签
            for (i = 0; i < itemLength; i++) {
                var link = arrItems[i].getElementsByTagName('link')[0];
                arrLinks[i] = link.getAttribute("href");
                arrTitles[i] = arrItems[i].getElementsByTagName('title')[0].
                childNodes[0].nodeValue.replace(/<.*?>/g, '');
            }
        }
    }

    // 开始获取根目录下 feed.xml 文件内的数据
    xhr.open('get', '/atom.xml', true);
    xhr.send();



    // 输入框内容变化后就开始匹配，可以不用点按钮
    // 经测试，onkeydown, onchange 等方法效果不太理想，
    // 存在输入延迟等问题，最后发现触发 input 事件最理想，
    // 并且可以处理中文输入法拼写的变化
    searchInput.oninput = function () {
        setTimeout(searchConfirm, 0);
    }
    searchInput.onfocus = function () {
        searchResults.style.display = 'block';
    }

    function searchConfirm() {
        if (searchInput.value == '') {
            searchResults.style.display = 'none';
        } else if (searchInput.value.search(/^\s+$/) >= 0) {
            // 检测输入值全是空白的情况
            searchInit();
            var itemDiv = tmpDiv.cloneNode(true);
            itemDiv.innerText = '请输入有效内容...';
            searchResults.appendChild(itemDiv);
        } else {
            // 合法输入值的情况
            searchInit();
            searchValue = searchInput.value;
            // 在标题、内容中查找
            searchMatching(arrTitles, searchValue);
        }
    }

    // 每次搜索完成后的初始化
    function searchInit() {
        arrResults = [];
        indexItem = [];
        searchResults.innerHTML = '';
        searchResults.style.display = 'block';
    }

    function searchMatching(arr1, input) {
        // 忽略输入大小写
        input = new RegExp(input, 'i');
        // 在所有文章标题、内容中匹配查询值
        for (i = 0; i < itemLength; i++) {
            if (arr1[i].search(input) !== -1) {
                var arr = arr1;
                indexItem.push(i); // 保存匹配值的索引
                var indexContent = arr[i].search(input);
                // 此时 input 为 RegExp 格式 /input/i，转换为原 input 字符串长度
                var l = input.toString().length - 3;
                var step = 10;

                // 将匹配到内容的地方进行黄色标记，并包括周围一定数量的文本
                arrResults.push(arr[i].slice(indexContent - step, indexContent));
            }
        }

        // 输出总共匹配到的数目
        var totalDiv = tmpDiv.cloneNode(true);
        totalDiv.innerHTML = '<b>总匹配：' + indexItem.length + ' 项<hr></b>';
        searchResults.appendChild(totalDiv);

        // 未匹配到内容的情况
        if (indexItem.length == 0) {
            var itemDiv = tmpDiv.cloneNode(true);
            itemDiv.innerText = '未匹配到内容...';
            searchResults.appendChild(itemDiv);
        }

        // 将所有匹配内容进行组合
        for (i = 0; i < arrResults.length; i++) {
            var itemDiv = tmpDiv.cloneNode(true);
            itemDiv.innerHTML = '<b>[' + arrTitles[indexItem[i]] +
                ']</b><p>' + arrResults[i] + "</p><hr />";
            itemDiv.setAttribute('onclick', 'changeHref(arrLinks[indexItem[' + i + ']])');
            searchResults.appendChild(itemDiv);
        }
    }

    function changeHref(href) {
        location.href = href;
    }

    function showqq() {
        var qq = document.getElementById("qq").innerHTML;
        if (qq != '')
            alert("博主的QQ联系方式为：" + qq);
        else
            alert("博主暂未设置QQ联系方式");
    }
</script></div>
    
    <div id="main" class="col-xs-12 col-sm-7" style="width:50%;margin-top:50px;left:27%">
        <link rel="stylesheet" href="https://s-hmily.github.io/media/css/font-awesome.css">
<style>

</style>

<body>
    <div class="allcontent" id="postdetail">
        <div class="postshow">
            
            <div class="postdetailimg" style="width:100%;overflow: hidden;display: none">
                
                <img src="https://pic2.zhimg.com/80/v2-bcbb1a4f932ab78c198b0a99af266d4e_720w.jpg?source=1940ef5c" class="postimage">
                
            </div>
            <div class="postinfo-detail">
                <div class="postdate"><i class="fa fa-calendar"></i>2020-07-21</div>
            <div class="poststatus postdate"><i class="fa fa-clock-o"></i> 33 min read</div>
            <div class="posttag">
                
            </div>
        </div>
        
        <div id="texttitle" style="text-align: center">
            <h2>浅层神经网络的解析及其应用</h2>
            <!-- id 将作为查询条件 -->
            <div id="pl" style="display:none">https://s-hmily.github.io/post/qian-ceng-shen-jing-wang-luo-de-jie-xi-ji-qi-ying-yong/</div>
            <div id="rootaddr" style="display:none">https://s-hmily.github.io</div>
            <span id="hotnum" class="leancloud_visitors" data-flag-title="浅层神经网络的解析及其应用">
                <h4 class="readercount">热度🔥: <i class="leancloud-visitors-count">loading...</i></h4>
            </span>
        </div>
        <div class="text ">
            <h2 id="1前言">1.前言</h2>
<p>众所周知，我们生活中遇到的很多问题都是非线性的问题，例如预测花的生长情况。那么如果我们遇到了一些非线性问题，我们想要对这个非线性问题进行预测估计的时候我们需要怎么办呢？</p>
<p>这里就需要引入神经网络的概念了。</p>
<h2 id="2神经网络的概述">2.神经网络的概述</h2>
<p>我们知道，我们日常生活中很多问题，甚至说大多数问题都不是线性可分问题，那我们要解决非线性可分问题该怎样处理呢？这就是这部分我们要引出的“多层”的概念。既然单层感知机解决不了非线性问题，那我们就采用多层感知机，下图就是一个两层感知机解决异或问题的示意图：</p>
<figure data-type="image" tabindex="1"><img src="https://images2015.cnblogs.com/blog/764050/201606/764050-20160619152400976-1951202651.gif" alt="img" loading="lazy"></figure>
<p>构建好上述网络以后，通过训练得到最后的分类面如下：</p>
<figure data-type="image" tabindex="2"><img src="https://images2015.cnblogs.com/blog/764050/201606/764050-20160619152721601-526579103.png" alt="img" loading="lazy"></figure>
<p>由此可见，多层感知机可以很好的解决非线性可分问题，我们通常将多层感知机这样的多层结构称之为是神经网络。但是，多层感知机虽然可以在理论上可以解决非线性问题，但是实际生活中问题的复杂性要远不止异或问题这么简单，所以我们往往要构建多层网络，而对于多层神经网络采用什么样的学习算法又是一项巨大的挑战，如下图所示的具有4层隐含层的网络结构中至少有33个参数（不计偏置bias参数），我们应该如何去确定呢？</p>
<figure data-type="image" tabindex="3"><img src="https://images2015.cnblogs.com/blog/764050/201606/764050-20160619154842679-926221650.jpg" alt="img" loading="lazy"></figure>
<p>我们先看一个图：</p>
<figure data-type="image" tabindex="4"><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630140644406-409859737.png" alt="img" loading="lazy"></figure>
<p>这是典型的三层神经网络的基本组成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，我们现在手里有一堆数据{x1,x2,x3,...,xn},输出也是一堆数据{y1,y2,y3,...,yn},现在要他们在隐含层做某种变换，让你把数据灌进去后得到你期望的输出。如果你希望你的输出和原始输入一样，那么就是最常见的自编码模型（Auto-Encoder）。可能有人会问，为什么要输入输出都一样呢？有什么用啊？其实应用挺广的，在图像识别，文本分类等等都会用到，包括一些变种之类的。如果你的输出和原始输入不一样，那么就是很常见的人工神经网络了，相当于让原始数据通过一个映射来得到我们想要的输出数据，也就是我们今天要讲的话题。</p>
<h2 id="3神经网络的实现过程利用神经网络分类二维数据集">3.神经网络的实现过程（利用神经网络分类二维数据集）</h2>
<p><strong>提示</strong>：建立神经网络的一般方法：</p>
<p>1.定义神经网络结构（输入单元数，隐藏单元数等）.</p>
<p>2.初始化模型的参数</p>
<p>3.循环：</p>
<ul>
<li>
<p>实现前向传播</p>
</li>
<li>
<p>计算损失</p>
</li>
<li>
<p>后向传播以获得梯度</p>
</li>
<li>
<p>更新参数（梯度下降）</p>
</li>
<li>
<p>我们通常会构建辅助函数来计算第1-3步，然后将它们合并为<code>nn_model()</code>函数。一旦构建了<code>nn_model()</code>并学习了正确的参数，就可以对新数据进行预测。</p>
<h3 id="31安装包">3.1安装包</h3>
<p>让我们首先导入在作业过程中需要的所有软件包。</p>
<ul>
<li>
<p><a href="https://www.kesci.com/api/notebooks/5e85d6bf95b029002ca7e7e6/www.numpy.org">numpy</a>是Python科学计算的基本包。</p>
</li>
<li>
<p><a href="http://scikit-learn.org/stable/">sklearn</a>提供了用于数据挖掘和分析的简单有效的工具。</p>
</li>
<li>
<p><a href="http://matplotlib.org/">matplotlib</a> 是在Python中常用的绘制图形的库。</p>
</li>
<li>
<p>testCases提供了一些测试示例用以评估函数的正确性</p>
</li>
<li>
<p>planar_utils提供了此作业中使用的各种函数</p>
<pre><code># Package imports
import numpy as np
import matplotlib.pyplot as plt
from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

%matplotlib inline

np.random.seed(1) # set a seed so that the results are consistent
</code></pre>
</li>
</ul>
<h3 id="32-数据集">3.2 数据集</h3>
<pre><code>X, Y = load_planar_dataset() 
# Visualize the data:
plt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0,:].shape), s=40, cmap=plt.cm.Spectral)
</code></pre>
<p>其中X是花瓣的点，表示特征矩阵（X1,X2）这个X数据集可以在后边用X.shape得到这个X是2行400列的</p>
<p>Y数据集是1行400列的，表示标签(红色：0，蓝色：1)</p>
<p>获得numpy数组的shape维度代码</p>
<pre><code>### START CODE HERE ### (≈ 3 lines of code)
shape_X = X.shape
shape_Y = Y.shape

m = shape_X[1]  # training set size
### END CODE HERE ###

print ('The shape of X is: ' + str(shape_X))
print ('The shape of Y is: ' + str(shape_Y))
print ('I have m = %d training examples!' % (m))
</code></pre>
<p>结果：</p>
<p>The shape of X is: (2, 400)</p>
<p>The shape of Y is: (1, 400)</p>
<p>I have m = 400 training examples!</p>
<h3 id="33定义神经网络结构">3.3定义神经网络结构</h3>
<h4 id="331定义变量">3.3.1定义变量</h4>
<p><strong>目标</strong>：定义三个变量     n_x:输入层大小  n_h：隐藏层大小(这里设置成4)   n_y:输出层大小</p>
<p><strong>提示</strong>：使用shape函数来找到n_x和n_y。另外 将隐藏层大小硬编码为4</p>
<pre><code># GRADED FUNCTION: layer_sizes

def layer_sizes(X, Y):
    &quot;&quot;&quot;
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    &quot;&quot;&quot;
    ### START CODE HERE ### (≈ 3 lines of code)
    n_x = X.shape[0] # size of input layer
    n_h = 4
    n_y = Y.shape[0] # size of output layer
    ### END CODE HERE ###
    return (n_x, n_h, n_y)
</code></pre>
<p>​    导入一组数据进行测试：</p>
<pre><code>X_assess, Y_assess = layer_sizes_test_case()
(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)
print(&quot;The size of the input layer is: n_x = &quot; + str(n_x))
print(&quot;The size of the hidden layer is: n_h = &quot; + str(n_h))
print(&quot;The size of the output layer is: n_y = &quot; + str(n_y))
</code></pre>
<p>结果：</p>
<p>The size of the input layer is: n_x = 5</p>
<p>The size of the hidden layer is: n_h = 4</p>
<p>The size of the output layer is: n_y = 2</p>
</li>
</ul>
<p><strong>预期输出</strong>（仅用于评估刚刚编码的函数，并不代表实际网络大小）。 输入层的大小为：n_x = 5 隐藏层的大小为：n_h = 4 输出层的大小为：n_y = 2</p>
<p>对于我们这个整个题来讲，应该是n_x = 2 n_h = 4 n_y = 21</p>
<h4 id="332-初始化模型参数">3.3.2 初始化模型参数</h4>
<p><strong>目标</strong>：实现函数 <code>initialize_parameters()</code>。</p>
<p><strong>提示：</strong></p>
<ul>
<li>请确保参数大小正确。 如果需要，也可参考上面的神经网络图。</li>
<li>使用随机值初始化权重矩阵。      - 使用：<code>np.random.randn（a，b）* 0.01</code>随机初始化维度为（a，b）的矩阵。</li>
<li>将偏差向量初始化为零。      - 使用：<code>np.zeros((a,b))</code> 初始化维度为（a，b）零的矩阵。</li>
</ul>
<pre><code>#初始化模型的参数
def initialize_parameters(n_x,n_h,n_y):
    '''
    参数：
    n_x:输入层结点的数量
    n_h:隐藏层结点的数量
    n_y:输出层结点的数量
    返回：
    paraneters:包含参数的字典
    W1:权重矩阵，维度为(n_h,n_x)
    b1:偏向量，维度为(n_h,1)
    W2：权重矩阵，维度为(n_y,n_h)
    b2:偏向量，维度为(n_y,1)
    '''
    #通过本函数可以返回一个或一组服从“0~1”均匀分布的随机样本值。随机样本取值范围是[0,1)，不包括1。
    np.random.seed(2)#指定一个随机种子
    W1=np.random.rand(n_h,n_x)*0.01
    #zeros 生成0矩阵
    b1=np.zeros(shape=(n_h,1))
    W2=np.random.rand(n_y,n_h)
    b2=np.random.rand(n_y,1)
    #使用断言确保我的数据格式是正确的
    #assert是用于对程序进行调试的，对于执行结构的判断，而不是对于业务流程的判断。（相当于一个if ()语句，如果满足断言的执行程序，如果不满足则抛错误
    assert(W1.shape==(n_h,n_x))
    assert(b1.shape==(n_h,1))
    assert(W2.shape==(n_y,n_h))
    assert(b2.shape==(n_y,1))
    parameters={
        &quot;W1&quot;:W1,
        &quot;b1&quot;:b1,
        &quot;W2&quot;:W2,
        &quot;b2&quot;:b2
    }
    return parameters
</code></pre>
<p>测试代码：</p>
<pre><code>n_x, n_h, n_y = initialize_parameters_test_case()

parameters = initialize_parameters(n_x, n_h, n_y)
print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>结果：W1 = [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]
b2 = [[0.]]
</code></pre>
<p><strong>预期输出</strong>: W1 = [[-0.00416758 -0.00056267][-0.02136196 0.01640271] [-0.01793436 -0.00841747][ 0.00502881 -0.01245288]] b1 = [[0.][0.] [0.][0.]] W2 = [[-0.01057952 -0.00909008 0.00551454 0.02292208]] b2 = [[0.]]</p>
<h4 id="333-循环">3.3.3 循环</h4>
<p><strong>目标</strong>：实现<code>forward_propagation（）</code>（实现正向传播）。</p>
<p>提示：</p>
<ul>
<li>
<p>在上方查看分类器的数学表示形式。</p>
</li>
<li>
<p>你可以使用内置在笔记本中的<code>sigmoid()</code>函数。</p>
</li>
<li>
<p>你也可以使用numpy库中的<code>np.tanh（）</code>函数。</p>
</li>
<li>
<p>必须执行以下步骤：      1.使用<code>parameters [“ ..”]</code>从字典“ parameters”（这是<code>initialize_parameters（）</code>的输出）中检索出每个参数。      2.实现正向传播，计算Z[1],A[1],Z[2] 和 A[2] （所有训练数据的预测结果向量）。</p>
</li>
<li>
<p>向后传播所需的值存储在<code>cache</code>中， <code>cache</code>将作为反向传播函数的输入。</p>
<p>代码：</p>
<pre><code># GRADED FUNCTION: forward_propagation

def forward_propagation(X, parameters):
    &quot;&quot;&quot;
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)
    
    Returns:
    A2 -- The sigmoid output of the second activation
    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;
    &quot;&quot;&quot;
    # Retrieve each parameter from the dictionary &quot;parameters&quot;
    ### START CODE HERE ### (≈ 4 lines of code)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Implement Forward Propagation to calculate A2 (probabilities)
    ### START CODE HERE ### (≈ 4 lines of code)
    Z1 = np.dot(W1,X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2,A1) + b2
    A2 = sigmoid(Z2)
    ### END CODE HERE ###
    
    assert(A2.shape == (1, X.shape[1]))
    
    cache = {&quot;Z1&quot;: Z1,
             &quot;A1&quot;: A1,
             &quot;Z2&quot;: Z2,
             &quot;A2&quot;: A2}
    
    return A2, cache
</code></pre>
</li>
</ul>
<p>测试代码：</p>
<pre><code>X_assess, parameters = forward_propagation_test_case()

A2, cache = forward_propagation(X_assess, parameters)

# Note: we use the mean here just to make sure that your output matches ours. 
print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
</code></pre>
<pre><code>-0.0004997557777419913 -0.000496963353231779 0.00043818745095914653 0.500109546852431
</code></pre>
<p><strong>预期输出</strong>: -0.0004997557777419913  -0.000496963353231779 0.00043818745095914653        0.500109546852431</p>
<p>现在，你已经计算了包含每个示例的a<a href="i">2</a> 的 A[2] （在Python变量“<code>A2</code>”中），其中，你可以计算损失函数 如下：<img src="C:%5CUsers%5CWGJS%5CAppData%5CLocal%5CTemp%5C1595039574626.png" alt="1595039574626" loading="lazy"></p>
<p><strong>目的</strong>：实现<code>compute_cost（）</code>以计算损失J的值。</p>
<p>有很多种方法可以实现交叉熵损失。 我们为你提供了实现方法 ：<img src="C:%5CUsers%5CWGJS%5CAppData%5CLocal%5CTemp%5C1595039675666.png" alt="1595039675666" loading="lazy"></p>
<pre><code>logprobs = np.multiply(np.log(A2),Y)
cost = - np.sum(logprobs)   
</code></pre>
<p>（你也可以使用np.multiply()然后使用np.sum()或直接使用np.dot()）。</p>
<pre><code># GRADED FUNCTION: compute_cost

def compute_cost(A2, Y, parameters):
    &quot;&quot;&quot;
    Computes the cross-entropy cost given in equation (13)
    
    Arguments:
    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    Y -- &quot;true&quot; labels vector of shape (1, number of examples)
    parameters -- python dictionary containing your parameters W1, b1, W2 and b2
    
    Returns:
    cost -- cross-entropy cost given equation (13)
    &quot;&quot;&quot;
    
    m = Y.shape[1] # number of example

    # Compute the cross-entropy cost
     ### START CODE HERE ### (≈ 2 lines of code)
    logprobs = Y*np.log(A2) + (1-Y)* np.log(1-A2)
    cost = -1/m * np.sum(logprobs)
    ### END CODE HERE ###
    
    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. 
                                # E.g., turns [[17]] into 17 
    assert(isinstance(cost, float))
    
    return cost
</code></pre>
<p><strong>测试</strong>：</p>
<pre><code>A2, Y_assess, parameters = compute_cost_test_case()

print(&quot;cost = &quot; + str(compute_cost(A2, Y_assess, parameters)))
</code></pre>
<pre><code>cost = 0.6929198937761265
</code></pre>
<p>现在，通过使用在正向传播期间计算的缓存，你可以实现后向传播。</p>
<p><strong>问题</strong>：实现函数<code>backward_propagation（）</code>。</p>
<p><strong>说明</strong>： 反向传播通常是深度学习中最难（最数学）的部分。为了帮助你更好地了解，我们提供了反向传播课程的幻灯片。你将要使用此幻灯片右侧的六个方程式以构建向量化实现。</p>
<figure data-type="image" tabindex="5"><img src="https://cdn.kesci.com/upload/image/q17hcd4yra.png?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<figure data-type="image" tabindex="6"><img src="C:%5CUsers%5CWGJS%5CAppData%5CLocal%5CTemp%5C1595040132956.png" alt="1595040132956" loading="lazy"></figure>
<pre><code># GRADED FUNCTION: backward_propagation

def backward_propagation(parameters, cache, X, Y):
    &quot;&quot;&quot;
    Implement the backward propagation using the instructions above.
    
    Arguments:
    parameters -- python dictionary containing our parameters 
    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;.
    X -- input data of shape (2, number of examples)
    Y -- &quot;true&quot; labels vector of shape (1, number of examples)
    
    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    &quot;&quot;&quot;
    m = X.shape[1]
    
    # First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.
    ### START CODE HERE ### (≈ 2 lines of code)
    W1 = parameters[&quot;W1&quot;]
    W2 = parameters[&quot;W2&quot;]
    ### END CODE HERE ###
        
    # Retrieve also A1 and A2 from dictionary &quot;cache&quot;.
    ### START CODE HERE ### (≈ 2 lines of code)
    A1 = cache[&quot;A1&quot;]
    A2 = cache[&quot;A2&quot;]
    ### END CODE HERE ###
    
    # Backward propagation: calculate dW1, db1, dW2, db2. 
    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)
    dZ2= A2 - Y
    dW2 = 1 / m * np.dot(dZ2,A1.T)
    db2 = 1 / m * np.sum(dZ2,axis=1,keepdims=True)
    dZ1 = np.dot(W2.T,dZ2) * (1-np.power(A1,2))
    dW1 = 1 / m * np.dot(dZ1,X.T)
    db1 = 1 / m * np.sum(dZ1,axis=1,keepdims=True)
    ### END CODE HERE ###
    
    grads = {&quot;dW1&quot;: dW1,
             &quot;db1&quot;: db1,
             &quot;dW2&quot;: dW2,
             &quot;db2&quot;: db2}
    
    return grads
</code></pre>
<p>In [16]:</p>
<pre><code>parameters, cache, X_assess, Y_assess = backward_propagation_test_case()

grads = backward_propagation(parameters, cache, X_assess, Y_assess)
print (&quot;dW1 = &quot;+ str(grads[&quot;dW1&quot;]))
print (&quot;db1 = &quot;+ str(grads[&quot;db1&quot;]))
print (&quot;dW2 = &quot;+ str(grads[&quot;dW2&quot;]))
print (&quot;db2 = &quot;+ str(grads[&quot;db2&quot;]))
</code></pre>
<pre><code>dW1 = [[ 0.01018708 -0.00708701]
 [ 0.00873447 -0.0060768 ]
 [-0.00530847  0.00369379]
 [-0.02206365  0.01535126]]
db1 = [[-0.00069728]
 [-0.00060606]
 [ 0.000364  ]
 [ 0.00151207]]
dW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]
db2 = [[0.06589489]]
</code></pre>
<p><strong>预期输出</strong>: dW1 = [[ 0.01018708 -0.00708701][ 0.00873447 -0.0060768 ] [-0.00530847 0.00369379][-0.02206365 0.01535126]] db1 = [[-0.00069728][-0.00060606] [ 0.000364 ][ 0.00151207]] dW2 = [[ 0.00363613 0.03153604 0.01162914 -0.01318316]] db2 = [[0.06589489]]</p>
<p><strong>问题</strong>：实现参数更新。 使用梯度下降，你必须使用（dW1，db1，dW2，db2）才能更新（W1，b1，W2，b2）。</p>
<p><strong>一般的梯度下降规则</strong>：θ=θ−α∂J∂θ其中α是学习率，而θ 代表一个参数。</p>
<p><strong>图示</strong>：具有良好的学习速率（收敛）和较差的学习速率（发散）的梯度下降算法。 图片由Adam Harley提供。</p>
<figure data-type="image" tabindex="7"><img src="https://cdn.kesci.com/upload/image/q17hh4otzu.gif?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<figure data-type="image" tabindex="8"><img src="https://cdn.kesci.com/upload/image/q17hharbth.gif?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<pre><code># GRADED FUNCTION: update_parameters

def update_parameters(parameters, grads, learning_rate = 1.2):
    &quot;&quot;&quot;
    Updates parameters using the gradient descent update rule given above
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients 
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    &quot;&quot;&quot;
    # Retrieve each parameter from the dictionary &quot;parameters&quot;
    ### START CODE HERE ### (≈ 4 lines of code)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Retrieve each gradient from the dictionary &quot;grads&quot;
    ### START CODE HERE ### (≈ 4 lines of code)
    dW1 = grads[&quot;dW1&quot;]
    db1 = grads[&quot;db1&quot;]
    dW2 = grads[&quot;dW2&quot;]
    db2 = grads[&quot;db2&quot;]
    ## END CODE HERE ###
    
    # Update rule for each parameter
    ### START CODE HERE ### (≈ 4 lines of code)
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    ### END CODE HERE ###
    
    parameters = {&quot;W1&quot;: W1,
                  &quot;b1&quot;: b1,
                  &quot;W2&quot;: W2,
                  &quot;b2&quot;: b2}
    
    return parameters
</code></pre>
<p>测试：</p>
<pre><code>parameters, grads = update_parameters_test_case()
parameters = update_parameters(parameters, grads)

print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>结果：
W1 = [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]
b1 = [[-1.02420756e-06]
 [ 1.27373948e-05]
 [ 8.32996807e-07]
 [-3.20136836e-06]]
W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]
b2 = [[0.00010457]]
</code></pre>
<p><strong>预期输出</strong>: dW1 = [[ 0.01018708 -0.00708701][ 0.00873447 -0.0060768 ] [-0.00530847 0.00369379][-0.02206365 0.01535126]] db1 = [[-0.00069728][-0.00060606] [ 0.000364 ][ 0.00151207]] dW2 = [[ 0.00363613 0.03153604 0.01162914 -0.01318316]] db2 = [[0.06589489]]</p>
<h4 id="334-在nn_model中集成331-332和333部分中的函数">3.3.4- 在nn_model（）中集成3.3.1、3.3.2和3.3.3部分中的函数</h4>
<p><strong>问题</strong>：在nn_model（）中建立你的神经网络模型。</p>
<p><strong>说明</strong>：神经网络模型必须以正确的顺序组合先前构建的函数。</p>
<p>In [19]:</p>
<pre><code># GRADED FUNCTION: nn_model

def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    &quot;&quot;&quot;
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    &quot;&quot;&quot;
    
    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.
    ### START CODE HERE ### (≈ 5 lines of code)
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Loop (gradient descent)

    for i in range(0, num_iterations):
         
        ### START CODE HERE ### (≈ 4 lines of code)
        # Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.
        A2, cache = forward_propagation(X, parameters)
        
        # Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.
        cost = compute_cost(A2, Y, parameters)
 
        # Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.
        grads = backward_propagation(parameters, cache, X, Y)
 
        # Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.
        parameters = update_parameters(parameters, grads)
        
        ### END CODE HERE ###
        
        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))

    return parameters
</code></pre>
<p>测试</p>
<pre><code>X_assess, Y_assess = nn_model_test_case()

parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=False)
print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log
/home/kesci/work/planar_utils.py:34: RuntimeWarning: overflow encountered in exp
  s = 1/(1+np.exp(-x))
</code></pre>
<pre><code>结果：
W1 = [[-4.18503197  5.33214315]
 [-7.52988635  1.24306559]
 [-4.19302427  5.32627154]
 [ 7.52984762 -1.24308746]]
b1 = [[ 2.32926944]
 [ 3.79460252]
 [ 2.33002498]
 [-3.79466751]]
W2 = [[-6033.83668723 -6008.12983227 -6033.10091631  6008.06624417]]
b2 = [[-52.66610924]]
</code></pre>
<p><strong>预期输出</strong>: W1 = [[-4.18503197 5.33214315][-7.52988635 1.24306559] [-4.19302427 5.32627154][ 7.52984762 -1.24308746]] b1 = [[ 2.32926944][ 3.79460252] [ 2.33002498][-3.79466751]] W2 = [[-6033.83668723 -6008.12983227 -6033.10091631 6008.06624417]] b2 = [[-52.66610924]]</p>
<h3 id="45-预测">4.5- 预测</h3>
<p><strong>问题</strong>：使用你的模型通过构建predict()函数进行预测。 使用正向传播来预测结果。</p>
<p><strong>提示</strong>：predictions = yprediction=1{activation &gt; 0.5}={1if activation&gt;0.50otherwise<br>
例如，如果你想基于阈值将矩阵X设为0和1，则可以执行以下操作： <code>X_new = (X &gt; threshold)</code></p>
<pre><code># GRADED FUNCTION: predict

def predict(parameters, X):
    &quot;&quot;&quot;
    Using the learned parameters, predicts a class for each example in X
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    X -- input data of size (n_x, m)
    
    Returns
    predictions -- vector of predictions of our model (red: 0 / blue: 1)
    &quot;&quot;&quot;
    
    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.
  ### START CODE HERE ### (≈ 2 lines of code)
    A2, cache = forward_propagation(X, parameters)
    predictions = np.round(A2)
    ### END CODE HERE ###
    
    return predictions
</code></pre>
<p>测试：</p>
<pre><code>parameters, X_assess = predict_test_case()

predictions = predict(parameters, X_assess)
print(&quot;predictions mean = &quot; + str(np.mean(predictions)))
</code></pre>
<pre><code>predictions mean = 0.6666666666666666
</code></pre>
<p><strong>预期输出</strong>: predictions mean = 0.6666666666666666</p>
<p>现在运行模型以查看其如何在二维数据集上运行。 运行以下代码以使用含有nh隐藏单元的单个隐藏层测试模型。</p>
<pre><code># Build a model with a n_h-dimensional hidden layer
parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)

# Plot the decision boundary
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4))
</code></pre>
<pre><code>结果：
Cost after iteration 0: 0.693048
Cost after iteration 1000: 0.288083
Cost after iteration 2000: 0.254385
Cost after iteration 3000: 0.233864
Cost after iteration 4000: 0.226792
Cost after iteration 5000: 0.222644
Cost after iteration 6000: 0.219731
Cost after iteration 7000: 0.217504
Cost after iteration 8000: 0.219467
Cost after iteration 9000: 0.218561
</code></pre>
<pre><code>Text(0.5, 1.0, 'Decision Boundary for hidden layer size 4')

</code></pre>
<figure data-type="image" tabindex="9"><img src="https://cdn.kesci.com/rt_upload/CB068684F93C4A2A8AE816EB492CDCBE/q17hj9pr80.png" alt="img" loading="lazy"></figure>
<p><strong>预期输出</strong>: Cost after iteration 9000: 0.218561</p>
<pre><code># Print accuracy
predictions = predict(parameters, X)
print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')

</code></pre>
<pre><code>Accuracy: 90%

</code></pre>
<p><strong>预期输出</strong>: Accuracy: 90%</p>
<p>与Logistic回归相比，准确性确实更高。 该模型学习了flower的叶子图案！ 与逻辑回归不同，神经网络甚至能够学习非线性的决策边界。</p>
<p>现在，让我们尝试几种不同的隐藏层大小。</p>
<h2 id="4-调整隐藏层大小可选练习">4 调整隐藏层大小（可选练习）</h2>
<p>运行以下代码（可能需要1-2分钟）， 你将观察到不同大小隐藏层的模型的不同表现。</p>
<p>In [25]:</p>
<pre><code># This may take about 2 minutes to run

plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 10, 20]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print (&quot;Accuracy for {} hidden units: {} %&quot;.format(n_h, accuracy))

</code></pre>
<pre><code>Accuracy for 1 hidden units: 67.5 %
Accuracy for 2 hidden units: 67.25 %
Accuracy for 3 hidden units: 90.75 %
Accuracy for 4 hidden units: 90.5 %
Accuracy for 5 hidden units: 91.25 %
Accuracy for 10 hidden units: 90.25 %
Accuracy for 20 hidden units: 90.5 %

</code></pre>
<figure data-type="image" tabindex="10"><img src="https://cdn.kesci.com/rt_upload/F70D5A23097642688F6245327ACE9DD7/q17hkk98sl.png" alt="img" loading="lazy"></figure>
<p>完整代码</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary,sigmoid,load_planar_dataset,load_extra_datasets
#设置一个固定的随机种子
np.random.seed(1)
X,Y=load_planar_dataset()
#用matplotlib可视化数据
plt.scatter(X[0,:],X[1,:],c=np.squeeze(Y),s=40,cmap=plt.cm.Spectral)
plt.show()
shape_X=X.shape
shape_Y=Y.shape
m=Y.shape[1] #训练集里面的数据
print(&quot;X的维度为：&quot;+str(shape_X))
print(&quot;Y的维度为：&quot;+str(shape_Y))
print(&quot;数据集里面的数据有：&quot;+str(m)+&quot;个&quot;)
def layer_sizes(X,Y):
    '''参数：
    X：输入数据集，维度是（输入的数量，训练/测试的数量）
    Y：标签
    返回：
    n_x:输入层的数量
    n_h:隐藏层的数量
    n_y:输出层的数量'''
    n_x=X.shape[0]#输入层
    n_h=4#隐藏层，硬编码为4
    n_y=Y.shape[0]
    return (n_x,n_h,n_y)
#测试layer_sizes
print(&quot;=========================测试layer_sizes=========================&quot;)
X_asses , Y_asses = layer_sizes_test_case()
(n_x,n_h,n_y) =  layer_sizes(X_asses,Y_asses)
print(&quot;输入层的节点数量为: n_x = &quot; + str(n_x))
print(&quot;隐藏层的节点数量为: n_h = &quot; + str(n_h))
print(&quot;输出层的节点数量为: n_y = &quot; + str(n_y))
#初始化模型的参数
def initialize_parameters(n_x,n_h,n_y):
    '''
    参数：
    n_x:输入层结点的数量
    n_h:隐藏层结点的数量
    n_y:输出层结点的数量
    返回：
    paraneters:包含参数的字典
    W1:权重矩阵，维度为(n_h,n_x)
    b1:偏向量，维度为(n_h,1)
    W2：权重矩阵，维度为(n_y,n_h)
    b2:偏向量，维度为(n_y,1)
    '''
    #通过本函数可以返回一个或一组服从“0~1”均匀分布的随机样本值。随机样本取值范围是[0,1)，不包括1。
    np.random.seed(2)#指定一个随机种子
    W1=np.random.rand(n_h,n_x)*0.01
    #zeros 生成0矩阵
    b1=np.zeros(shape=(n_h,1))
    W2=np.random.rand(n_y,n_h)
    b2=np.random.rand(n_y,1)
    #使用断言确保我的数据格式是正确的
    #assert是用于对程序进行调试的，对于执行结构的判断，而不是对于业务流程的判断。（相当于一个if ()语句，如果满足断言的执行程序，如果不满足则抛错误
    assert(W1.shape==(n_h,n_x))
    assert(b1.shape==(n_h,1))
    assert(W2.shape==(n_y,n_h))
    assert(b2.shape==(n_y,1))
    parameters={
        &quot;W1&quot;:W1,
        &quot;b1&quot;:b1,
        &quot;W2&quot;:W2,
        &quot;b2&quot;:b2
    }
    return parameters
#测试initialize_parameters
n_x,n_h,n_y=initialize_parameters_test_case()
parameters=initialize_parameters(n_x,n_h,n_y)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters[&quot;b2&quot;]))
#实现循环
'''我们现在要实现前向传播函数forward_propagation()。 
我们可以使用sigmoid()函数，也可以使用np.tanh()函数。  
步骤如下：
使用字典类型的parameters（它是initialize_parameters() 的输出）检索每个参数。
实现向前传播, 计算Z[1],A[1],Z[2]Z[1],A[1],Z[2] 和 A[2]A[2]（ 训练集里面所有例子的预测向量）。
反向传播所需的值存储在“cache”中，cache将作为反向传播函数的输入。
'''
def forward_propagation(X,parameters):
    '''
    参数：
    X-维度为(n_x,m)的输入数据
    parameters:初始化函数(initialize_parameters)的输出
    返回:
    A2:使用激活函数后的激活值
    cache-包含“Z1&quot;,&quot;A1&quot;,&quot;Z2&quot;和“A2”的字典型变量
    '''
    W1=parameters[&quot;W1&quot;]
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    #向前传播计算A2
    #dot 计算矩阵
    Z1=np.dot(W1,X)+b1
    A1=np.tanh(Z1)
    Z2=np.dot(W2,A1)+b2
    A2=sigmoid(Z2)
    #使用断言保证我的数据格式是正确的
    assert(A2.shape==(1,X.shape[1]))
    cache={&quot;Z1&quot;:Z1,
           &quot;A1&quot;:A1,
           &quot;Z2&quot;:Z2,
           &quot;A2&quot;:A2
    }
    return (A2,cache)
#测试模型
X_assess,parameters=forward_propagation_test_case()
A2,cache=forward_propagation(X_assess,parameters)
print(np.mean(cache[&quot;Z1&quot;]),np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
#计算成本函数与损失函数
 
def compute_cost(A2,Y,parameters):
    '''计算交叉熵
    参数：
    A2：使用sigmoid函数计算的第二次激活后的函数值
    Y:True标签向量，维度为(1,数量)
    parameters:一个包含W1，W2，b1,b2的字典类型的变量
    返回：成本交叉熵函数，给出方程
    '''
    m=Y.shape[1]
    W1=parameters['W1']
    W2=parameters['W2']
    #计算成本
    logprobs=logprobs=np.multiply(np.log(A2),Y)+np.multiply((1-Y),np.log(1-A2))
    cost=-np.sum(logprobs)/m
    cost=float(np.squeeze(cost))
    assert(isinstance(cost,float))
    return cost
#预测compute_cost
A2,Y_assess,parameters=compute_cost_test_case()
print(&quot;cost=&quot;+str(compute_cost(A2,Y_assess,parameters)))
#搭建反向传播函数
def backward_propagation(parameters,cache,X,Y):
    '''参数：
    parameters:包含参数的一个字典变量
    cache:包含Z1,A1,Z2,A2的字典类型的变量
    X:输入数据，维度为(2,数量)
    Y:输出数据，唯独为(1,数量)
    返回:
    grads:包含W和b的导数的一个字典变量'''
    m=X.shape[1]
    W1=parameters['W1']
    W2=parameters['W2']
    A1=cache['A1']
    A2=cache['A2']
    dZ2=A2-Y
    dW2=(1/m)*np.dot(dZ2,A1.T)
    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)
    dZ1=np.multiply(np.dot(W2.T,dZ2),1-np.power(A1,2))
    dW1 = (1 / m) * np.dot(dZ1, X.T)
    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)
    grads={'dW1':dW1,
           'db1':db1,
           'dW2':dW2,
           'db2':db2}
    return grads
#更新参数
def update_parameters(parameters,grads,learning_rate=1.2):
    '''参数：
    parameters:包含参数的字典类型数据的变量
    grads:包含导数值得字典类型变量
    learning_rate:学习速率
    返回：
    parameters:包含更新参数的字典类型和变量'''
    W1,W2=parameters['W1'],parameters['W2']
    b1,b2=parameters['b1'],parameters['b2']
    dW1,dW2=grads['dW1'],grads['dW2']
    db1,db2=grads['db1'],grads['db2']
    W1=W1-learning_rate*dW1
    b1=b1-learning_rate*db1
    W2=W2-learning_rate*dW2
    b2=b2-learning_rate*db2
    parameters={&quot;W1&quot;:W1,
                &quot;b1&quot;:b1,
                &quot;W2&quot;:W2,
                &quot;b2&quot;:b2}
    return parameters
#测试update_parameters
parameters,grads=update_parameters_test_case()
parameters=update_parameters(parameters,grads)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters['b2']))
#把上述内容整合到nn_model()中，神经网络模型必须以正确的顺序使用先前的功能
def nn_model(X,Y,n_h,num_iterations,print_cost=False):
    '''参数：
    X-数据集，维度为(2,示例数)
    Y-标签,维度为（1，示例数）
    n_h:隐藏层的数量
    num_iterations:梯度下降循环中的迭代次数
    print_cost:如果为True,则每100次迭代打印一次成本数值
    返回:
    parameters:模型学习的参数，它们可以用来预测
    '''
    np.random.seed(3)#指定随机种子
    n_x=layer_sizes(X,Y)[0]
    n_y=layer_sizes(X,Y)[2]
    parameters=initialize_parameters(n_x,n_h,n_y)
    W1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    for i in range(num_iterations):
        A2,cache=forward_propagation(X,parameters)
        cost=compute_cost(A2,Y,parameters)
        grads=backward_propagation(parameters,cache,X,Y)
        parameters=update_parameters(parameters,grads,learning_rate=0.5)
        if print_cost:
            if i%1000==0:
                print(&quot;第&quot;,i,&quot;次循环&quot;,&quot;成本为:&quot;+str(cost))
    return parameters
#测试nn_model
X_assess,Y_assess=nn_model_test_case()
parameters=nn_model(X_assess,Y_assess,4,num_iterations=10000,print_cost=False)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters[&quot;b2&quot;]))
#predict模型
'''激活值大于0.5，预测值为1，否则为0'''
def predict(parameters,X):
    '''使用学习的参数为X分类
    参数：
    parameters:包含参数的字典类型的变量
    X:输入数据
    返回：
    predictions:我们模型预测的向量（红色：0/蓝色：1）
    '''
    A2,cache=forward_propagation(X,parameters)
    predictions=np.round(A2)
    return predictions
#测试predict
parameters,X_assess=predict_test_case()
predictions=predict(parameters,X_assess)
print(&quot;预测的平均值=&quot;+str(np.mean(predictions)))
parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)
 
#绘制边界
'''注：数组的可视化
通常算法的结果是可以表示向量的数组，直接利用数组画图时界面为空
利用sequeeze()函数转化为秩为1的数组可以正常画图'''
plot_decision_boundary(lambda x: predict(parameters, x.T), X, np.squeeze(Y))
plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4))
plt.show()
predictions = predict(parameters, X)
print ('准确率: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')
#更改隐藏结点的数量
plt.figure(figsize=(16,32))
hidden_layer_sizes=[1,2,3,4,5,20,50]#隐藏层数量
for i,n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5,2,i+1)
    plt.title('Hidden Layer of Size%d'% n_h)
    parameters=nn_model(X,Y,n_h,num_iterations=5000)
    plot_decision_boundary(lambda x:predict(parameters,x.T),X,np.squeeze(Y))
    predictions=predict(parameters,X)
    accuracy=float((np.dot(Y,predictions.T)+np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print(&quot;隐藏结点的数量：{}，准确率：{}%&quot;.format(n_h,accuracy))

</code></pre>
<p><strong>说明</strong>：</p>
<ul>
<li>
<p>较大的模型（具有更多隐藏的单元）能够更好地拟合训练集，直到最终最大的模型过拟合数据为止。</p>
</li>
<li>
<p>隐藏层的最佳大小似乎在n_h = 5左右。的确，此值似乎很好地拟合了数据，而又不会引起明显的过度拟合。</p>
</li>
<li>
<p>稍后你还将学习正则化，帮助构建更大的模型（例如n_h = 50）而不会过度拟合。</p>
<p>参考文章：</p>
<p><a href="https://www.kesci.com/home/project/5dd3946900b0b900365f3a48/code">文章一</a></p>
<p><a href="https://www.cnblogs.com/charlotte77/p/5629865.html">文章二</a></p>
<p><a href="https://www.cnblogs.com/maybe2030/p/5597716.html">文章三</a></p>
<p>数据包在文章一中 可以直接下载使用</p>
</li>
</ul>

        </div>
        
        
        <div class="next-post">
            下一篇
            <a href="https://s-hmily.github.io/post/juan-ji-shen-jing-wang-luo-jie-xi/">
                卷积神经网络解析
            </a>
        </div>
        
    </div>
    </div>
</body>
<script>
    var t_img; // 定时�?
    var isLoad = true; // 控制变量
    isImgLoad(function () {
        // 加载完成
        $('.postdetailimg').css("display", "block");
    });
    // 判断图片加载的函�?
    function isImgLoad(callback) {
        // 注意我的图片类名都是cover，因为我�?需要�?�理cover。其它图片可以不管�?
        // 查找所有封面图，迭代�?�理
        $('.postdetailimg').each(function () {
            // 找到�?0就将isLoad设为false，并退出each
            if (this.height === 0) {
                isLoad = false;
                return false;
            }
        });
        // 为true，没有发现为0的。加载完�?
        if (isLoad) {
            clearTimeout(t_img); // 清除定时�?
            // 回调函数
            callback();
            // 为false，因为找到了没有加载完成的图，将调用定时器递归
        } else {
            isLoad = true;
            t_img = setTimeout(function () {
                isImgLoad(callback); // 递归�?�?
            }, 500); // 我这里�?�置的是500�?秒就�?描一次，�?以自己调�?
        }
    }

    //文章阅读热度
    var pl = $("#pl").html();
    var rootaddr = $("#rootaddr").html();
    pl = pl.replace(rootaddr, "");
    $("#hotnum").attr('id', pl);
</script>
        <div name="comment" style="background: white">
            <div class="commentcontainer">
                
                <p>请到客户端“主题--自定义配置--valine”中填入ID和KEY</p>
                
            </div>
        </div>
    </div>
     
                <div class="toc-container">
                    <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#1%E5%89%8D%E8%A8%80">1.前言</a></li>
<li><a href="#2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A6%82%E8%BF%B0">2.神经网络的概述</a></li>
<li><a href="#3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B%E5%88%A9%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E7%B1%BB%E4%BA%8C%E7%BB%B4%E6%95%B0%E6%8D%AE%E9%9B%86">3.神经网络的实现过程（利用神经网络分类二维数据集）</a>
<ul>
<li><a href="#31%E5%AE%89%E8%A3%85%E5%8C%85">3.1安装包</a></li>
<li><a href="#32-%E6%95%B0%E6%8D%AE%E9%9B%86">3.2 数据集</a></li>
<li><a href="#33%E5%AE%9A%E4%B9%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">3.3定义神经网络结构</a>
<ul>
<li><a href="#331%E5%AE%9A%E4%B9%89%E5%8F%98%E9%87%8F">3.3.1定义变量</a></li>
<li><a href="#332-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0">3.3.2 初始化模型参数</a></li>
<li><a href="#333-%E5%BE%AA%E7%8E%AF">3.3.3 循环</a></li>
<li><a href="#334-%E5%9C%A8nn_model%E4%B8%AD%E9%9B%86%E6%88%90331-332%E5%92%8C333%E9%83%A8%E5%88%86%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0">3.3.4- 在nn_model（）中集成3.3.1、3.3.2和3.3.3部分中的函数</a></li>
</ul>
</li>
<li><a href="#45-%E9%A2%84%E6%B5%8B">4.5- 预测</a></li>
</ul>
</li>
<li><a href="#4-%E8%B0%83%E6%95%B4%E9%9A%90%E8%97%8F%E5%B1%82%E5%A4%A7%E5%B0%8F%E5%8F%AF%E9%80%89%E7%BB%83%E4%B9%A0">4 调整隐藏层大小（可选练习）</a></li>
</ul>
</li>
</ul>

    </div>
    </div>
    </div>
    <div class="toggleContainer">
        <div class="toggle">
            <i class="fas fa-angle-double-up"></i>
        </div>
    </div>
    <div id="bg">
    </div>
    <div id="bgchoice" style="display: none">link</div>
    
    <div id="bgurl" style="display:none">https://pic2.zhimg.com/80/v2-bcbb1a4f932ab78c198b0a99af266d4e_720w.jpg?source=1940ef5c</div>
       
    </div>
    <!-- 响应式布局，针对手机端内容显示 -->
    <div class="nav-small">
        <head>
  <!-- 引入Bootstrap核心样式文件 -->
  <link rel="stylesheet" href="https://s-hmily.github.io/media/css/bootstrap.min.css">
</head>

<body>
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
          data-target="#barmenu" aria-expanded="false" id="barbutton">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="https://s-hmily.github.io">叮叮当&nbsp;&nbsp;|</a>
      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="barmenu">
        <ul class="nav navbar-nav">
          
          
          <li>
            <a href="https://s-hmily.github.io">
              首页
            </a>
          </li>
          
          
          
          <li>
            <a href="/archives">
              归档
            </a>
          </li>
          
          
          
          <li>
            <a href="https://s-hmily.github.io/tags">
              标签
            </a>
          </li>
          
          
          
            <li><a href="https://s-hmily.github.io/talk">说说</a></li>
            
          
          <li><a href="https://s-hmily.github.io/friends">友链</a></li>

          
        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>


  <!-- 引入jQuery核心js文件 -->
  <script src="http://cdn.static.runoob.com/libs/jquery/2.1.1/jquery.min.js"></script>
  <script>
  var btstate = false;
  var bt = $("#barbutton");
  var bm = $("#barmenu");
  bt.click(function(){
    dropdown();
  })
  function dropdown(){
    console.log(btstate);
    //下拉
    if(btstate==false){
      bt.removeClass("collapsed");
      bt.attr("aria-expanded","true");
      bm.attr("aria-expanded","true")
      bm.fadeIn(700);
      btstate = true;
    }
    else{
      bt.addClass("collapsed");
      bt.attr("aria-expanded","false");
      bm.removeClass("in");
      bm.hide();
      bm.attr("aria-expanded","false");
      btstate = false;
    }
  }
  </script> 
</body>
    <div style="margin-top:30px"></div>
    <link rel="stylesheet" href="https://s-hmily.github.io/media/css/font-awesome.css">
<style>

</style>

<body>
    <div class="allcontent" id="postdetail">
        <div class="postshow">
            
            <div class="postdetailimg" style="width:100%;overflow: hidden;display: none">
                
                <img src="https://pic2.zhimg.com/80/v2-bcbb1a4f932ab78c198b0a99af266d4e_720w.jpg?source=1940ef5c" class="postimage">
                
            </div>
            <div class="postinfo-detail">
                <div class="postdate"><i class="fa fa-calendar"></i>2020-07-21</div>
            <div class="poststatus postdate"><i class="fa fa-clock-o"></i> 33 min read</div>
            <div class="posttag">
                
            </div>
        </div>
        
        <div id="texttitle" style="text-align: center">
            <h2>浅层神经网络的解析及其应用</h2>
            <!-- id 将作为查询条件 -->
            <div id="pl" style="display:none">https://s-hmily.github.io/post/qian-ceng-shen-jing-wang-luo-de-jie-xi-ji-qi-ying-yong/</div>
            <div id="rootaddr" style="display:none">https://s-hmily.github.io</div>
            <span id="hotnum" class="leancloud_visitors" data-flag-title="浅层神经网络的解析及其应用">
                <h4 class="readercount">热度🔥: <i class="leancloud-visitors-count">loading...</i></h4>
            </span>
        </div>
        <div class="text ">
            <h2 id="1前言">1.前言</h2>
<p>众所周知，我们生活中遇到的很多问题都是非线性的问题，例如预测花的生长情况。那么如果我们遇到了一些非线性问题，我们想要对这个非线性问题进行预测估计的时候我们需要怎么办呢？</p>
<p>这里就需要引入神经网络的概念了。</p>
<h2 id="2神经网络的概述">2.神经网络的概述</h2>
<p>我们知道，我们日常生活中很多问题，甚至说大多数问题都不是线性可分问题，那我们要解决非线性可分问题该怎样处理呢？这就是这部分我们要引出的“多层”的概念。既然单层感知机解决不了非线性问题，那我们就采用多层感知机，下图就是一个两层感知机解决异或问题的示意图：</p>
<figure data-type="image" tabindex="1"><img src="https://images2015.cnblogs.com/blog/764050/201606/764050-20160619152400976-1951202651.gif" alt="img" loading="lazy"></figure>
<p>构建好上述网络以后，通过训练得到最后的分类面如下：</p>
<figure data-type="image" tabindex="2"><img src="https://images2015.cnblogs.com/blog/764050/201606/764050-20160619152721601-526579103.png" alt="img" loading="lazy"></figure>
<p>由此可见，多层感知机可以很好的解决非线性可分问题，我们通常将多层感知机这样的多层结构称之为是神经网络。但是，多层感知机虽然可以在理论上可以解决非线性问题，但是实际生活中问题的复杂性要远不止异或问题这么简单，所以我们往往要构建多层网络，而对于多层神经网络采用什么样的学习算法又是一项巨大的挑战，如下图所示的具有4层隐含层的网络结构中至少有33个参数（不计偏置bias参数），我们应该如何去确定呢？</p>
<figure data-type="image" tabindex="3"><img src="https://images2015.cnblogs.com/blog/764050/201606/764050-20160619154842679-926221650.jpg" alt="img" loading="lazy"></figure>
<p>我们先看一个图：</p>
<figure data-type="image" tabindex="4"><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630140644406-409859737.png" alt="img" loading="lazy"></figure>
<p>这是典型的三层神经网络的基本组成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，我们现在手里有一堆数据{x1,x2,x3,...,xn},输出也是一堆数据{y1,y2,y3,...,yn},现在要他们在隐含层做某种变换，让你把数据灌进去后得到你期望的输出。如果你希望你的输出和原始输入一样，那么就是最常见的自编码模型（Auto-Encoder）。可能有人会问，为什么要输入输出都一样呢？有什么用啊？其实应用挺广的，在图像识别，文本分类等等都会用到，包括一些变种之类的。如果你的输出和原始输入不一样，那么就是很常见的人工神经网络了，相当于让原始数据通过一个映射来得到我们想要的输出数据，也就是我们今天要讲的话题。</p>
<h2 id="3神经网络的实现过程利用神经网络分类二维数据集">3.神经网络的实现过程（利用神经网络分类二维数据集）</h2>
<p><strong>提示</strong>：建立神经网络的一般方法：</p>
<p>1.定义神经网络结构（输入单元数，隐藏单元数等）.</p>
<p>2.初始化模型的参数</p>
<p>3.循环：</p>
<ul>
<li>
<p>实现前向传播</p>
</li>
<li>
<p>计算损失</p>
</li>
<li>
<p>后向传播以获得梯度</p>
</li>
<li>
<p>更新参数（梯度下降）</p>
</li>
<li>
<p>我们通常会构建辅助函数来计算第1-3步，然后将它们合并为<code>nn_model()</code>函数。一旦构建了<code>nn_model()</code>并学习了正确的参数，就可以对新数据进行预测。</p>
<h3 id="31安装包">3.1安装包</h3>
<p>让我们首先导入在作业过程中需要的所有软件包。</p>
<ul>
<li>
<p><a href="https://www.kesci.com/api/notebooks/5e85d6bf95b029002ca7e7e6/www.numpy.org">numpy</a>是Python科学计算的基本包。</p>
</li>
<li>
<p><a href="http://scikit-learn.org/stable/">sklearn</a>提供了用于数据挖掘和分析的简单有效的工具。</p>
</li>
<li>
<p><a href="http://matplotlib.org/">matplotlib</a> 是在Python中常用的绘制图形的库。</p>
</li>
<li>
<p>testCases提供了一些测试示例用以评估函数的正确性</p>
</li>
<li>
<p>planar_utils提供了此作业中使用的各种函数</p>
<pre><code># Package imports
import numpy as np
import matplotlib.pyplot as plt
from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

%matplotlib inline

np.random.seed(1) # set a seed so that the results are consistent
</code></pre>
</li>
</ul>
<h3 id="32-数据集">3.2 数据集</h3>
<pre><code>X, Y = load_planar_dataset() 
# Visualize the data:
plt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0,:].shape), s=40, cmap=plt.cm.Spectral)
</code></pre>
<p>其中X是花瓣的点，表示特征矩阵（X1,X2）这个X数据集可以在后边用X.shape得到这个X是2行400列的</p>
<p>Y数据集是1行400列的，表示标签(红色：0，蓝色：1)</p>
<p>获得numpy数组的shape维度代码</p>
<pre><code>### START CODE HERE ### (≈ 3 lines of code)
shape_X = X.shape
shape_Y = Y.shape

m = shape_X[1]  # training set size
### END CODE HERE ###

print ('The shape of X is: ' + str(shape_X))
print ('The shape of Y is: ' + str(shape_Y))
print ('I have m = %d training examples!' % (m))
</code></pre>
<p>结果：</p>
<p>The shape of X is: (2, 400)</p>
<p>The shape of Y is: (1, 400)</p>
<p>I have m = 400 training examples!</p>
<h3 id="33定义神经网络结构">3.3定义神经网络结构</h3>
<h4 id="331定义变量">3.3.1定义变量</h4>
<p><strong>目标</strong>：定义三个变量     n_x:输入层大小  n_h：隐藏层大小(这里设置成4)   n_y:输出层大小</p>
<p><strong>提示</strong>：使用shape函数来找到n_x和n_y。另外 将隐藏层大小硬编码为4</p>
<pre><code># GRADED FUNCTION: layer_sizes

def layer_sizes(X, Y):
    &quot;&quot;&quot;
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    &quot;&quot;&quot;
    ### START CODE HERE ### (≈ 3 lines of code)
    n_x = X.shape[0] # size of input layer
    n_h = 4
    n_y = Y.shape[0] # size of output layer
    ### END CODE HERE ###
    return (n_x, n_h, n_y)
</code></pre>
<p>​    导入一组数据进行测试：</p>
<pre><code>X_assess, Y_assess = layer_sizes_test_case()
(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)
print(&quot;The size of the input layer is: n_x = &quot; + str(n_x))
print(&quot;The size of the hidden layer is: n_h = &quot; + str(n_h))
print(&quot;The size of the output layer is: n_y = &quot; + str(n_y))
</code></pre>
<p>结果：</p>
<p>The size of the input layer is: n_x = 5</p>
<p>The size of the hidden layer is: n_h = 4</p>
<p>The size of the output layer is: n_y = 2</p>
</li>
</ul>
<p><strong>预期输出</strong>（仅用于评估刚刚编码的函数，并不代表实际网络大小）。 输入层的大小为：n_x = 5 隐藏层的大小为：n_h = 4 输出层的大小为：n_y = 2</p>
<p>对于我们这个整个题来讲，应该是n_x = 2 n_h = 4 n_y = 21</p>
<h4 id="332-初始化模型参数">3.3.2 初始化模型参数</h4>
<p><strong>目标</strong>：实现函数 <code>initialize_parameters()</code>。</p>
<p><strong>提示：</strong></p>
<ul>
<li>请确保参数大小正确。 如果需要，也可参考上面的神经网络图。</li>
<li>使用随机值初始化权重矩阵。      - 使用：<code>np.random.randn（a，b）* 0.01</code>随机初始化维度为（a，b）的矩阵。</li>
<li>将偏差向量初始化为零。      - 使用：<code>np.zeros((a,b))</code> 初始化维度为（a，b）零的矩阵。</li>
</ul>
<pre><code>#初始化模型的参数
def initialize_parameters(n_x,n_h,n_y):
    '''
    参数：
    n_x:输入层结点的数量
    n_h:隐藏层结点的数量
    n_y:输出层结点的数量
    返回：
    paraneters:包含参数的字典
    W1:权重矩阵，维度为(n_h,n_x)
    b1:偏向量，维度为(n_h,1)
    W2：权重矩阵，维度为(n_y,n_h)
    b2:偏向量，维度为(n_y,1)
    '''
    #通过本函数可以返回一个或一组服从“0~1”均匀分布的随机样本值。随机样本取值范围是[0,1)，不包括1。
    np.random.seed(2)#指定一个随机种子
    W1=np.random.rand(n_h,n_x)*0.01
    #zeros 生成0矩阵
    b1=np.zeros(shape=(n_h,1))
    W2=np.random.rand(n_y,n_h)
    b2=np.random.rand(n_y,1)
    #使用断言确保我的数据格式是正确的
    #assert是用于对程序进行调试的，对于执行结构的判断，而不是对于业务流程的判断。（相当于一个if ()语句，如果满足断言的执行程序，如果不满足则抛错误
    assert(W1.shape==(n_h,n_x))
    assert(b1.shape==(n_h,1))
    assert(W2.shape==(n_y,n_h))
    assert(b2.shape==(n_y,1))
    parameters={
        &quot;W1&quot;:W1,
        &quot;b1&quot;:b1,
        &quot;W2&quot;:W2,
        &quot;b2&quot;:b2
    }
    return parameters
</code></pre>
<p>测试代码：</p>
<pre><code>n_x, n_h, n_y = initialize_parameters_test_case()

parameters = initialize_parameters(n_x, n_h, n_y)
print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>结果：W1 = [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]
b2 = [[0.]]
</code></pre>
<p><strong>预期输出</strong>: W1 = [[-0.00416758 -0.00056267][-0.02136196 0.01640271] [-0.01793436 -0.00841747][ 0.00502881 -0.01245288]] b1 = [[0.][0.] [0.][0.]] W2 = [[-0.01057952 -0.00909008 0.00551454 0.02292208]] b2 = [[0.]]</p>
<h4 id="333-循环">3.3.3 循环</h4>
<p><strong>目标</strong>：实现<code>forward_propagation（）</code>（实现正向传播）。</p>
<p>提示：</p>
<ul>
<li>
<p>在上方查看分类器的数学表示形式。</p>
</li>
<li>
<p>你可以使用内置在笔记本中的<code>sigmoid()</code>函数。</p>
</li>
<li>
<p>你也可以使用numpy库中的<code>np.tanh（）</code>函数。</p>
</li>
<li>
<p>必须执行以下步骤：      1.使用<code>parameters [“ ..”]</code>从字典“ parameters”（这是<code>initialize_parameters（）</code>的输出）中检索出每个参数。      2.实现正向传播，计算Z[1],A[1],Z[2] 和 A[2] （所有训练数据的预测结果向量）。</p>
</li>
<li>
<p>向后传播所需的值存储在<code>cache</code>中， <code>cache</code>将作为反向传播函数的输入。</p>
<p>代码：</p>
<pre><code># GRADED FUNCTION: forward_propagation

def forward_propagation(X, parameters):
    &quot;&quot;&quot;
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)
    
    Returns:
    A2 -- The sigmoid output of the second activation
    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;
    &quot;&quot;&quot;
    # Retrieve each parameter from the dictionary &quot;parameters&quot;
    ### START CODE HERE ### (≈ 4 lines of code)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Implement Forward Propagation to calculate A2 (probabilities)
    ### START CODE HERE ### (≈ 4 lines of code)
    Z1 = np.dot(W1,X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2,A1) + b2
    A2 = sigmoid(Z2)
    ### END CODE HERE ###
    
    assert(A2.shape == (1, X.shape[1]))
    
    cache = {&quot;Z1&quot;: Z1,
             &quot;A1&quot;: A1,
             &quot;Z2&quot;: Z2,
             &quot;A2&quot;: A2}
    
    return A2, cache
</code></pre>
</li>
</ul>
<p>测试代码：</p>
<pre><code>X_assess, parameters = forward_propagation_test_case()

A2, cache = forward_propagation(X_assess, parameters)

# Note: we use the mean here just to make sure that your output matches ours. 
print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
</code></pre>
<pre><code>-0.0004997557777419913 -0.000496963353231779 0.00043818745095914653 0.500109546852431
</code></pre>
<p><strong>预期输出</strong>: -0.0004997557777419913  -0.000496963353231779 0.00043818745095914653        0.500109546852431</p>
<p>现在，你已经计算了包含每个示例的a<a href="i">2</a> 的 A[2] （在Python变量“<code>A2</code>”中），其中，你可以计算损失函数 如下：<img src="C:%5CUsers%5CWGJS%5CAppData%5CLocal%5CTemp%5C1595039574626.png" alt="1595039574626" loading="lazy"></p>
<p><strong>目的</strong>：实现<code>compute_cost（）</code>以计算损失J的值。</p>
<p>有很多种方法可以实现交叉熵损失。 我们为你提供了实现方法 ：<img src="C:%5CUsers%5CWGJS%5CAppData%5CLocal%5CTemp%5C1595039675666.png" alt="1595039675666" loading="lazy"></p>
<pre><code>logprobs = np.multiply(np.log(A2),Y)
cost = - np.sum(logprobs)   
</code></pre>
<p>（你也可以使用np.multiply()然后使用np.sum()或直接使用np.dot()）。</p>
<pre><code># GRADED FUNCTION: compute_cost

def compute_cost(A2, Y, parameters):
    &quot;&quot;&quot;
    Computes the cross-entropy cost given in equation (13)
    
    Arguments:
    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    Y -- &quot;true&quot; labels vector of shape (1, number of examples)
    parameters -- python dictionary containing your parameters W1, b1, W2 and b2
    
    Returns:
    cost -- cross-entropy cost given equation (13)
    &quot;&quot;&quot;
    
    m = Y.shape[1] # number of example

    # Compute the cross-entropy cost
     ### START CODE HERE ### (≈ 2 lines of code)
    logprobs = Y*np.log(A2) + (1-Y)* np.log(1-A2)
    cost = -1/m * np.sum(logprobs)
    ### END CODE HERE ###
    
    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. 
                                # E.g., turns [[17]] into 17 
    assert(isinstance(cost, float))
    
    return cost
</code></pre>
<p><strong>测试</strong>：</p>
<pre><code>A2, Y_assess, parameters = compute_cost_test_case()

print(&quot;cost = &quot; + str(compute_cost(A2, Y_assess, parameters)))
</code></pre>
<pre><code>cost = 0.6929198937761265
</code></pre>
<p>现在，通过使用在正向传播期间计算的缓存，你可以实现后向传播。</p>
<p><strong>问题</strong>：实现函数<code>backward_propagation（）</code>。</p>
<p><strong>说明</strong>： 反向传播通常是深度学习中最难（最数学）的部分。为了帮助你更好地了解，我们提供了反向传播课程的幻灯片。你将要使用此幻灯片右侧的六个方程式以构建向量化实现。</p>
<figure data-type="image" tabindex="5"><img src="https://cdn.kesci.com/upload/image/q17hcd4yra.png?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<figure data-type="image" tabindex="6"><img src="C:%5CUsers%5CWGJS%5CAppData%5CLocal%5CTemp%5C1595040132956.png" alt="1595040132956" loading="lazy"></figure>
<pre><code># GRADED FUNCTION: backward_propagation

def backward_propagation(parameters, cache, X, Y):
    &quot;&quot;&quot;
    Implement the backward propagation using the instructions above.
    
    Arguments:
    parameters -- python dictionary containing our parameters 
    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;.
    X -- input data of shape (2, number of examples)
    Y -- &quot;true&quot; labels vector of shape (1, number of examples)
    
    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    &quot;&quot;&quot;
    m = X.shape[1]
    
    # First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.
    ### START CODE HERE ### (≈ 2 lines of code)
    W1 = parameters[&quot;W1&quot;]
    W2 = parameters[&quot;W2&quot;]
    ### END CODE HERE ###
        
    # Retrieve also A1 and A2 from dictionary &quot;cache&quot;.
    ### START CODE HERE ### (≈ 2 lines of code)
    A1 = cache[&quot;A1&quot;]
    A2 = cache[&quot;A2&quot;]
    ### END CODE HERE ###
    
    # Backward propagation: calculate dW1, db1, dW2, db2. 
    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)
    dZ2= A2 - Y
    dW2 = 1 / m * np.dot(dZ2,A1.T)
    db2 = 1 / m * np.sum(dZ2,axis=1,keepdims=True)
    dZ1 = np.dot(W2.T,dZ2) * (1-np.power(A1,2))
    dW1 = 1 / m * np.dot(dZ1,X.T)
    db1 = 1 / m * np.sum(dZ1,axis=1,keepdims=True)
    ### END CODE HERE ###
    
    grads = {&quot;dW1&quot;: dW1,
             &quot;db1&quot;: db1,
             &quot;dW2&quot;: dW2,
             &quot;db2&quot;: db2}
    
    return grads
</code></pre>
<p>In [16]:</p>
<pre><code>parameters, cache, X_assess, Y_assess = backward_propagation_test_case()

grads = backward_propagation(parameters, cache, X_assess, Y_assess)
print (&quot;dW1 = &quot;+ str(grads[&quot;dW1&quot;]))
print (&quot;db1 = &quot;+ str(grads[&quot;db1&quot;]))
print (&quot;dW2 = &quot;+ str(grads[&quot;dW2&quot;]))
print (&quot;db2 = &quot;+ str(grads[&quot;db2&quot;]))
</code></pre>
<pre><code>dW1 = [[ 0.01018708 -0.00708701]
 [ 0.00873447 -0.0060768 ]
 [-0.00530847  0.00369379]
 [-0.02206365  0.01535126]]
db1 = [[-0.00069728]
 [-0.00060606]
 [ 0.000364  ]
 [ 0.00151207]]
dW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]
db2 = [[0.06589489]]
</code></pre>
<p><strong>预期输出</strong>: dW1 = [[ 0.01018708 -0.00708701][ 0.00873447 -0.0060768 ] [-0.00530847 0.00369379][-0.02206365 0.01535126]] db1 = [[-0.00069728][-0.00060606] [ 0.000364 ][ 0.00151207]] dW2 = [[ 0.00363613 0.03153604 0.01162914 -0.01318316]] db2 = [[0.06589489]]</p>
<p><strong>问题</strong>：实现参数更新。 使用梯度下降，你必须使用（dW1，db1，dW2，db2）才能更新（W1，b1，W2，b2）。</p>
<p><strong>一般的梯度下降规则</strong>：θ=θ−α∂J∂θ其中α是学习率，而θ 代表一个参数。</p>
<p><strong>图示</strong>：具有良好的学习速率（收敛）和较差的学习速率（发散）的梯度下降算法。 图片由Adam Harley提供。</p>
<figure data-type="image" tabindex="7"><img src="https://cdn.kesci.com/upload/image/q17hh4otzu.gif?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<figure data-type="image" tabindex="8"><img src="https://cdn.kesci.com/upload/image/q17hharbth.gif?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<pre><code># GRADED FUNCTION: update_parameters

def update_parameters(parameters, grads, learning_rate = 1.2):
    &quot;&quot;&quot;
    Updates parameters using the gradient descent update rule given above
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients 
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    &quot;&quot;&quot;
    # Retrieve each parameter from the dictionary &quot;parameters&quot;
    ### START CODE HERE ### (≈ 4 lines of code)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Retrieve each gradient from the dictionary &quot;grads&quot;
    ### START CODE HERE ### (≈ 4 lines of code)
    dW1 = grads[&quot;dW1&quot;]
    db1 = grads[&quot;db1&quot;]
    dW2 = grads[&quot;dW2&quot;]
    db2 = grads[&quot;db2&quot;]
    ## END CODE HERE ###
    
    # Update rule for each parameter
    ### START CODE HERE ### (≈ 4 lines of code)
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    ### END CODE HERE ###
    
    parameters = {&quot;W1&quot;: W1,
                  &quot;b1&quot;: b1,
                  &quot;W2&quot;: W2,
                  &quot;b2&quot;: b2}
    
    return parameters
</code></pre>
<p>测试：</p>
<pre><code>parameters, grads = update_parameters_test_case()
parameters = update_parameters(parameters, grads)

print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>结果：
W1 = [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]
b1 = [[-1.02420756e-06]
 [ 1.27373948e-05]
 [ 8.32996807e-07]
 [-3.20136836e-06]]
W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]
b2 = [[0.00010457]]
</code></pre>
<p><strong>预期输出</strong>: dW1 = [[ 0.01018708 -0.00708701][ 0.00873447 -0.0060768 ] [-0.00530847 0.00369379][-0.02206365 0.01535126]] db1 = [[-0.00069728][-0.00060606] [ 0.000364 ][ 0.00151207]] dW2 = [[ 0.00363613 0.03153604 0.01162914 -0.01318316]] db2 = [[0.06589489]]</p>
<h4 id="334-在nn_model中集成331-332和333部分中的函数">3.3.4- 在nn_model（）中集成3.3.1、3.3.2和3.3.3部分中的函数</h4>
<p><strong>问题</strong>：在nn_model（）中建立你的神经网络模型。</p>
<p><strong>说明</strong>：神经网络模型必须以正确的顺序组合先前构建的函数。</p>
<p>In [19]:</p>
<pre><code># GRADED FUNCTION: nn_model

def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    &quot;&quot;&quot;
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    &quot;&quot;&quot;
    
    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.
    ### START CODE HERE ### (≈ 5 lines of code)
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Loop (gradient descent)

    for i in range(0, num_iterations):
         
        ### START CODE HERE ### (≈ 4 lines of code)
        # Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.
        A2, cache = forward_propagation(X, parameters)
        
        # Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.
        cost = compute_cost(A2, Y, parameters)
 
        # Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.
        grads = backward_propagation(parameters, cache, X, Y)
 
        # Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.
        parameters = update_parameters(parameters, grads)
        
        ### END CODE HERE ###
        
        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))

    return parameters
</code></pre>
<p>测试</p>
<pre><code>X_assess, Y_assess = nn_model_test_case()

parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=False)
print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log
/home/kesci/work/planar_utils.py:34: RuntimeWarning: overflow encountered in exp
  s = 1/(1+np.exp(-x))
</code></pre>
<pre><code>结果：
W1 = [[-4.18503197  5.33214315]
 [-7.52988635  1.24306559]
 [-4.19302427  5.32627154]
 [ 7.52984762 -1.24308746]]
b1 = [[ 2.32926944]
 [ 3.79460252]
 [ 2.33002498]
 [-3.79466751]]
W2 = [[-6033.83668723 -6008.12983227 -6033.10091631  6008.06624417]]
b2 = [[-52.66610924]]
</code></pre>
<p><strong>预期输出</strong>: W1 = [[-4.18503197 5.33214315][-7.52988635 1.24306559] [-4.19302427 5.32627154][ 7.52984762 -1.24308746]] b1 = [[ 2.32926944][ 3.79460252] [ 2.33002498][-3.79466751]] W2 = [[-6033.83668723 -6008.12983227 -6033.10091631 6008.06624417]] b2 = [[-52.66610924]]</p>
<h3 id="45-预测">4.5- 预测</h3>
<p><strong>问题</strong>：使用你的模型通过构建predict()函数进行预测。 使用正向传播来预测结果。</p>
<p><strong>提示</strong>：predictions = yprediction=1{activation &gt; 0.5}={1if activation&gt;0.50otherwise<br>
例如，如果你想基于阈值将矩阵X设为0和1，则可以执行以下操作： <code>X_new = (X &gt; threshold)</code></p>
<pre><code># GRADED FUNCTION: predict

def predict(parameters, X):
    &quot;&quot;&quot;
    Using the learned parameters, predicts a class for each example in X
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    X -- input data of size (n_x, m)
    
    Returns
    predictions -- vector of predictions of our model (red: 0 / blue: 1)
    &quot;&quot;&quot;
    
    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.
  ### START CODE HERE ### (≈ 2 lines of code)
    A2, cache = forward_propagation(X, parameters)
    predictions = np.round(A2)
    ### END CODE HERE ###
    
    return predictions
</code></pre>
<p>测试：</p>
<pre><code>parameters, X_assess = predict_test_case()

predictions = predict(parameters, X_assess)
print(&quot;predictions mean = &quot; + str(np.mean(predictions)))
</code></pre>
<pre><code>predictions mean = 0.6666666666666666
</code></pre>
<p><strong>预期输出</strong>: predictions mean = 0.6666666666666666</p>
<p>现在运行模型以查看其如何在二维数据集上运行。 运行以下代码以使用含有nh隐藏单元的单个隐藏层测试模型。</p>
<pre><code># Build a model with a n_h-dimensional hidden layer
parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)

# Plot the decision boundary
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4))
</code></pre>
<pre><code>结果：
Cost after iteration 0: 0.693048
Cost after iteration 1000: 0.288083
Cost after iteration 2000: 0.254385
Cost after iteration 3000: 0.233864
Cost after iteration 4000: 0.226792
Cost after iteration 5000: 0.222644
Cost after iteration 6000: 0.219731
Cost after iteration 7000: 0.217504
Cost after iteration 8000: 0.219467
Cost after iteration 9000: 0.218561
</code></pre>
<pre><code>Text(0.5, 1.0, 'Decision Boundary for hidden layer size 4')

</code></pre>
<figure data-type="image" tabindex="9"><img src="https://cdn.kesci.com/rt_upload/CB068684F93C4A2A8AE816EB492CDCBE/q17hj9pr80.png" alt="img" loading="lazy"></figure>
<p><strong>预期输出</strong>: Cost after iteration 9000: 0.218561</p>
<pre><code># Print accuracy
predictions = predict(parameters, X)
print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')

</code></pre>
<pre><code>Accuracy: 90%

</code></pre>
<p><strong>预期输出</strong>: Accuracy: 90%</p>
<p>与Logistic回归相比，准确性确实更高。 该模型学习了flower的叶子图案！ 与逻辑回归不同，神经网络甚至能够学习非线性的决策边界。</p>
<p>现在，让我们尝试几种不同的隐藏层大小。</p>
<h2 id="4-调整隐藏层大小可选练习">4 调整隐藏层大小（可选练习）</h2>
<p>运行以下代码（可能需要1-2分钟）， 你将观察到不同大小隐藏层的模型的不同表现。</p>
<p>In [25]:</p>
<pre><code># This may take about 2 minutes to run

plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 10, 20]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print (&quot;Accuracy for {} hidden units: {} %&quot;.format(n_h, accuracy))

</code></pre>
<pre><code>Accuracy for 1 hidden units: 67.5 %
Accuracy for 2 hidden units: 67.25 %
Accuracy for 3 hidden units: 90.75 %
Accuracy for 4 hidden units: 90.5 %
Accuracy for 5 hidden units: 91.25 %
Accuracy for 10 hidden units: 90.25 %
Accuracy for 20 hidden units: 90.5 %

</code></pre>
<figure data-type="image" tabindex="10"><img src="https://cdn.kesci.com/rt_upload/F70D5A23097642688F6245327ACE9DD7/q17hkk98sl.png" alt="img" loading="lazy"></figure>
<p>完整代码</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary,sigmoid,load_planar_dataset,load_extra_datasets
#设置一个固定的随机种子
np.random.seed(1)
X,Y=load_planar_dataset()
#用matplotlib可视化数据
plt.scatter(X[0,:],X[1,:],c=np.squeeze(Y),s=40,cmap=plt.cm.Spectral)
plt.show()
shape_X=X.shape
shape_Y=Y.shape
m=Y.shape[1] #训练集里面的数据
print(&quot;X的维度为：&quot;+str(shape_X))
print(&quot;Y的维度为：&quot;+str(shape_Y))
print(&quot;数据集里面的数据有：&quot;+str(m)+&quot;个&quot;)
def layer_sizes(X,Y):
    '''参数：
    X：输入数据集，维度是（输入的数量，训练/测试的数量）
    Y：标签
    返回：
    n_x:输入层的数量
    n_h:隐藏层的数量
    n_y:输出层的数量'''
    n_x=X.shape[0]#输入层
    n_h=4#隐藏层，硬编码为4
    n_y=Y.shape[0]
    return (n_x,n_h,n_y)
#测试layer_sizes
print(&quot;=========================测试layer_sizes=========================&quot;)
X_asses , Y_asses = layer_sizes_test_case()
(n_x,n_h,n_y) =  layer_sizes(X_asses,Y_asses)
print(&quot;输入层的节点数量为: n_x = &quot; + str(n_x))
print(&quot;隐藏层的节点数量为: n_h = &quot; + str(n_h))
print(&quot;输出层的节点数量为: n_y = &quot; + str(n_y))
#初始化模型的参数
def initialize_parameters(n_x,n_h,n_y):
    '''
    参数：
    n_x:输入层结点的数量
    n_h:隐藏层结点的数量
    n_y:输出层结点的数量
    返回：
    paraneters:包含参数的字典
    W1:权重矩阵，维度为(n_h,n_x)
    b1:偏向量，维度为(n_h,1)
    W2：权重矩阵，维度为(n_y,n_h)
    b2:偏向量，维度为(n_y,1)
    '''
    #通过本函数可以返回一个或一组服从“0~1”均匀分布的随机样本值。随机样本取值范围是[0,1)，不包括1。
    np.random.seed(2)#指定一个随机种子
    W1=np.random.rand(n_h,n_x)*0.01
    #zeros 生成0矩阵
    b1=np.zeros(shape=(n_h,1))
    W2=np.random.rand(n_y,n_h)
    b2=np.random.rand(n_y,1)
    #使用断言确保我的数据格式是正确的
    #assert是用于对程序进行调试的，对于执行结构的判断，而不是对于业务流程的判断。（相当于一个if ()语句，如果满足断言的执行程序，如果不满足则抛错误
    assert(W1.shape==(n_h,n_x))
    assert(b1.shape==(n_h,1))
    assert(W2.shape==(n_y,n_h))
    assert(b2.shape==(n_y,1))
    parameters={
        &quot;W1&quot;:W1,
        &quot;b1&quot;:b1,
        &quot;W2&quot;:W2,
        &quot;b2&quot;:b2
    }
    return parameters
#测试initialize_parameters
n_x,n_h,n_y=initialize_parameters_test_case()
parameters=initialize_parameters(n_x,n_h,n_y)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters[&quot;b2&quot;]))
#实现循环
'''我们现在要实现前向传播函数forward_propagation()。 
我们可以使用sigmoid()函数，也可以使用np.tanh()函数。  
步骤如下：
使用字典类型的parameters（它是initialize_parameters() 的输出）检索每个参数。
实现向前传播, 计算Z[1],A[1],Z[2]Z[1],A[1],Z[2] 和 A[2]A[2]（ 训练集里面所有例子的预测向量）。
反向传播所需的值存储在“cache”中，cache将作为反向传播函数的输入。
'''
def forward_propagation(X,parameters):
    '''
    参数：
    X-维度为(n_x,m)的输入数据
    parameters:初始化函数(initialize_parameters)的输出
    返回:
    A2:使用激活函数后的激活值
    cache-包含“Z1&quot;,&quot;A1&quot;,&quot;Z2&quot;和“A2”的字典型变量
    '''
    W1=parameters[&quot;W1&quot;]
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    #向前传播计算A2
    #dot 计算矩阵
    Z1=np.dot(W1,X)+b1
    A1=np.tanh(Z1)
    Z2=np.dot(W2,A1)+b2
    A2=sigmoid(Z2)
    #使用断言保证我的数据格式是正确的
    assert(A2.shape==(1,X.shape[1]))
    cache={&quot;Z1&quot;:Z1,
           &quot;A1&quot;:A1,
           &quot;Z2&quot;:Z2,
           &quot;A2&quot;:A2
    }
    return (A2,cache)
#测试模型
X_assess,parameters=forward_propagation_test_case()
A2,cache=forward_propagation(X_assess,parameters)
print(np.mean(cache[&quot;Z1&quot;]),np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
#计算成本函数与损失函数
 
def compute_cost(A2,Y,parameters):
    '''计算交叉熵
    参数：
    A2：使用sigmoid函数计算的第二次激活后的函数值
    Y:True标签向量，维度为(1,数量)
    parameters:一个包含W1，W2，b1,b2的字典类型的变量
    返回：成本交叉熵函数，给出方程
    '''
    m=Y.shape[1]
    W1=parameters['W1']
    W2=parameters['W2']
    #计算成本
    logprobs=logprobs=np.multiply(np.log(A2),Y)+np.multiply((1-Y),np.log(1-A2))
    cost=-np.sum(logprobs)/m
    cost=float(np.squeeze(cost))
    assert(isinstance(cost,float))
    return cost
#预测compute_cost
A2,Y_assess,parameters=compute_cost_test_case()
print(&quot;cost=&quot;+str(compute_cost(A2,Y_assess,parameters)))
#搭建反向传播函数
def backward_propagation(parameters,cache,X,Y):
    '''参数：
    parameters:包含参数的一个字典变量
    cache:包含Z1,A1,Z2,A2的字典类型的变量
    X:输入数据，维度为(2,数量)
    Y:输出数据，唯独为(1,数量)
    返回:
    grads:包含W和b的导数的一个字典变量'''
    m=X.shape[1]
    W1=parameters['W1']
    W2=parameters['W2']
    A1=cache['A1']
    A2=cache['A2']
    dZ2=A2-Y
    dW2=(1/m)*np.dot(dZ2,A1.T)
    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)
    dZ1=np.multiply(np.dot(W2.T,dZ2),1-np.power(A1,2))
    dW1 = (1 / m) * np.dot(dZ1, X.T)
    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)
    grads={'dW1':dW1,
           'db1':db1,
           'dW2':dW2,
           'db2':db2}
    return grads
#更新参数
def update_parameters(parameters,grads,learning_rate=1.2):
    '''参数：
    parameters:包含参数的字典类型数据的变量
    grads:包含导数值得字典类型变量
    learning_rate:学习速率
    返回：
    parameters:包含更新参数的字典类型和变量'''
    W1,W2=parameters['W1'],parameters['W2']
    b1,b2=parameters['b1'],parameters['b2']
    dW1,dW2=grads['dW1'],grads['dW2']
    db1,db2=grads['db1'],grads['db2']
    W1=W1-learning_rate*dW1
    b1=b1-learning_rate*db1
    W2=W2-learning_rate*dW2
    b2=b2-learning_rate*db2
    parameters={&quot;W1&quot;:W1,
                &quot;b1&quot;:b1,
                &quot;W2&quot;:W2,
                &quot;b2&quot;:b2}
    return parameters
#测试update_parameters
parameters,grads=update_parameters_test_case()
parameters=update_parameters(parameters,grads)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters['b2']))
#把上述内容整合到nn_model()中，神经网络模型必须以正确的顺序使用先前的功能
def nn_model(X,Y,n_h,num_iterations,print_cost=False):
    '''参数：
    X-数据集，维度为(2,示例数)
    Y-标签,维度为（1，示例数）
    n_h:隐藏层的数量
    num_iterations:梯度下降循环中的迭代次数
    print_cost:如果为True,则每100次迭代打印一次成本数值
    返回:
    parameters:模型学习的参数，它们可以用来预测
    '''
    np.random.seed(3)#指定随机种子
    n_x=layer_sizes(X,Y)[0]
    n_y=layer_sizes(X,Y)[2]
    parameters=initialize_parameters(n_x,n_h,n_y)
    W1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    for i in range(num_iterations):
        A2,cache=forward_propagation(X,parameters)
        cost=compute_cost(A2,Y,parameters)
        grads=backward_propagation(parameters,cache,X,Y)
        parameters=update_parameters(parameters,grads,learning_rate=0.5)
        if print_cost:
            if i%1000==0:
                print(&quot;第&quot;,i,&quot;次循环&quot;,&quot;成本为:&quot;+str(cost))
    return parameters
#测试nn_model
X_assess,Y_assess=nn_model_test_case()
parameters=nn_model(X_assess,Y_assess,4,num_iterations=10000,print_cost=False)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters[&quot;b2&quot;]))
#predict模型
'''激活值大于0.5，预测值为1，否则为0'''
def predict(parameters,X):
    '''使用学习的参数为X分类
    参数：
    parameters:包含参数的字典类型的变量
    X:输入数据
    返回：
    predictions:我们模型预测的向量（红色：0/蓝色：1）
    '''
    A2,cache=forward_propagation(X,parameters)
    predictions=np.round(A2)
    return predictions
#测试predict
parameters,X_assess=predict_test_case()
predictions=predict(parameters,X_assess)
print(&quot;预测的平均值=&quot;+str(np.mean(predictions)))
parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)
 
#绘制边界
'''注：数组的可视化
通常算法的结果是可以表示向量的数组，直接利用数组画图时界面为空
利用sequeeze()函数转化为秩为1的数组可以正常画图'''
plot_decision_boundary(lambda x: predict(parameters, x.T), X, np.squeeze(Y))
plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4))
plt.show()
predictions = predict(parameters, X)
print ('准确率: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')
#更改隐藏结点的数量
plt.figure(figsize=(16,32))
hidden_layer_sizes=[1,2,3,4,5,20,50]#隐藏层数量
for i,n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5,2,i+1)
    plt.title('Hidden Layer of Size%d'% n_h)
    parameters=nn_model(X,Y,n_h,num_iterations=5000)
    plot_decision_boundary(lambda x:predict(parameters,x.T),X,np.squeeze(Y))
    predictions=predict(parameters,X)
    accuracy=float((np.dot(Y,predictions.T)+np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print(&quot;隐藏结点的数量：{}，准确率：{}%&quot;.format(n_h,accuracy))

</code></pre>
<p><strong>说明</strong>：</p>
<ul>
<li>
<p>较大的模型（具有更多隐藏的单元）能够更好地拟合训练集，直到最终最大的模型过拟合数据为止。</p>
</li>
<li>
<p>隐藏层的最佳大小似乎在n_h = 5左右。的确，此值似乎很好地拟合了数据，而又不会引起明显的过度拟合。</p>
</li>
<li>
<p>稍后你还将学习正则化，帮助构建更大的模型（例如n_h = 50）而不会过度拟合。</p>
<p>参考文章：</p>
<p><a href="https://www.kesci.com/home/project/5dd3946900b0b900365f3a48/code">文章一</a></p>
<p><a href="https://www.cnblogs.com/charlotte77/p/5629865.html">文章二</a></p>
<p><a href="https://www.cnblogs.com/maybe2030/p/5597716.html">文章三</a></p>
<p>数据包在文章一中 可以直接下载使用</p>
</li>
</ul>

        </div>
        
        
        <div class="next-post">
            下一篇
            <a href="https://s-hmily.github.io/post/juan-ji-shen-jing-wang-luo-jie-xi/">
                卷积神经网络解析
            </a>
        </div>
        
    </div>
    </div>
</body>
<script>
    var t_img; // 定时�?
    var isLoad = true; // 控制变量
    isImgLoad(function () {
        // 加载完成
        $('.postdetailimg').css("display", "block");
    });
    // 判断图片加载的函�?
    function isImgLoad(callback) {
        // 注意我的图片类名都是cover，因为我�?需要�?�理cover。其它图片可以不管�?
        // 查找所有封面图，迭代�?�理
        $('.postdetailimg').each(function () {
            // 找到�?0就将isLoad设为false，并退出each
            if (this.height === 0) {
                isLoad = false;
                return false;
            }
        });
        // 为true，没有发现为0的。加载完�?
        if (isLoad) {
            clearTimeout(t_img); // 清除定时�?
            // 回调函数
            callback();
            // 为false，因为找到了没有加载完成的图，将调用定时器递归
        } else {
            isLoad = true;
            t_img = setTimeout(function () {
                isImgLoad(callback); // 递归�?�?
            }, 500); // 我这里�?�置的是500�?秒就�?描一次，�?以自己调�?
        }
    }

    //文章阅读热度
    var pl = $("#pl").html();
    var rootaddr = $("#rootaddr").html();
    pl = pl.replace(rootaddr, "");
    $("#hotnum").attr('id', pl);
</script>
    <div name="comment" style="background: white;margin-top:100px">
        <div class="commentcontainer">
            
            <p>请到客户端“主题--自定义配置--valine”中填入ID和KEY</p>
            
        </div>
    </div>
    </div>
    <div id="landlord">
        <div class="message" style="opacity:0"></div>
        <canvas id="live2d" width="240" height="250" class="live2d"></canvas>
    </div>
    <div id="codeCopyText" style="display: none">代码复制成功了哦</div>
    <div id="domainname" style="display:none">https://s-hmily.github.io</div>
    </body>
    <script src="https://s-hmily.github.io/media/js/post.js"></script>
    <script>
        //寻找所有code标签，加复制按钮鸭！(行内代码除外)
        var codes = document.getElementsByTagName('code');
        if (codes.length) {
            for (var i = 0; i < codes.length; i++) {
                //高度/行高=文本行数
                // var rowNum=Math.round(codes[i].height()/parseFloat(codes[i].css('line-height')));
                // console.log("当前有"+rowNum+"行");
                var code_id = "code_id_" + i;
                codes[i].setAttribute("id", code_id);
                var ci = "#" + code_id;
                var codedot = $(ci);
                var rowNum = Math.round(codedot.height() / parseFloat(codedot.css('line-height')));
                if (rowNum <= 1) continue;
                var btn = document.createElement("button");
                btn.setAttribute("class", "copybt");
                btn.setAttribute("data-clipboard-target", "#" + code_id);
                btn.innerHTML = '复制代码';
                codes[i].parentNode.insertBefore(btn, codes[i]);
            }
        };
        var cop = new ClipboardJS('.copybt');
        var codeCopyText = $("#codeCopyText").html();
        cop.on('success', function (e) {
            alert(codeCopyText);
            e.clearSelection();
        });
        cop.on('error', function (e) {
            alert("矮油，复制失败了...手动复制吧勇士！");
            e.clearSelection();
        });
    </script>
    
    <script type="text/javascript">
        var message_Path = '/live2d/'
        var home_Path = document.getElementById("domainname").innerHTML+"/"; //此处修改为你的域名，必须带斜杠
    </script>
    <script type="text/javascript" src="https://s-hmily.github.io/media/live2d/js/live2d.js"></script>
    <script type="text/javascript" src="https://s-hmily.github.io/media/live2d/js/message.js"></script>
    <script type="text/javascript">
        loadlive2d("live2d", "https://s-hmily.github.io/media/live2d/assets/tororo.model.json");
    </script>
    
<script>
$(function () {
    $('.toggleContainer').click(function(){$('html,body').animate({scrollTop: '0px'}, 800);});
	$(window).scroll(function() {
        var st = $(window).scrollTop();
        if(st > 30){
            $(".toggleContainer").fadeIn(400);
        }else{
            $(".toggleContainer").fadeOut(100);
        }
	});
});
</script>

<script>
        var bgchoice=$('#bgchoice').html();
        var bg = $('#bg');
        var bgurl = document.getElementById("bgurl").innerHTML;
        if(bgchoice=='default')
            for (var i = 0; i < 3; i++)
                bgurl = bgurl.replace("\\", "/");
        bg.css("background", "url('" + bgurl + "')");
</script>
