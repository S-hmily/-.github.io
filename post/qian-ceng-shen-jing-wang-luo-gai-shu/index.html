<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="1.å‰è¨€
ä¼—æ‰€å‘¨çŸ¥ï¼Œæˆ‘ä»¬ç”Ÿæ´»ä¸­é‡åˆ°çš„å¾ˆå¤šé—®é¢˜éƒ½æ˜¯éçº¿æ€§çš„é—®é¢˜ï¼Œä¾‹å¦‚é¢„æµ‹èŠ±çš„ç”Ÿé•¿æƒ…å†µã€‚é‚£ä¹ˆå¦‚æœæˆ‘ä»¬é‡åˆ°äº†ä¸€äº›éçº¿æ€§é—®é¢˜ï¼Œæˆ‘ä»¬æƒ³è¦å¯¹è¿™ä¸ªéçº¿æ€§é—®é¢˜è¿›è¡Œé¢„æµ‹ä¼°è®¡çš„æ—¶å€™æˆ‘ä»¬éœ€è¦æ€ä¹ˆåŠå‘¢ï¼Ÿ
è¿™é‡Œå°±éœ€è¦å¼•å…¥ç¥ç»ç½‘ç»œçš„æ¦‚å¿µäº†ã€‚
2.ç¥ç»ç½‘ç»œçš„æ¦‚è¿°
æˆ‘ä»¬çŸ¥..." />
    <meta name="keywords" content="æœºå™¨å­¦ä¹ " />
    <link rel="stylesheet" href="https://s-hmily.github.io/media/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://s-hmily.github.io/styles/main.css">
    
    <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/styles/default.min.css">
              
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/highlight.min.js"></script>
    <script src="https://s-hmily.github.io/media/js/clipboard.min.js"></script>
    <link rel="stylesheet" href="https://s-hmily.github.io/media/css/live2d.css">
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>
    <!-- æœ€æ–°ç‰ˆæœ¬çš„ Bootstrap æ ¸å¿ƒ CSS æ–‡ä»¶ -->
    <link href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.13.1/css/all.min.css" rel="stylesheet">
    <!-- æ•°å­¦å…¬å¼ -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css"
        integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js"
        integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm" crossorigin="anonymous">
    </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js"
        integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH" crossorigin="anonymous">
    </script>
    <script>
        renderMathInElement(document.body, {
            delimiters: [{
                    left: "$$",
                    right: "$$",
                    display: true
                },
                {
                    left: "$",
                    right: "$",
                    display: false
                }
            ]
        });
    </script>

    
    <title>å®å®å½“</title>
    
    <style>
        .markdownIt-TOC {
            padding-left: 2px;
            width: 100%;
        }
        .markdownIt-TOC li{
            padding-left: 2%;
        }
    </style>
    
</head>

<body>
    <!-- å“åº”å¼å¸ƒå±€ï¼Œé’ˆå¯¹PCç«¯å†…å®¹æ˜¾ç¤º -->
    <div id="content">
        <div class="nav-large">
            <div class="row">
                <div class="side"><head>
    <meta name="description" content="â€œä½ ä¹°çš„ä»€ä¹ˆä¹¦ï¼Ÿâ€
â€œã€Šè¾¹åŸã€‹â€
â€œC++è¿˜æ˜¯pythonï¼Ÿâ€
â€œæ²ˆä»æ–‡â€" />
    <link rel="stylesheet" href="https://s-hmily.github.io/media/css/bootstrap.min.css">
</head>


<body>
    



    
    <nav class="navbar navbar-inverse navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <a class="navbar-brand" href="https://s-hmily.github.io"
                    style="font-size:21px">å®å®å½“&nbsp;&nbsp;|&nbsp;&nbsp;</a>
                <a class="navbar-brand" href=""
                    style="font-size:15px;font-family:kaiti">â€œä½ ä¹°çš„ä»€ä¹ˆä¹¦ï¼Ÿâ€
â€œã€Šè¾¹åŸã€‹â€
â€œC++è¿˜æ˜¯pythonï¼Ÿâ€
â€œæ²ˆä»æ–‡â€</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse">
                
                <div class="search nav navbar-nav" style="margin-top:8px">
                    <!-- <input type="text" class="search-input" placeholder="æ ‡é¢˜æœç´¢(â—'â—¡'â—)" /> -->
                    <input type="text" class="search-input" placeholder="æ ‡é¢˜æœç´¢ âš†_âš† ã¤â™¡">
                    <div class="search-results"></div>
                </div>
                
                <div class="search nav navbar-nav">
                <a title="text" onclick="document.getElementById('socialMenu').style.display='block'"><i><img class="social"
                    src="https://s-hmily.github.io/media/images/social.png" alt=""></i></a>
            </div>
            <ul class="nav navbar-nav" style="float: right;margin-right:5%">
                
                
                <li>
                    <a href="https://s-hmily.github.io" style="color:white">
                        é¦–é¡µ
                    </a>
                </li>
                
                
                
                <li>
                    <a href="/archives" style="color:white">
                        å½’æ¡£
                    </a>
                </li>
                
                
                
                <li>
                    <a href="https://s-hmily.github.io/tags" style="color:white">
                        æ ‡ç­¾
                    </a>
                </li>
                
                
                
                <li><a href="https://s-hmily.github.io/talk" style="color:white;">è¯´è¯´</a></li>
                
                  
                <li><a href="https://s-hmily.github.io/friends" style="color:white">å‹é“¾</a></li>
                
                  <li><img src="https://s-hmily.github.io/images/avatar.png?v=1599643702527" alt=""
                class="menutopavatar"></li>
            </ul>
        </div><!-- /.navbar-collapse -->
        </div><!-- /.container-fluid -->
    </nav>
    <div id="socialMenu" class="modal">
        <div class="animate">
            <div class="socialContainer">
                
                
                <a onclick="showqq()" style="cursor:pointer"><i><img class="icon" src="https://s-hmily.github.io/media/images/QQ.png"
                            alt=""></i></a>
                
                
                
                
                <a href="LB180928" target="_blank"><i><img class="icon"
                            src="https://s-hmily.github.io/media/images/wechat.png" alt=""></i></a>
                
                
            </div>
            <div id="qq" style="display:none">897438019</div>
        </div>
    </div>
    <!-- å¼•å…¥jQueryæ ¸å¿ƒjsæ–‡ä»¶ -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>
    <script>
        var social = document.getElementById('socialMenu');
        // é¼ æ ‡ç‚¹å‡»æ¨¡å‹å¤–åŒºåŸŸå…³é—­ç™»å½•æ¡†
        window.onclick = function (event) {
            if (event.target == social) {
                social.style.display = "none";
            }
        }
    </script>
    
</body>
<script>
    //-------------------------------------------------æœç´¢
    // è·å–æœç´¢æ¡†ã€æœç´¢æŒ‰é’®ã€æ¸…ç©ºæœç´¢ã€ç»“æœè¾“å‡ºå¯¹åº”çš„å…ƒç´ 
    var searchInput = document.querySelector('.search-input');
    var searchResults = document.querySelector('.search-results');

    // ç”³æ˜ä¿å­˜æ–‡ç« çš„æ ‡é¢˜ã€é“¾æ¥ã€å†…å®¹çš„æ•°ç»„å˜é‡
    var searchValue = '',
        arrItems = [],
        arrLinks = [],
        arrTitles = [],
        arrResults = [],
        indexItem = [],
        itemLength = 0;
    var tmpDiv = document.createElement('div');
    tmpDiv.className = 'result-item';

    // ajax çš„å…¼å®¹å†™æ³•
    var xhr = new XMLHttpRequest() || new ActiveXObject('Microsoft.XMLHTTP');
    xhr.onreadystatechange = function () {
        if (xhr.readyState == 4 && xhr.status == 200) {
            xml = xhr.responseXML;
            arrItems = xml.getElementsByTagName('entry');
            itemLength = arrItems.length;
            // éå†å¹¶ä¿å­˜æ‰€æœ‰æ–‡ç« å¯¹åº”çš„æ ‡é¢˜ã€é“¾æ¥ã€å†…å®¹åˆ°å¯¹åº”çš„æ•°ç»„ä¸­
            // åŒæ—¶è¿‡æ»¤æ‰ HTML æ ‡ç­¾
            for (i = 0; i < itemLength; i++) {
                var link = arrItems[i].getElementsByTagName('link')[0];
                arrLinks[i] = link.getAttribute("href");
                arrTitles[i] = arrItems[i].getElementsByTagName('title')[0].
                childNodes[0].nodeValue.replace(/<.*?>/g, '');
            }
        }
    }

    // å¼€å§‹è·å–æ ¹ç›®å½•ä¸‹ feed.xml æ–‡ä»¶å†…çš„æ•°æ®
    xhr.open('get', '/atom.xml', true);
    xhr.send();



    // è¾“å…¥æ¡†å†…å®¹å˜åŒ–åå°±å¼€å§‹åŒ¹é…ï¼Œå¯ä»¥ä¸ç”¨ç‚¹æŒ‰é’®
    // ç»æµ‹è¯•ï¼Œonkeydown, onchange ç­‰æ–¹æ³•æ•ˆæœä¸å¤ªç†æƒ³ï¼Œ
    // å­˜åœ¨è¾“å…¥å»¶è¿Ÿç­‰é—®é¢˜ï¼Œæœ€åå‘ç°è§¦å‘ input äº‹ä»¶æœ€ç†æƒ³ï¼Œ
    // å¹¶ä¸”å¯ä»¥å¤„ç†ä¸­æ–‡è¾“å…¥æ³•æ‹¼å†™çš„å˜åŒ–
    searchInput.oninput = function () {
        setTimeout(searchConfirm, 0);
    }
    searchInput.onfocus = function () {
        searchResults.style.display = 'block';
    }

    function searchConfirm() {
        if (searchInput.value == '') {
            searchResults.style.display = 'none';
        } else if (searchInput.value.search(/^\s+$/) >= 0) {
            // æ£€æµ‹è¾“å…¥å€¼å…¨æ˜¯ç©ºç™½çš„æƒ…å†µ
            searchInit();
            var itemDiv = tmpDiv.cloneNode(true);
            itemDiv.innerText = 'è¯·è¾“å…¥æœ‰æ•ˆå†…å®¹...';
            searchResults.appendChild(itemDiv);
        } else {
            // åˆæ³•è¾“å…¥å€¼çš„æƒ…å†µ
            searchInit();
            searchValue = searchInput.value;
            // åœ¨æ ‡é¢˜ã€å†…å®¹ä¸­æŸ¥æ‰¾
            searchMatching(arrTitles, searchValue);
        }
    }

    // æ¯æ¬¡æœç´¢å®Œæˆåçš„åˆå§‹åŒ–
    function searchInit() {
        arrResults = [];
        indexItem = [];
        searchResults.innerHTML = '';
        searchResults.style.display = 'block';
    }

    function searchMatching(arr1, input) {
        // å¿½ç•¥è¾“å…¥å¤§å°å†™
        input = new RegExp(input, 'i');
        // åœ¨æ‰€æœ‰æ–‡ç« æ ‡é¢˜ã€å†…å®¹ä¸­åŒ¹é…æŸ¥è¯¢å€¼
        for (i = 0; i < itemLength; i++) {
            if (arr1[i].search(input) !== -1) {
                var arr = arr1;
                indexItem.push(i); // ä¿å­˜åŒ¹é…å€¼çš„ç´¢å¼•
                var indexContent = arr[i].search(input);
                // æ­¤æ—¶ input ä¸º RegExp æ ¼å¼ /input/iï¼Œè½¬æ¢ä¸ºåŸ input å­—ç¬¦ä¸²é•¿åº¦
                var l = input.toString().length - 3;
                var step = 10;

                // å°†åŒ¹é…åˆ°å†…å®¹çš„åœ°æ–¹è¿›è¡Œé»„è‰²æ ‡è®°ï¼Œå¹¶åŒ…æ‹¬å‘¨å›´ä¸€å®šæ•°é‡çš„æ–‡æœ¬
                arrResults.push(arr[i].slice(indexContent - step, indexContent));
            }
        }

        // è¾“å‡ºæ€»å…±åŒ¹é…åˆ°çš„æ•°ç›®
        var totalDiv = tmpDiv.cloneNode(true);
        totalDiv.innerHTML = '<b>æ€»åŒ¹é…ï¼š' + indexItem.length + ' é¡¹<hr></b>';
        searchResults.appendChild(totalDiv);

        // æœªåŒ¹é…åˆ°å†…å®¹çš„æƒ…å†µ
        if (indexItem.length == 0) {
            var itemDiv = tmpDiv.cloneNode(true);
            itemDiv.innerText = 'æœªåŒ¹é…åˆ°å†…å®¹...';
            searchResults.appendChild(itemDiv);
        }

        // å°†æ‰€æœ‰åŒ¹é…å†…å®¹è¿›è¡Œç»„åˆ
        for (i = 0; i < arrResults.length; i++) {
            var itemDiv = tmpDiv.cloneNode(true);
            itemDiv.innerHTML = '<b>[' + arrTitles[indexItem[i]] +
                ']</b><p>' + arrResults[i] + "</p><hr />";
            itemDiv.setAttribute('onclick', 'changeHref(arrLinks[indexItem[' + i + ']])');
            searchResults.appendChild(itemDiv);
        }
    }

    function changeHref(href) {
        location.href = href;
    }

    function showqq() {
        var qq = document.getElementById("qq").innerHTML;
        if (qq != '')
            alert("åšä¸»çš„QQè”ç³»æ–¹å¼ä¸ºï¼š" + qq);
        else
            alert("åšä¸»æš‚æœªè®¾ç½®QQè”ç³»æ–¹å¼");
    }
</script></div>
    
    <div id="main" class="col-xs-12 col-sm-7" style="width:50%;margin-top:50px;left:27%">
        <link rel="stylesheet" href="https://s-hmily.github.io/media/css/font-awesome.css">
<style>

</style>

<body>
    <div class="allcontent" id="postdetail">
        <div class="postshow">
            
            <div class="postdetailimg" style="width:100%;overflow: hidden;display: none">
                <img src="https://s-hmily.github.io/post-images/qian-ceng-shen-jing-wang-luo-gai-shu.jpg" class="postimage" style="cursor:auto">
            </div>
            <div class="postinfo-detail">
                <div class="postdate"><i class="fa fa-calendar"></i>2020-07-18</div>
                <div class="poststatus postdate"><i class="fa fa-clock-o"></i> 33 min read</div>
                <div class="posttag">
                    
                    <a href="https://s-hmily.github.io/tag/lMYeCI_cf/" class="postlink">
                        <i class="fa fa-tag"></i> æœºå™¨å­¦ä¹ 
                    </a>
                    
                </div>
            </div>
            
        <div id="texttitle" style="text-align: center">
            <h2>æµ…å±‚ç¥ç»ç½‘ç»œæ¦‚è¿°å’Œåº”ç”¨</h2>
            <!-- id å°†ä½œä¸ºæŸ¥è¯¢æ¡ä»¶ -->
            <div id="pl" style="display:none">https://s-hmily.github.io/post/qian-ceng-shen-jing-wang-luo-gai-shu/</div>
            <div id="rootaddr" style="display:none">https://s-hmily.github.io</div>
            <span id="hotnum" class="leancloud_visitors" data-flag-title="æµ…å±‚ç¥ç»ç½‘ç»œæ¦‚è¿°å’Œåº”ç”¨">
                <h4 class="readercount">çƒ­åº¦ğŸ”¥: <i class="leancloud-visitors-count">loading...</i></h4>
            </span>
        </div>
        <div class="text ">
            <h2 id="1å‰è¨€">1.å‰è¨€</h2>
<p>ä¼—æ‰€å‘¨çŸ¥ï¼Œæˆ‘ä»¬ç”Ÿæ´»ä¸­é‡åˆ°çš„å¾ˆå¤šé—®é¢˜éƒ½æ˜¯éçº¿æ€§çš„é—®é¢˜ï¼Œä¾‹å¦‚é¢„æµ‹èŠ±çš„ç”Ÿé•¿æƒ…å†µã€‚é‚£ä¹ˆå¦‚æœæˆ‘ä»¬é‡åˆ°äº†ä¸€äº›éçº¿æ€§é—®é¢˜ï¼Œæˆ‘ä»¬æƒ³è¦å¯¹è¿™ä¸ªéçº¿æ€§é—®é¢˜è¿›è¡Œé¢„æµ‹ä¼°è®¡çš„æ—¶å€™æˆ‘ä»¬éœ€è¦æ€ä¹ˆåŠå‘¢ï¼Ÿ</p>
<p>è¿™é‡Œå°±éœ€è¦å¼•å…¥ç¥ç»ç½‘ç»œçš„æ¦‚å¿µäº†ã€‚</p>
<h2 id="2ç¥ç»ç½‘ç»œçš„æ¦‚è¿°">2.ç¥ç»ç½‘ç»œçš„æ¦‚è¿°</h2>
<p>æˆ‘ä»¬çŸ¥é“ï¼Œæˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­å¾ˆå¤šé—®é¢˜ï¼Œç”šè‡³è¯´å¤§å¤šæ•°é—®é¢˜éƒ½ä¸æ˜¯çº¿æ€§å¯åˆ†é—®é¢˜ï¼Œé‚£æˆ‘ä»¬è¦è§£å†³éçº¿æ€§å¯åˆ†é—®é¢˜è¯¥æ€æ ·å¤„ç†å‘¢ï¼Ÿè¿™å°±æ˜¯è¿™éƒ¨åˆ†æˆ‘ä»¬è¦å¼•å‡ºçš„â€œå¤šå±‚â€çš„æ¦‚å¿µã€‚æ—¢ç„¶å•å±‚æ„ŸçŸ¥æœºè§£å†³ä¸äº†éçº¿æ€§é—®é¢˜ï¼Œé‚£æˆ‘ä»¬å°±é‡‡ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼Œä¸‹å›¾å°±æ˜¯ä¸€ä¸ªä¸¤å±‚æ„ŸçŸ¥æœºè§£å†³å¼‚æˆ–é—®é¢˜çš„ç¤ºæ„å›¾ï¼š</p>
<figure data-type="image" tabindex="1"><img src="https://s-hmily.github.io/post-images/1595296483863.png" alt="" loading="lazy"></figure>
<p>æ„å»ºå¥½ä¸Šè¿°ç½‘ç»œä»¥åï¼Œé€šè¿‡è®­ç»ƒå¾—åˆ°æœ€åçš„åˆ†ç±»é¢å¦‚ä¸‹ï¼š</p>
<figure data-type="image" tabindex="2"><img src="https://s-hmily.github.io/post-images/1595296506152.png" alt="" loading="lazy"></figure>
<p>ç”±æ­¤å¯è§ï¼Œå¤šå±‚æ„ŸçŸ¥æœºå¯ä»¥å¾ˆå¥½çš„è§£å†³éçº¿æ€§å¯åˆ†é—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸å°†å¤šå±‚æ„ŸçŸ¥æœºè¿™æ ·çš„å¤šå±‚ç»“æ„ç§°ä¹‹ä¸ºæ˜¯ç¥ç»ç½‘ç»œã€‚ä½†æ˜¯ï¼Œå¤šå±‚æ„ŸçŸ¥æœºè™½ç„¶å¯ä»¥åœ¨ç†è®ºä¸Šå¯ä»¥è§£å†³éçº¿æ€§é—®é¢˜ï¼Œä½†æ˜¯å®é™…ç”Ÿæ´»ä¸­é—®é¢˜çš„å¤æ‚æ€§è¦è¿œä¸æ­¢å¼‚æˆ–é—®é¢˜è¿™ä¹ˆç®€å•ï¼Œæ‰€ä»¥æˆ‘ä»¬å¾€å¾€è¦æ„å»ºå¤šå±‚ç½‘ç»œï¼Œè€Œå¯¹äºå¤šå±‚ç¥ç»ç½‘ç»œé‡‡ç”¨ä»€ä¹ˆæ ·çš„å­¦ä¹ ç®—æ³•åˆæ˜¯ä¸€é¡¹å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºçš„å…·æœ‰4å±‚éšå«å±‚çš„ç½‘ç»œç»“æ„ä¸­è‡³å°‘æœ‰33ä¸ªå‚æ•°ï¼ˆä¸è®¡åç½®biaså‚æ•°ï¼‰ï¼Œæˆ‘ä»¬åº”è¯¥å¦‚ä½•å»ç¡®å®šå‘¢ï¼Ÿ</p>
<figure data-type="image" tabindex="3"><img src="https://s-hmily.github.io/post-images/1595296520486.png" alt="" loading="lazy"></figure>
<p>æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸ªå›¾ï¼š</p>
<figure data-type="image" tabindex="4"><img src="https://s-hmily.github.io/post-images/1595296534309.png" alt="" loading="lazy"></figure>
<p>è¿™æ˜¯å…¸å‹çš„ä¸‰å±‚ç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»„æˆï¼ŒLayer L1æ˜¯è¾“å…¥å±‚ï¼ŒLayer L2æ˜¯éšå«å±‚ï¼ŒLayer L3æ˜¯éšå«å±‚ï¼Œæˆ‘ä»¬ç°åœ¨æ‰‹é‡Œæœ‰ä¸€å †æ•°æ®{x1,x2,x3,...,xn},è¾“å‡ºä¹Ÿæ˜¯ä¸€å †æ•°æ®{y1,y2,y3,...,yn},ç°åœ¨è¦ä»–ä»¬åœ¨éšå«å±‚åšæŸç§å˜æ¢ï¼Œè®©ä½ æŠŠæ•°æ®çŒè¿›å»åå¾—åˆ°ä½ æœŸæœ›çš„è¾“å‡ºã€‚å¦‚æœä½ å¸Œæœ›ä½ çš„è¾“å‡ºå’ŒåŸå§‹è¾“å…¥ä¸€æ ·ï¼Œé‚£ä¹ˆå°±æ˜¯æœ€å¸¸è§çš„è‡ªç¼–ç æ¨¡å‹ï¼ˆAuto-Encoderï¼‰ã€‚å¯èƒ½æœ‰äººä¼šé—®ï¼Œä¸ºä»€ä¹ˆè¦è¾“å…¥è¾“å‡ºéƒ½ä¸€æ ·å‘¢ï¼Ÿæœ‰ä»€ä¹ˆç”¨å•Šï¼Ÿå…¶å®åº”ç”¨æŒºå¹¿çš„ï¼Œåœ¨å›¾åƒè¯†åˆ«ï¼Œæ–‡æœ¬åˆ†ç±»ç­‰ç­‰éƒ½ä¼šç”¨åˆ°ï¼ŒåŒ…æ‹¬ä¸€äº›å˜ç§ä¹‹ç±»çš„ã€‚å¦‚æœä½ çš„è¾“å‡ºå’ŒåŸå§‹è¾“å…¥ä¸ä¸€æ ·ï¼Œé‚£ä¹ˆå°±æ˜¯å¾ˆå¸¸è§çš„äººå·¥ç¥ç»ç½‘ç»œäº†ï¼Œç›¸å½“äºè®©åŸå§‹æ•°æ®é€šè¿‡ä¸€ä¸ªæ˜ å°„æ¥å¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„è¾“å‡ºæ•°æ®ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬ä»Šå¤©è¦è®²çš„è¯é¢˜ã€‚</p>
<h2 id="3ç¥ç»ç½‘ç»œçš„å®ç°è¿‡ç¨‹åˆ©ç”¨ç¥ç»ç½‘ç»œåˆ†ç±»äºŒç»´æ•°æ®é›†">3.ç¥ç»ç½‘ç»œçš„å®ç°è¿‡ç¨‹ï¼ˆåˆ©ç”¨ç¥ç»ç½‘ç»œåˆ†ç±»äºŒç»´æ•°æ®é›†ï¼‰</h2>
<p><strong>æç¤º</strong>ï¼šå»ºç«‹ç¥ç»ç½‘ç»œçš„ä¸€èˆ¬æ–¹æ³•ï¼š</p>
<p>1.å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„ï¼ˆè¾“å…¥å•å…ƒæ•°ï¼Œéšè—å•å…ƒæ•°ç­‰ï¼‰.</p>
<p>2.åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°</p>
<p>3.å¾ªç¯ï¼š</p>
<ul>
<li>
<p>å®ç°å‰å‘ä¼ æ’­</p>
</li>
<li>
<p>è®¡ç®—æŸå¤±</p>
</li>
<li>
<p>åå‘ä¼ æ’­ä»¥è·å¾—æ¢¯åº¦</p>
</li>
<li>
<p>æ›´æ–°å‚æ•°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰</p>
</li>
<li>
<p>æˆ‘ä»¬é€šå¸¸ä¼šæ„å»ºè¾…åŠ©å‡½æ•°æ¥è®¡ç®—ç¬¬1-3æ­¥ï¼Œç„¶åå°†å®ƒä»¬åˆå¹¶ä¸º<code>nn_model()</code>å‡½æ•°ã€‚ä¸€æ—¦æ„å»ºäº†<code>nn_model()</code>å¹¶å­¦ä¹ äº†æ­£ç¡®çš„å‚æ•°ï¼Œå°±å¯ä»¥å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚</p>
<h3 id="31å®‰è£…åŒ…">3.1å®‰è£…åŒ…</h3>
<p>è®©æˆ‘ä»¬é¦–å…ˆå¯¼å…¥åœ¨ä½œä¸šè¿‡ç¨‹ä¸­éœ€è¦çš„æ‰€æœ‰è½¯ä»¶åŒ…ã€‚</p>
<ul>
<li>
<p><a href="https://www.kesci.com/api/notebooks/5e85d6bf95b029002ca7e7e6/www.numpy.org">numpy</a>æ˜¯Pythonç§‘å­¦è®¡ç®—çš„åŸºæœ¬åŒ…ã€‚</p>
</li>
<li>
<p><a href="http://scikit-learn.org/stable/">sklearn</a>æä¾›äº†ç”¨äºæ•°æ®æŒ–æ˜å’Œåˆ†æçš„ç®€å•æœ‰æ•ˆçš„å·¥å…·ã€‚</p>
</li>
<li>
<p><a href="http://matplotlib.org/">matplotlib</a> æ˜¯åœ¨Pythonä¸­å¸¸ç”¨çš„ç»˜åˆ¶å›¾å½¢çš„åº“ã€‚</p>
</li>
<li>
<p>testCasesæä¾›äº†ä¸€äº›æµ‹è¯•ç¤ºä¾‹ç”¨ä»¥è¯„ä¼°å‡½æ•°çš„æ­£ç¡®æ€§</p>
</li>
<li>
<p>planar_utilsæä¾›äº†æ­¤ä½œä¸šä¸­ä½¿ç”¨çš„å„ç§å‡½æ•°</p>
<pre><code># Package imports
import numpy as np
import matplotlib.pyplot as plt
from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

%matplotlib inline

np.random.seed(1) # set a seed so that the results are consistent
</code></pre>
</li>
</ul>
<h3 id="32-æ•°æ®é›†">3.2 æ•°æ®é›†</h3>
<pre><code>X, Y = load_planar_dataset() 
# Visualize the data:
plt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0,:].shape), s=40, cmap=plt.cm.Spectral)
</code></pre>
<p>å…¶ä¸­Xæ˜¯èŠ±ç“£çš„ç‚¹ï¼Œè¡¨ç¤ºç‰¹å¾çŸ©é˜µï¼ˆX1,X2ï¼‰è¿™ä¸ªXæ•°æ®é›†å¯ä»¥åœ¨åè¾¹ç”¨X.shapeå¾—åˆ°è¿™ä¸ªXæ˜¯2è¡Œ400åˆ—çš„</p>
<p>Yæ•°æ®é›†æ˜¯1è¡Œ400åˆ—çš„ï¼Œè¡¨ç¤ºæ ‡ç­¾(çº¢è‰²ï¼š0ï¼Œè“è‰²ï¼š1)</p>
<p>è·å¾—numpyæ•°ç»„çš„shapeç»´åº¦ä»£ç </p>
<pre><code>### START CODE HERE ### (â‰ˆ 3 lines of code)
shape_X = X.shape
shape_Y = Y.shape

m = shape_X[1]  # training set size
### END CODE HERE ###

print ('The shape of X is: ' + str(shape_X))
print ('The shape of Y is: ' + str(shape_Y))
print ('I have m = %d training examples!' % (m))
</code></pre>
<p>ç»“æœï¼š</p>
<p>The shape of X is: (2, 400)</p>
<p>The shape of Y is: (1, 400)</p>
<p>I have m = 400 training examples!</p>
<h3 id="33å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„">3.3å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„</h3>
<h4 id="331å®šä¹‰å˜é‡">3.3.1å®šä¹‰å˜é‡</h4>
<p><strong>ç›®æ ‡</strong>ï¼šå®šä¹‰ä¸‰ä¸ªå˜é‡     n_x:è¾“å…¥å±‚å¤§å°  n_hï¼šéšè—å±‚å¤§å°(è¿™é‡Œè®¾ç½®æˆ4)   n_y:è¾“å‡ºå±‚å¤§å°</p>
<p><strong>æç¤º</strong>ï¼šä½¿ç”¨shapeå‡½æ•°æ¥æ‰¾åˆ°n_xå’Œn_yã€‚å¦å¤– å°†éšè—å±‚å¤§å°ç¡¬ç¼–ç ä¸º4</p>
<pre><code># GRADED FUNCTION: layer_sizes

def layer_sizes(X, Y):
    &quot;&quot;&quot;
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    &quot;&quot;&quot;
    ### START CODE HERE ### (â‰ˆ 3 lines of code)
    n_x = X.shape[0] # size of input layer
    n_h = 4
    n_y = Y.shape[0] # size of output layer
    ### END CODE HERE ###
    return (n_x, n_h, n_y)
</code></pre>
<p>â€‹    å¯¼å…¥ä¸€ç»„æ•°æ®è¿›è¡Œæµ‹è¯•ï¼š</p>
<pre><code>X_assess, Y_assess = layer_sizes_test_case()
(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)
print(&quot;The size of the input layer is: n_x = &quot; + str(n_x))
print(&quot;The size of the hidden layer is: n_h = &quot; + str(n_h))
print(&quot;The size of the output layer is: n_y = &quot; + str(n_y))
</code></pre>
<p>ç»“æœï¼š</p>
<p>The size of the input layer is: n_x = 5</p>
<p>The size of the hidden layer is: n_h = 4</p>
<p>The size of the output layer is: n_y = 2</p>
</li>
</ul>
<p><strong>é¢„æœŸè¾“å‡º</strong>ï¼ˆä»…ç”¨äºè¯„ä¼°åˆšåˆšç¼–ç çš„å‡½æ•°ï¼Œå¹¶ä¸ä»£è¡¨å®é™…ç½‘ç»œå¤§å°ï¼‰ã€‚ è¾“å…¥å±‚çš„å¤§å°ä¸ºï¼šn_x = 5 éšè—å±‚çš„å¤§å°ä¸ºï¼šn_h = 4 è¾“å‡ºå±‚çš„å¤§å°ä¸ºï¼šn_y = 2</p>
<p>å¯¹äºæˆ‘ä»¬è¿™ä¸ªæ•´ä¸ªé¢˜æ¥è®²ï¼Œåº”è¯¥æ˜¯n_x = 2 n_h = 4 n_y = 21</p>
<h4 id="332-åˆå§‹åŒ–æ¨¡å‹å‚æ•°">3.3.2 åˆå§‹åŒ–æ¨¡å‹å‚æ•°</h4>
<p><strong>ç›®æ ‡</strong>ï¼šå®ç°å‡½æ•°Â <code>initialize_parameters()</code>ã€‚</p>
<p><strong>æç¤ºï¼š</strong></p>
<ul>
<li>è¯·ç¡®ä¿å‚æ•°å¤§å°æ­£ç¡®ã€‚ å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯å‚è€ƒä¸Šé¢çš„ç¥ç»ç½‘ç»œå›¾ã€‚</li>
<li>ä½¿ç”¨éšæœºå€¼åˆå§‹åŒ–æƒé‡çŸ©é˜µã€‚      - ä½¿ç”¨ï¼š<code>np.random.randnï¼ˆaï¼Œbï¼‰* 0.01</code>éšæœºåˆå§‹åŒ–ç»´åº¦ä¸ºï¼ˆaï¼Œbï¼‰çš„çŸ©é˜µã€‚</li>
<li>å°†åå·®å‘é‡åˆå§‹åŒ–ä¸ºé›¶ã€‚      - ä½¿ç”¨ï¼š<code>np.zeros((a,b))</code> åˆå§‹åŒ–ç»´åº¦ä¸ºï¼ˆaï¼Œbï¼‰é›¶çš„çŸ©é˜µã€‚</li>
</ul>
<pre><code>#åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°
def initialize_parameters(n_x,n_h,n_y):
    '''
    å‚æ•°ï¼š
    n_x:è¾“å…¥å±‚ç»“ç‚¹çš„æ•°é‡
    n_h:éšè—å±‚ç»“ç‚¹çš„æ•°é‡
    n_y:è¾“å‡ºå±‚ç»“ç‚¹çš„æ•°é‡
    è¿”å›ï¼š
    paraneters:åŒ…å«å‚æ•°çš„å­—å…¸
    W1:æƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º(n_h,n_x)
    b1:åå‘é‡ï¼Œç»´åº¦ä¸º(n_h,1)
    W2ï¼šæƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º(n_y,n_h)
    b2:åå‘é‡ï¼Œç»´åº¦ä¸º(n_y,1)
    '''
    #é€šè¿‡æœ¬å‡½æ•°å¯ä»¥è¿”å›ä¸€ä¸ªæˆ–ä¸€ç»„æœä»â€œ0~1â€å‡åŒ€åˆ†å¸ƒçš„éšæœºæ ·æœ¬å€¼ã€‚éšæœºæ ·æœ¬å–å€¼èŒƒå›´æ˜¯[0,1)ï¼Œä¸åŒ…æ‹¬1ã€‚
    np.random.seed(2)#æŒ‡å®šä¸€ä¸ªéšæœºç§å­
    W1=np.random.rand(n_h,n_x)*0.01
    #zeros ç”Ÿæˆ0çŸ©é˜µ
    b1=np.zeros(shape=(n_h,1))
    W2=np.random.rand(n_y,n_h)
    b2=np.random.rand(n_y,1)
    #ä½¿ç”¨æ–­è¨€ç¡®ä¿æˆ‘çš„æ•°æ®æ ¼å¼æ˜¯æ­£ç¡®çš„
    #assertæ˜¯ç”¨äºå¯¹ç¨‹åºè¿›è¡Œè°ƒè¯•çš„ï¼Œå¯¹äºæ‰§è¡Œç»“æ„çš„åˆ¤æ–­ï¼Œè€Œä¸æ˜¯å¯¹äºä¸šåŠ¡æµç¨‹çš„åˆ¤æ–­ã€‚ï¼ˆç›¸å½“äºä¸€ä¸ªif ()è¯­å¥ï¼Œå¦‚æœæ»¡è¶³æ–­è¨€çš„æ‰§è¡Œç¨‹åºï¼Œå¦‚æœä¸æ»¡è¶³åˆ™æŠ›é”™è¯¯
    assert(W1.shape==(n_h,n_x))
    assert(b1.shape==(n_h,1))
    assert(W2.shape==(n_y,n_h))
    assert(b2.shape==(n_y,1))
    parameters={
        &quot;W1&quot;:W1,
        &quot;b1&quot;:b1,
        &quot;W2&quot;:W2,
        &quot;b2&quot;:b2
    }
    return parameters
</code></pre>
<p>æµ‹è¯•ä»£ç ï¼š</p>
<pre><code>n_x, n_h, n_y = initialize_parameters_test_case()

parameters = initialize_parameters(n_x, n_h, n_y)
print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>ç»“æœï¼šW1 = [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]
b2 = [[0.]]
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: W1 = [[-0.00416758 -0.00056267][-0.02136196 0.01640271] [-0.01793436 -0.00841747][ 0.00502881 -0.01245288]] b1 = [[0.][0.] [0.][0.]] W2 = [[-0.01057952 -0.00909008 0.00551454 0.02292208]] b2 = [[0.]]</p>
<h4 id="333-å¾ªç¯">3.3.3 å¾ªç¯</h4>
<p><strong>ç›®æ ‡</strong>ï¼šå®ç°<code>forward_propagationï¼ˆï¼‰</code>ï¼ˆå®ç°æ­£å‘ä¼ æ’­ï¼‰ã€‚</p>
<p>æç¤ºï¼š</p>
<ul>
<li>
<p>åœ¨ä¸Šæ–¹æŸ¥çœ‹åˆ†ç±»å™¨çš„æ•°å­¦è¡¨ç¤ºå½¢å¼ã€‚</p>
</li>
<li>
<p>ä½ å¯ä»¥ä½¿ç”¨å†…ç½®åœ¨ç¬”è®°æœ¬ä¸­çš„<code>sigmoid()</code>å‡½æ•°ã€‚</p>
</li>
<li>
<p>ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨numpyåº“ä¸­çš„<code>np.tanhï¼ˆï¼‰</code>å‡½æ•°ã€‚</p>
</li>
<li>
<p>å¿…é¡»æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š      1.ä½¿ç”¨<code>parameters [â€œ ..â€]</code>ä»å­—å…¸â€œ parametersâ€ï¼ˆè¿™æ˜¯<code>initialize_parametersï¼ˆï¼‰</code>çš„è¾“å‡ºï¼‰ä¸­æ£€ç´¢å‡ºæ¯ä¸ªå‚æ•°ã€‚      2.å®ç°æ­£å‘ä¼ æ’­ï¼Œè®¡ç®—Z[1],A[1],Z[2] å’Œ A[2] ï¼ˆæ‰€æœ‰è®­ç»ƒæ•°æ®çš„é¢„æµ‹ç»“æœå‘é‡ï¼‰ã€‚</p>
</li>
<li>
<p>å‘åä¼ æ’­æ‰€éœ€çš„å€¼å­˜å‚¨åœ¨<code>cache</code>ä¸­ï¼Œ <code>cache</code>å°†ä½œä¸ºåå‘ä¼ æ’­å‡½æ•°çš„è¾“å…¥ã€‚</p>
<p>ä»£ç ï¼š</p>
<pre><code># GRADED FUNCTION: forward_propagation

def forward_propagation(X, parameters):
    &quot;&quot;&quot;
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)
    
    Returns:
    A2 -- The sigmoid output of the second activation
    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;
    &quot;&quot;&quot;
    # Retrieve each parameter from the dictionary &quot;parameters&quot;
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Implement Forward Propagation to calculate A2 (probabilities)
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    Z1 = np.dot(W1,X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2,A1) + b2
    A2 = sigmoid(Z2)
    ### END CODE HERE ###
    
    assert(A2.shape == (1, X.shape[1]))
    
    cache = {&quot;Z1&quot;: Z1,
             &quot;A1&quot;: A1,
             &quot;Z2&quot;: Z2,
             &quot;A2&quot;: A2}
    
    return A2, cache
</code></pre>
</li>
</ul>
<p>æµ‹è¯•ä»£ç ï¼š</p>
<pre><code>X_assess, parameters = forward_propagation_test_case()

A2, cache = forward_propagation(X_assess, parameters)

# Note: we use the mean here just to make sure that your output matches ours. 
print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
</code></pre>
<pre><code>-0.0004997557777419913 -0.000496963353231779 0.00043818745095914653 0.500109546852431
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: -0.0004997557777419913  -0.000496963353231779 0.00043818745095914653        0.500109546852431</p>
<p>ç°åœ¨ï¼Œä½ å·²ç»è®¡ç®—äº†åŒ…å«æ¯ä¸ªç¤ºä¾‹çš„a<a href="i">2</a>Â çš„Â A[2]Â ï¼ˆåœ¨Pythonå˜é‡â€œ<code>A2</code>â€ä¸­ï¼‰ï¼Œå…¶ä¸­ï¼Œä½ å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•° å¦‚ä¸‹ï¼š</p>
<figure data-type="image" tabindex="5"><img src="https://s-hmily.github.io/post-images/1595296602244.png" alt="" loading="lazy"></figure>
<p><strong>ç›®çš„</strong>ï¼šå®ç°<code>compute_costï¼ˆï¼‰</code>ä»¥è®¡ç®—æŸå¤±Jçš„å€¼ã€‚</p>
<p>æœ‰å¾ˆå¤šç§æ–¹æ³•å¯ä»¥å®ç°äº¤å‰ç†µæŸå¤±ã€‚ æˆ‘ä»¬ä¸ºä½ æä¾›äº†å®ç°æ–¹æ³• ï¼š</p>
<figure data-type="image" tabindex="6"><img src="https://s-hmily.github.io/post-images/1595296616699.png" alt="" loading="lazy"></figure>
<pre><code>logprobs = np.multiply(np.log(A2),Y)
cost = - np.sum(logprobs)   
</code></pre>
<p>ï¼ˆä½ ä¹Ÿå¯ä»¥ä½¿ç”¨np.multiply()ç„¶åä½¿ç”¨np.sum()æˆ–ç›´æ¥ä½¿ç”¨np.dot()ï¼‰ã€‚</p>
<pre><code># GRADED FUNCTION: compute_cost

def compute_cost(A2, Y, parameters):
    &quot;&quot;&quot;
    Computes the cross-entropy cost given in equation (13)
    
    Arguments:
    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    Y -- &quot;true&quot; labels vector of shape (1, number of examples)
    parameters -- python dictionary containing your parameters W1, b1, W2 and b2
    
    Returns:
    cost -- cross-entropy cost given equation (13)
    &quot;&quot;&quot;
    
    m = Y.shape[1] # number of example

    # Compute the cross-entropy cost
     ### START CODE HERE ### (â‰ˆ 2 lines of code)
    logprobs = Y*np.log(A2) + (1-Y)* np.log(1-A2)
    cost = -1/m * np.sum(logprobs)
    ### END CODE HERE ###
    
    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. 
                                # E.g., turns [[17]] into 17 
    assert(isinstance(cost, float))
    
    return cost
</code></pre>
<p><strong>æµ‹è¯•</strong>ï¼š</p>
<pre><code>A2, Y_assess, parameters = compute_cost_test_case()

print(&quot;cost = &quot; + str(compute_cost(A2, Y_assess, parameters)))
</code></pre>
<pre><code>cost = 0.6929198937761265
</code></pre>
<p>ç°åœ¨ï¼Œé€šè¿‡ä½¿ç”¨åœ¨æ­£å‘ä¼ æ’­æœŸé—´è®¡ç®—çš„ç¼“å­˜ï¼Œä½ å¯ä»¥å®ç°åå‘ä¼ æ’­ã€‚</p>
<p><strong>é—®é¢˜</strong>ï¼šå®ç°å‡½æ•°<code>backward_propagationï¼ˆï¼‰</code>ã€‚</p>
<p><strong>è¯´æ˜</strong>ï¼š åå‘ä¼ æ’­é€šå¸¸æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€éš¾ï¼ˆæœ€æ•°å­¦ï¼‰çš„éƒ¨åˆ†ã€‚ä¸ºäº†å¸®åŠ©ä½ æ›´å¥½åœ°äº†è§£ï¼Œæˆ‘ä»¬æä¾›äº†åå‘ä¼ æ’­è¯¾ç¨‹çš„å¹»ç¯ç‰‡ã€‚ä½ å°†è¦ä½¿ç”¨æ­¤å¹»ç¯ç‰‡å³ä¾§çš„å…­ä¸ªæ–¹ç¨‹å¼ä»¥æ„å»ºå‘é‡åŒ–å®ç°ã€‚</p>
<figure data-type="image" tabindex="7"><img src="https://cdn.kesci.com/upload/image/q17hcd4yra.png?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<figure data-type="image" tabindex="8"><img src="https://s-hmily.github.io/post-images/1595296633924.png" alt="" loading="lazy"></figure>
<pre><code># GRADED FUNCTION: backward_propagation

def backward_propagation(parameters, cache, X, Y):
    &quot;&quot;&quot;
    Implement the backward propagation using the instructions above.
    
    Arguments:
    parameters -- python dictionary containing our parameters 
    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;.
    X -- input data of shape (2, number of examples)
    Y -- &quot;true&quot; labels vector of shape (1, number of examples)
    
    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    &quot;&quot;&quot;
    m = X.shape[1]
    
    # First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    W1 = parameters[&quot;W1&quot;]
    W2 = parameters[&quot;W2&quot;]
    ### END CODE HERE ###
        
    # Retrieve also A1 and A2 from dictionary &quot;cache&quot;.
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    A1 = cache[&quot;A1&quot;]
    A2 = cache[&quot;A2&quot;]
    ### END CODE HERE ###
    
    # Backward propagation: calculate dW1, db1, dW2, db2. 
    ### START CODE HERE ### (â‰ˆ 6 lines of code, corresponding to 6 equations on slide above)
    dZ2= A2 - Y
    dW2 = 1 / m * np.dot(dZ2,A1.T)
    db2 = 1 / m * np.sum(dZ2,axis=1,keepdims=True)
    dZ1 = np.dot(W2.T,dZ2) * (1-np.power(A1,2))
    dW1 = 1 / m * np.dot(dZ1,X.T)
    db1 = 1 / m * np.sum(dZ1,axis=1,keepdims=True)
    ### END CODE HERE ###
    
    grads = {&quot;dW1&quot;: dW1,
             &quot;db1&quot;: db1,
             &quot;dW2&quot;: dW2,
             &quot;db2&quot;: db2}
    
    return grads
</code></pre>
<p>In [16]:</p>
<pre><code>parameters, cache, X_assess, Y_assess = backward_propagation_test_case()

grads = backward_propagation(parameters, cache, X_assess, Y_assess)
print (&quot;dW1 = &quot;+ str(grads[&quot;dW1&quot;]))
print (&quot;db1 = &quot;+ str(grads[&quot;db1&quot;]))
print (&quot;dW2 = &quot;+ str(grads[&quot;dW2&quot;]))
print (&quot;db2 = &quot;+ str(grads[&quot;db2&quot;]))
</code></pre>
<pre><code>dW1 = [[ 0.01018708 -0.00708701]
 [ 0.00873447 -0.0060768 ]
 [-0.00530847  0.00369379]
 [-0.02206365  0.01535126]]
db1 = [[-0.00069728]
 [-0.00060606]
 [ 0.000364  ]
 [ 0.00151207]]
dW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]
db2 = [[0.06589489]]
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: dW1 = [[ 0.01018708 -0.00708701] [ 0.00873447 -0.0060768 ] [-0.00530847 0.00369379] [-0.02206365 0.01535126]] db1 = [[-0.00069728] [-0.00060606] [ 0.000364 ] [ 0.00151207]] dW2 = [[ 0.00363613 0.03153604 0.01162914 -0.01318316]] db2 = [[0.06589489]]</p>
<p><strong>é—®é¢˜</strong>ï¼šå®ç°å‚æ•°æ›´æ–°ã€‚ ä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼Œä½ å¿…é¡»ä½¿ç”¨ï¼ˆdW1ï¼Œdb1ï¼ŒdW2ï¼Œdb2ï¼‰æ‰èƒ½æ›´æ–°ï¼ˆW1ï¼Œb1ï¼ŒW2ï¼Œb2ï¼‰ã€‚</p>
<p><strong>ä¸€èˆ¬çš„æ¢¯åº¦ä¸‹é™è§„åˆ™</strong>ï¼šÎ¸=Î¸âˆ’Î±âˆ‚Jâˆ‚Î¸å…¶ä¸­Î±æ˜¯å­¦ä¹ ç‡ï¼Œè€ŒÎ¸ ä»£è¡¨ä¸€ä¸ªå‚æ•°ã€‚</p>
<p><strong>å›¾ç¤º</strong>ï¼šå…·æœ‰è‰¯å¥½çš„å­¦ä¹ é€Ÿç‡ï¼ˆæ”¶æ•›ï¼‰å’Œè¾ƒå·®çš„å­¦ä¹ é€Ÿç‡ï¼ˆå‘æ•£ï¼‰çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚ å›¾ç‰‡ç”±Adam Harleyæä¾›ã€‚</p>
<figure data-type="image" tabindex="9"><img src="https://cdn.kesci.com/upload/image/q17hh4otzu.gif?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<figure data-type="image" tabindex="10"><img src="https://cdn.kesci.com/upload/image/q17hharbth.gif?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<pre><code># GRADED FUNCTION: update_parameters

def update_parameters(parameters, grads, learning_rate = 1.2):
    &quot;&quot;&quot;
    Updates parameters using the gradient descent update rule given above
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients 
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    &quot;&quot;&quot;
    # Retrieve each parameter from the dictionary &quot;parameters&quot;
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Retrieve each gradient from the dictionary &quot;grads&quot;
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    dW1 = grads[&quot;dW1&quot;]
    db1 = grads[&quot;db1&quot;]
    dW2 = grads[&quot;dW2&quot;]
    db2 = grads[&quot;db2&quot;]
    ## END CODE HERE ###
    
    # Update rule for each parameter
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    ### END CODE HERE ###
    
    parameters = {&quot;W1&quot;: W1,
                  &quot;b1&quot;: b1,
                  &quot;W2&quot;: W2,
                  &quot;b2&quot;: b2}
    
    return parameters
</code></pre>
<p>æµ‹è¯•ï¼š</p>
<pre><code>parameters, grads = update_parameters_test_case()
parameters = update_parameters(parameters, grads)

print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>ç»“æœï¼š
W1 = [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]
b1 = [[-1.02420756e-06]
 [ 1.27373948e-05]
 [ 8.32996807e-07]
 [-3.20136836e-06]]
W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]
b2 = [[0.00010457]]
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: dW1 = [[ 0.01018708 -0.00708701] [ 0.00873447 -0.0060768 ] [-0.00530847 0.00369379] [-0.02206365 0.01535126]] db1 = [[-0.00069728] [-0.00060606] [ 0.000364 ] [ 0.00151207]] dW2 = [[ 0.00363613 0.03153604 0.01162914 -0.01318316]] db2 = [[0.06589489]]</p>
<h4 id="334-åœ¨nn_modelä¸­é›†æˆ331-332å’Œ333éƒ¨åˆ†ä¸­çš„å‡½æ•°">3.3.4- åœ¨nn_modelï¼ˆï¼‰ä¸­é›†æˆ3.3.1ã€3.3.2å’Œ3.3.3éƒ¨åˆ†ä¸­çš„å‡½æ•°</h4>
<p><strong>é—®é¢˜</strong>ï¼šåœ¨nn_modelï¼ˆï¼‰ä¸­å»ºç«‹ä½ çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚</p>
<p><strong>è¯´æ˜</strong>ï¼šç¥ç»ç½‘ç»œæ¨¡å‹å¿…é¡»ä»¥æ­£ç¡®çš„é¡ºåºç»„åˆå…ˆå‰æ„å»ºçš„å‡½æ•°ã€‚</p>
<p>In [19]:</p>
<pre><code># GRADED FUNCTION: nn_model

def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    &quot;&quot;&quot;
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    &quot;&quot;&quot;
    
    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.
    ### START CODE HERE ### (â‰ˆ 5 lines of code)
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Loop (gradient descent)

    for i in range(0, num_iterations):
         
        ### START CODE HERE ### (â‰ˆ 4 lines of code)
        # Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.
        A2, cache = forward_propagation(X, parameters)
        
        # Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.
        cost = compute_cost(A2, Y, parameters)
 
        # Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.
        grads = backward_propagation(parameters, cache, X, Y)
 
        # Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.
        parameters = update_parameters(parameters, grads)
        
        ### END CODE HERE ###
        
        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))

    return parameters
</code></pre>
<p>æµ‹è¯•</p>
<pre><code>X_assess, Y_assess = nn_model_test_case()

parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=False)
print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log
/home/kesci/work/planar_utils.py:34: RuntimeWarning: overflow encountered in exp
  s = 1/(1+np.exp(-x))
</code></pre>
<pre><code>ç»“æœï¼š
W1 = [[-4.18503197  5.33214315]
 [-7.52988635  1.24306559]
 [-4.19302427  5.32627154]
 [ 7.52984762 -1.24308746]]
b1 = [[ 2.32926944]
 [ 3.79460252]
 [ 2.33002498]
 [-3.79466751]]
W2 = [[-6033.83668723 -6008.12983227 -6033.10091631  6008.06624417]]
b2 = [[-52.66610924]]
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: W1 = [[-4.18503197 5.33214315] [-7.52988635 1.24306559] [-4.19302427 5.32627154] [ 7.52984762 -1.24308746]] b1 = [[ 2.32926944] [ 3.79460252] [ 2.33002498] [-3.79466751]] W2 = [[-6033.83668723 -6008.12983227 -6033.10091631 6008.06624417]] b2 = [[-52.66610924]]</p>
<h3 id="45-é¢„æµ‹">4.5- é¢„æµ‹</h3>
<p><strong>é—®é¢˜</strong>ï¼šä½¿ç”¨ä½ çš„æ¨¡å‹é€šè¿‡æ„å»ºpredict()å‡½æ•°è¿›è¡Œé¢„æµ‹ã€‚ ä½¿ç”¨æ­£å‘ä¼ æ’­æ¥é¢„æµ‹ç»“æœã€‚</p>
<p><strong>æç¤º</strong>ï¼špredictions = yprediction=1{activation &gt; 0.5}={1if activation&gt;0.50otherwise<br>
ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³åŸºäºé˜ˆå€¼å°†çŸ©é˜µXè®¾ä¸º0å’Œ1ï¼Œåˆ™å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š <code>X_new = (X &gt; threshold)</code></p>
<pre><code># GRADED FUNCTION: predict

def predict(parameters, X):
    &quot;&quot;&quot;
    Using the learned parameters, predicts a class for each example in X
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    X -- input data of size (n_x, m)
    
    Returns
    predictions -- vector of predictions of our model (red: 0 / blue: 1)
    &quot;&quot;&quot;
    
    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.
  ### START CODE HERE ### (â‰ˆ 2 lines of code)
    A2, cache = forward_propagation(X, parameters)
    predictions = np.round(A2)
    ### END CODE HERE ###
    
    return predictions
</code></pre>
<p>æµ‹è¯•ï¼š</p>
<pre><code>parameters, X_assess = predict_test_case()

predictions = predict(parameters, X_assess)
print(&quot;predictions mean = &quot; + str(np.mean(predictions)))
</code></pre>
<pre><code>predictions mean = 0.6666666666666666
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: predictions mean = 0.6666666666666666</p>
<p>ç°åœ¨è¿è¡Œæ¨¡å‹ä»¥æŸ¥çœ‹å…¶å¦‚ä½•åœ¨äºŒç»´æ•°æ®é›†ä¸Šè¿è¡Œã€‚ è¿è¡Œä»¥ä¸‹ä»£ç ä»¥ä½¿ç”¨å«æœ‰nhéšè—å•å…ƒçš„å•ä¸ªéšè—å±‚æµ‹è¯•æ¨¡å‹ã€‚</p>
<pre><code># Build a model with a n_h-dimensional hidden layer
parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)

# Plot the decision boundary
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4))
</code></pre>
<pre><code>ç»“æœï¼š
Cost after iteration 0: 0.693048
Cost after iteration 1000: 0.288083
Cost after iteration 2000: 0.254385
Cost after iteration 3000: 0.233864
Cost after iteration 4000: 0.226792
Cost after iteration 5000: 0.222644
Cost after iteration 6000: 0.219731
Cost after iteration 7000: 0.217504
Cost after iteration 8000: 0.219467
Cost after iteration 9000: 0.218561
</code></pre>
<pre><code>Text(0.5, 1.0, 'Decision Boundary for hidden layer size 4')
</code></pre>
<figure data-type="image" tabindex="11"><img src="https://cdn.kesci.com/rt_upload/CB068684F93C4A2A8AE816EB492CDCBE/q17hj9pr80.png" alt="img" loading="lazy"></figure>
<p><strong>é¢„æœŸè¾“å‡º</strong>: Cost after iteration 9000: 0.218561</p>
<pre><code># Print accuracy
predictions = predict(parameters, X)
print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')
</code></pre>
<pre><code>Accuracy: 90%
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: Accuracy: 90%</p>
<p>ä¸Logisticå›å½’ç›¸æ¯”ï¼Œå‡†ç¡®æ€§ç¡®å®æ›´é«˜ã€‚ è¯¥æ¨¡å‹å­¦ä¹ äº†flowerçš„å¶å­å›¾æ¡ˆï¼ ä¸é€»è¾‘å›å½’ä¸åŒï¼Œç¥ç»ç½‘ç»œç”šè‡³èƒ½å¤Ÿå­¦ä¹ éçº¿æ€§çš„å†³ç­–è¾¹ç•Œã€‚</p>
<p>ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°è¯•å‡ ç§ä¸åŒçš„éšè—å±‚å¤§å°ã€‚</p>
<h2 id="4-è°ƒæ•´éšè—å±‚å¤§å°å¯é€‰ç»ƒä¹ ">4 è°ƒæ•´éšè—å±‚å¤§å°ï¼ˆå¯é€‰ç»ƒä¹ ï¼‰</h2>
<p>è¿è¡Œä»¥ä¸‹ä»£ç ï¼ˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼‰ï¼Œ ä½ å°†è§‚å¯Ÿåˆ°ä¸åŒå¤§å°éšè—å±‚çš„æ¨¡å‹çš„ä¸åŒè¡¨ç°ã€‚</p>
<p>In [25]:</p>
<pre><code># This may take about 2 minutes to run

plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 10, 20]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print (&quot;Accuracy for {} hidden units: {} %&quot;.format(n_h, accuracy))
</code></pre>
<pre><code>Accuracy for 1 hidden units: 67.5 %
Accuracy for 2 hidden units: 67.25 %
Accuracy for 3 hidden units: 90.75 %
Accuracy for 4 hidden units: 90.5 %
Accuracy for 5 hidden units: 91.25 %
Accuracy for 10 hidden units: 90.25 %
Accuracy for 20 hidden units: 90.5 %
</code></pre>
<figure data-type="image" tabindex="12"><img src="https://cdn.kesci.com/rt_upload/F70D5A23097642688F6245327ACE9DD7/q17hkk98sl.png" alt="img" loading="lazy"></figure>
<p>å®Œæ•´ä»£ç </p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary,sigmoid,load_planar_dataset,load_extra_datasets
#è®¾ç½®ä¸€ä¸ªå›ºå®šçš„éšæœºç§å­
np.random.seed(1)
X,Y=load_planar_dataset()
#ç”¨matplotlibå¯è§†åŒ–æ•°æ®
plt.scatter(X[0,:],X[1,:],c=np.squeeze(Y),s=40,cmap=plt.cm.Spectral)
plt.show()
shape_X=X.shape
shape_Y=Y.shape
m=Y.shape[1] #è®­ç»ƒé›†é‡Œé¢çš„æ•°æ®
print(&quot;Xçš„ç»´åº¦ä¸ºï¼š&quot;+str(shape_X))
print(&quot;Yçš„ç»´åº¦ä¸ºï¼š&quot;+str(shape_Y))
print(&quot;æ•°æ®é›†é‡Œé¢çš„æ•°æ®æœ‰ï¼š&quot;+str(m)+&quot;ä¸ª&quot;)
def layer_sizes(X,Y):
    '''å‚æ•°ï¼š
    Xï¼šè¾“å…¥æ•°æ®é›†ï¼Œç»´åº¦æ˜¯ï¼ˆè¾“å…¥çš„æ•°é‡ï¼Œè®­ç»ƒ/æµ‹è¯•çš„æ•°é‡ï¼‰
    Yï¼šæ ‡ç­¾
    è¿”å›ï¼š
    n_x:è¾“å…¥å±‚çš„æ•°é‡
    n_h:éšè—å±‚çš„æ•°é‡
    n_y:è¾“å‡ºå±‚çš„æ•°é‡'''
    n_x=X.shape[0]#è¾“å…¥å±‚
    n_h=4#éšè—å±‚ï¼Œç¡¬ç¼–ç ä¸º4
    n_y=Y.shape[0]
    return (n_x,n_h,n_y)
#æµ‹è¯•layer_sizes
print(&quot;=========================æµ‹è¯•layer_sizes=========================&quot;)
X_asses , Y_asses = layer_sizes_test_case()
(n_x,n_h,n_y) =  layer_sizes(X_asses,Y_asses)
print(&quot;è¾“å…¥å±‚çš„èŠ‚ç‚¹æ•°é‡ä¸º: n_x = &quot; + str(n_x))
print(&quot;éšè—å±‚çš„èŠ‚ç‚¹æ•°é‡ä¸º: n_h = &quot; + str(n_h))
print(&quot;è¾“å‡ºå±‚çš„èŠ‚ç‚¹æ•°é‡ä¸º: n_y = &quot; + str(n_y))
#åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°
def initialize_parameters(n_x,n_h,n_y):
    '''
    å‚æ•°ï¼š
    n_x:è¾“å…¥å±‚ç»“ç‚¹çš„æ•°é‡
    n_h:éšè—å±‚ç»“ç‚¹çš„æ•°é‡
    n_y:è¾“å‡ºå±‚ç»“ç‚¹çš„æ•°é‡
    è¿”å›ï¼š
    paraneters:åŒ…å«å‚æ•°çš„å­—å…¸
    W1:æƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º(n_h,n_x)
    b1:åå‘é‡ï¼Œç»´åº¦ä¸º(n_h,1)
    W2ï¼šæƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º(n_y,n_h)
    b2:åå‘é‡ï¼Œç»´åº¦ä¸º(n_y,1)
    '''
    #é€šè¿‡æœ¬å‡½æ•°å¯ä»¥è¿”å›ä¸€ä¸ªæˆ–ä¸€ç»„æœä»â€œ0~1â€å‡åŒ€åˆ†å¸ƒçš„éšæœºæ ·æœ¬å€¼ã€‚éšæœºæ ·æœ¬å–å€¼èŒƒå›´æ˜¯[0,1)ï¼Œä¸åŒ…æ‹¬1ã€‚
    np.random.seed(2)#æŒ‡å®šä¸€ä¸ªéšæœºç§å­
    W1=np.random.rand(n_h,n_x)*0.01
    #zeros ç”Ÿæˆ0çŸ©é˜µ
    b1=np.zeros(shape=(n_h,1))
    W2=np.random.rand(n_y,n_h)
    b2=np.random.rand(n_y,1)
    #ä½¿ç”¨æ–­è¨€ç¡®ä¿æˆ‘çš„æ•°æ®æ ¼å¼æ˜¯æ­£ç¡®çš„
    #assertæ˜¯ç”¨äºå¯¹ç¨‹åºè¿›è¡Œè°ƒè¯•çš„ï¼Œå¯¹äºæ‰§è¡Œç»“æ„çš„åˆ¤æ–­ï¼Œè€Œä¸æ˜¯å¯¹äºä¸šåŠ¡æµç¨‹çš„åˆ¤æ–­ã€‚ï¼ˆç›¸å½“äºä¸€ä¸ªif ()è¯­å¥ï¼Œå¦‚æœæ»¡è¶³æ–­è¨€çš„æ‰§è¡Œç¨‹åºï¼Œå¦‚æœä¸æ»¡è¶³åˆ™æŠ›é”™è¯¯
    assert(W1.shape==(n_h,n_x))
    assert(b1.shape==(n_h,1))
    assert(W2.shape==(n_y,n_h))
    assert(b2.shape==(n_y,1))
    parameters={
        &quot;W1&quot;:W1,
        &quot;b1&quot;:b1,
        &quot;W2&quot;:W2,
        &quot;b2&quot;:b2
    }
    return parameters
#æµ‹è¯•initialize_parameters
n_x,n_h,n_y=initialize_parameters_test_case()
parameters=initialize_parameters(n_x,n_h,n_y)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters[&quot;b2&quot;]))
#å®ç°å¾ªç¯
'''æˆ‘ä»¬ç°åœ¨è¦å®ç°å‰å‘ä¼ æ’­å‡½æ•°forward_propagation()ã€‚ 
æˆ‘ä»¬å¯ä»¥ä½¿ç”¨sigmoid()å‡½æ•°ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨np.tanh()å‡½æ•°ã€‚  
æ­¥éª¤å¦‚ä¸‹ï¼š
ä½¿ç”¨å­—å…¸ç±»å‹çš„parametersï¼ˆå®ƒæ˜¯initialize_parameters() çš„è¾“å‡ºï¼‰æ£€ç´¢æ¯ä¸ªå‚æ•°ã€‚
å®ç°å‘å‰ä¼ æ’­, è®¡ç®—Z[1],A[1],Z[2]Z[1],A[1],Z[2] å’Œ A[2]A[2]ï¼ˆ è®­ç»ƒé›†é‡Œé¢æ‰€æœ‰ä¾‹å­çš„é¢„æµ‹å‘é‡ï¼‰ã€‚
åå‘ä¼ æ’­æ‰€éœ€çš„å€¼å­˜å‚¨åœ¨â€œcacheâ€ä¸­ï¼Œcacheå°†ä½œä¸ºåå‘ä¼ æ’­å‡½æ•°çš„è¾“å…¥ã€‚
'''
def forward_propagation(X,parameters):
    '''
    å‚æ•°ï¼š
    X-ç»´åº¦ä¸º(n_x,m)çš„è¾“å…¥æ•°æ®
    parameters:åˆå§‹åŒ–å‡½æ•°(initialize_parameters)çš„è¾“å‡º
    è¿”å›:
    A2:ä½¿ç”¨æ¿€æ´»å‡½æ•°åçš„æ¿€æ´»å€¼
    cache-åŒ…å«â€œZ1&quot;,&quot;A1&quot;,&quot;Z2&quot;å’Œâ€œA2â€çš„å­—å…¸å‹å˜é‡
    '''
    W1=parameters[&quot;W1&quot;]
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    #å‘å‰ä¼ æ’­è®¡ç®—A2
    #dot è®¡ç®—çŸ©é˜µ
    Z1=np.dot(W1,X)+b1
    A1=np.tanh(Z1)
    Z2=np.dot(W2,A1)+b2
    A2=sigmoid(Z2)
    #ä½¿ç”¨æ–­è¨€ä¿è¯æˆ‘çš„æ•°æ®æ ¼å¼æ˜¯æ­£ç¡®çš„
    assert(A2.shape==(1,X.shape[1]))
    cache={&quot;Z1&quot;:Z1,
           &quot;A1&quot;:A1,
           &quot;Z2&quot;:Z2,
           &quot;A2&quot;:A2
    }
    return (A2,cache)
#æµ‹è¯•æ¨¡å‹
X_assess,parameters=forward_propagation_test_case()
A2,cache=forward_propagation(X_assess,parameters)
print(np.mean(cache[&quot;Z1&quot;]),np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
#è®¡ç®—æˆæœ¬å‡½æ•°ä¸æŸå¤±å‡½æ•°
 
def compute_cost(A2,Y,parameters):
    '''è®¡ç®—äº¤å‰ç†µ
    å‚æ•°ï¼š
    A2ï¼šä½¿ç”¨sigmoidå‡½æ•°è®¡ç®—çš„ç¬¬äºŒæ¬¡æ¿€æ´»åçš„å‡½æ•°å€¼
    Y:Trueæ ‡ç­¾å‘é‡ï¼Œç»´åº¦ä¸º(1,æ•°é‡)
    parameters:ä¸€ä¸ªåŒ…å«W1ï¼ŒW2ï¼Œb1,b2çš„å­—å…¸ç±»å‹çš„å˜é‡
    è¿”å›ï¼šæˆæœ¬äº¤å‰ç†µå‡½æ•°ï¼Œç»™å‡ºæ–¹ç¨‹
    '''
    m=Y.shape[1]
    W1=parameters['W1']
    W2=parameters['W2']
    #è®¡ç®—æˆæœ¬
    logprobs=logprobs=np.multiply(np.log(A2),Y)+np.multiply((1-Y),np.log(1-A2))
    cost=-np.sum(logprobs)/m
    cost=float(np.squeeze(cost))
    assert(isinstance(cost,float))
    return cost
#é¢„æµ‹compute_cost
A2,Y_assess,parameters=compute_cost_test_case()
print(&quot;cost=&quot;+str(compute_cost(A2,Y_assess,parameters)))
#æ­å»ºåå‘ä¼ æ’­å‡½æ•°
def backward_propagation(parameters,cache,X,Y):
    '''å‚æ•°ï¼š
    parameters:åŒ…å«å‚æ•°çš„ä¸€ä¸ªå­—å…¸å˜é‡
    cache:åŒ…å«Z1,A1,Z2,A2çš„å­—å…¸ç±»å‹çš„å˜é‡
    X:è¾“å…¥æ•°æ®ï¼Œç»´åº¦ä¸º(2,æ•°é‡)
    Y:è¾“å‡ºæ•°æ®ï¼Œå”¯ç‹¬ä¸º(1,æ•°é‡)
    è¿”å›:
    grads:åŒ…å«Wå’Œbçš„å¯¼æ•°çš„ä¸€ä¸ªå­—å…¸å˜é‡'''
    m=X.shape[1]
    W1=parameters['W1']
    W2=parameters['W2']
    A1=cache['A1']
    A2=cache['A2']
    dZ2=A2-Y
    dW2=(1/m)*np.dot(dZ2,A1.T)
    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)
    dZ1=np.multiply(np.dot(W2.T,dZ2),1-np.power(A1,2))
    dW1 = (1 / m) * np.dot(dZ1, X.T)
    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)
    grads={'dW1':dW1,
           'db1':db1,
           'dW2':dW2,
           'db2':db2}
    return grads
#æ›´æ–°å‚æ•°
def update_parameters(parameters,grads,learning_rate=1.2):
    '''å‚æ•°ï¼š
    parameters:åŒ…å«å‚æ•°çš„å­—å…¸ç±»å‹æ•°æ®çš„å˜é‡
    grads:åŒ…å«å¯¼æ•°å€¼å¾—å­—å…¸ç±»å‹å˜é‡
    learning_rate:å­¦ä¹ é€Ÿç‡
    è¿”å›ï¼š
    parameters:åŒ…å«æ›´æ–°å‚æ•°çš„å­—å…¸ç±»å‹å’Œå˜é‡'''
    W1,W2=parameters['W1'],parameters['W2']
    b1,b2=parameters['b1'],parameters['b2']
    dW1,dW2=grads['dW1'],grads['dW2']
    db1,db2=grads['db1'],grads['db2']
    W1=W1-learning_rate*dW1
    b1=b1-learning_rate*db1
    W2=W2-learning_rate*dW2
    b2=b2-learning_rate*db2
    parameters={&quot;W1&quot;:W1,
                &quot;b1&quot;:b1,
                &quot;W2&quot;:W2,
                &quot;b2&quot;:b2}
    return parameters
#æµ‹è¯•update_parameters
parameters,grads=update_parameters_test_case()
parameters=update_parameters(parameters,grads)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters['b2']))
#æŠŠä¸Šè¿°å†…å®¹æ•´åˆåˆ°nn_model()ä¸­ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹å¿…é¡»ä»¥æ­£ç¡®çš„é¡ºåºä½¿ç”¨å…ˆå‰çš„åŠŸèƒ½
def nn_model(X,Y,n_h,num_iterations,print_cost=False):
    '''å‚æ•°ï¼š
    X-æ•°æ®é›†ï¼Œç»´åº¦ä¸º(2,ç¤ºä¾‹æ•°)
    Y-æ ‡ç­¾,ç»´åº¦ä¸ºï¼ˆ1ï¼Œç¤ºä¾‹æ•°ï¼‰
    n_h:éšè—å±‚çš„æ•°é‡
    num_iterations:æ¢¯åº¦ä¸‹é™å¾ªç¯ä¸­çš„è¿­ä»£æ¬¡æ•°
    print_cost:å¦‚æœä¸ºTrue,åˆ™æ¯100æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡æˆæœ¬æ•°å€¼
    è¿”å›:
    parameters:æ¨¡å‹å­¦ä¹ çš„å‚æ•°ï¼Œå®ƒä»¬å¯ä»¥ç”¨æ¥é¢„æµ‹
    '''
    np.random.seed(3)#æŒ‡å®šéšæœºç§å­
    n_x=layer_sizes(X,Y)[0]
    n_y=layer_sizes(X,Y)[2]
    parameters=initialize_parameters(n_x,n_h,n_y)
    W1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    for i in range(num_iterations):
        A2,cache=forward_propagation(X,parameters)
        cost=compute_cost(A2,Y,parameters)
        grads=backward_propagation(parameters,cache,X,Y)
        parameters=update_parameters(parameters,grads,learning_rate=0.5)
        if print_cost:
            if i%1000==0:
                print(&quot;ç¬¬&quot;,i,&quot;æ¬¡å¾ªç¯&quot;,&quot;æˆæœ¬ä¸º:&quot;+str(cost))
    return parameters
#æµ‹è¯•nn_model
X_assess,Y_assess=nn_model_test_case()
parameters=nn_model(X_assess,Y_assess,4,num_iterations=10000,print_cost=False)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters[&quot;b2&quot;]))
#predictæ¨¡å‹
'''æ¿€æ´»å€¼å¤§äº0.5ï¼Œé¢„æµ‹å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0'''
def predict(parameters,X):
    '''ä½¿ç”¨å­¦ä¹ çš„å‚æ•°ä¸ºXåˆ†ç±»
    å‚æ•°ï¼š
    parameters:åŒ…å«å‚æ•°çš„å­—å…¸ç±»å‹çš„å˜é‡
    X:è¾“å…¥æ•°æ®
    è¿”å›ï¼š
    predictions:æˆ‘ä»¬æ¨¡å‹é¢„æµ‹çš„å‘é‡ï¼ˆçº¢è‰²ï¼š0/è“è‰²ï¼š1ï¼‰
    '''
    A2,cache=forward_propagation(X,parameters)
    predictions=np.round(A2)
    return predictions
#æµ‹è¯•predict
parameters,X_assess=predict_test_case()
predictions=predict(parameters,X_assess)
print(&quot;é¢„æµ‹çš„å¹³å‡å€¼=&quot;+str(np.mean(predictions)))
parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)
 
#ç»˜åˆ¶è¾¹ç•Œ
'''æ³¨ï¼šæ•°ç»„çš„å¯è§†åŒ–
é€šå¸¸ç®—æ³•çš„ç»“æœæ˜¯å¯ä»¥è¡¨ç¤ºå‘é‡çš„æ•°ç»„ï¼Œç›´æ¥åˆ©ç”¨æ•°ç»„ç”»å›¾æ—¶ç•Œé¢ä¸ºç©º
åˆ©ç”¨sequeeze()å‡½æ•°è½¬åŒ–ä¸ºç§©ä¸º1çš„æ•°ç»„å¯ä»¥æ­£å¸¸ç”»å›¾'''
plot_decision_boundary(lambda x: predict(parameters, x.T), X, np.squeeze(Y))
plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4))
plt.show()
predictions = predict(parameters, X)
print ('å‡†ç¡®ç‡: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')
#æ›´æ”¹éšè—ç»“ç‚¹çš„æ•°é‡
plt.figure(figsize=(16,32))
hidden_layer_sizes=[1,2,3,4,5,20,50]#éšè—å±‚æ•°é‡
for i,n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5,2,i+1)
    plt.title('Hidden Layer of Size%d'% n_h)
    parameters=nn_model(X,Y,n_h,num_iterations=5000)
    plot_decision_boundary(lambda x:predict(parameters,x.T),X,np.squeeze(Y))
    predictions=predict(parameters,X)
    accuracy=float((np.dot(Y,predictions.T)+np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print(&quot;éšè—ç»“ç‚¹çš„æ•°é‡ï¼š{}ï¼Œå‡†ç¡®ç‡ï¼š{}%&quot;.format(n_h,accuracy))
</code></pre>
<p><strong>è¯´æ˜</strong>ï¼š</p>
<ul>
<li>
<p>è¾ƒå¤§çš„æ¨¡å‹ï¼ˆå…·æœ‰æ›´å¤šéšè—çš„å•å…ƒï¼‰èƒ½å¤Ÿæ›´å¥½åœ°æ‹Ÿåˆè®­ç»ƒé›†ï¼Œç›´åˆ°æœ€ç»ˆæœ€å¤§çš„æ¨¡å‹è¿‡æ‹Ÿåˆæ•°æ®ä¸ºæ­¢ã€‚</p>
</li>
<li>
<p>éšè—å±‚çš„æœ€ä½³å¤§å°ä¼¼ä¹åœ¨n_h = 5å·¦å³ã€‚çš„ç¡®ï¼Œæ­¤å€¼ä¼¼ä¹å¾ˆå¥½åœ°æ‹Ÿåˆäº†æ•°æ®ï¼Œè€Œåˆä¸ä¼šå¼•èµ·æ˜æ˜¾çš„è¿‡åº¦æ‹Ÿåˆã€‚</p>
</li>
<li>
<p>ç¨åä½ è¿˜å°†å­¦ä¹ æ­£åˆ™åŒ–ï¼Œå¸®åŠ©æ„å»ºæ›´å¤§çš„æ¨¡å‹ï¼ˆä¾‹å¦‚n_h = 50ï¼‰è€Œä¸ä¼šè¿‡åº¦æ‹Ÿåˆã€‚</p>
<p>å‚è€ƒæ–‡ç« ï¼š</p>
<p><a href="https://www.kesci.com/home/project/5dd3946900b0b900365f3a48/code">æ–‡ç« ä¸€</a></p>
<p><a href="https://www.cnblogs.com/charlotte77/p/5629865.html">æ–‡ç« äºŒ</a></p>
<p><a href="https://www.cnblogs.com/maybe2030/p/5597716.html">æ–‡ç« ä¸‰</a></p>
<p>æ•°æ®åŒ…åœ¨æ–‡ç« ä¸€ä¸­ å¯ä»¥ç›´æ¥ä¸‹è½½ä½¿ç”¨</p>
</li>
</ul>

        </div>
        
        <div class="prev-post">
            ä¸Šä¸€ç¯‡
            <a href="https://s-hmily.github.io/post/python-chang-yong-han-shu-jie-xi/">
                pythonå¸¸ç”¨å‡½æ•°è§£æ
            </a>
        </div>
        
        
        <div class="next-post">
            ä¸‹ä¸€ç¯‡
            <a href="https://s-hmily.github.io/post/sigmoid-han-shu/">
                sigmoidå‡½æ•°
            </a>
        </div>
        
    </div>
    </div>
</body>
<script>
    var t_img; // å®šæ—¶ï¿½?
    var isLoad = true; // æ§åˆ¶å˜é‡
    isImgLoad(function () {
        // åŠ è½½å®Œæˆ
        $('.postdetailimg').css("display", "block");
    });
    // åˆ¤æ–­å›¾ç‰‡åŠ è½½çš„å‡½ï¿½?
    function isImgLoad(callback) {
        // æ³¨æ„æˆ‘çš„å›¾ç‰‡ç±»åéƒ½æ˜¯coverï¼Œå› ä¸ºæˆ‘ï¿½?éœ€è¦ï¿½?ï¿½ç†coverã€‚å…¶å®ƒå›¾ç‰‡å¯ä»¥ä¸ç®¡ï¿½?
        // æŸ¥æ‰¾æ‰€æœ‰å°é¢å›¾ï¼Œè¿­ä»£ï¿½?ï¿½ç†
        $('.postdetailimg').each(function () {
            // æ‰¾åˆ°ï¿½?0å°±å°†isLoadè®¾ä¸ºfalseï¼Œå¹¶é€€å‡ºeach
            if (this.height === 0) {
                isLoad = false;
                return false;
            }
        });
        // ä¸ºtrueï¼Œæ²¡æœ‰å‘ç°ä¸º0çš„ã€‚åŠ è½½å®Œï¿½?
        if (isLoad) {
            clearTimeout(t_img); // æ¸…é™¤å®šæ—¶ï¿½?
            // å›è°ƒå‡½æ•°
            callback();
            // ä¸ºfalseï¼Œå› ä¸ºæ‰¾åˆ°äº†æ²¡æœ‰åŠ è½½å®Œæˆçš„å›¾ï¼Œå°†è°ƒç”¨å®šæ—¶å™¨é€’å½’
        } else {
            isLoad = true;
            t_img = setTimeout(function () {
                isImgLoad(callback); // é€’å½’ï¿½?ï¿½?
            }, 500); // æˆ‘è¿™é‡Œï¿½?ï¿½ç½®çš„æ˜¯500ï¿½?ç§’å°±ï¿½?æä¸€æ¬¡ï¼Œï¿½?ä»¥è‡ªå·±è°ƒï¿½?
        }
    }

    //æ–‡ç« é˜…è¯»çƒ­åº¦
    var pl = $("#pl").html();
    var rootaddr = $("#rootaddr").html();
    pl = pl.replace(rootaddr, "");
    $("#hotnum").attr('id', pl);
</script>
        <div name="comment" style="background: white">
            <div class="commentcontainer">
                
                <p>è¯·åˆ°å®¢æˆ·ç«¯â€œä¸»é¢˜--è‡ªå®šä¹‰é…ç½®--valineâ€ä¸­å¡«å…¥IDå’ŒKEY</p>
                
            </div>
        </div>
    </div>
     
                <div class="toc-container">
                    <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#1%E5%89%8D%E8%A8%80">1.å‰è¨€</a></li>
<li><a href="#2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A6%82%E8%BF%B0">2.ç¥ç»ç½‘ç»œçš„æ¦‚è¿°</a></li>
<li><a href="#3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B%E5%88%A9%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E7%B1%BB%E4%BA%8C%E7%BB%B4%E6%95%B0%E6%8D%AE%E9%9B%86">3.ç¥ç»ç½‘ç»œçš„å®ç°è¿‡ç¨‹ï¼ˆåˆ©ç”¨ç¥ç»ç½‘ç»œåˆ†ç±»äºŒç»´æ•°æ®é›†ï¼‰</a>
<ul>
<li><a href="#31%E5%AE%89%E8%A3%85%E5%8C%85">3.1å®‰è£…åŒ…</a></li>
<li><a href="#32-%E6%95%B0%E6%8D%AE%E9%9B%86">3.2 æ•°æ®é›†</a></li>
<li><a href="#33%E5%AE%9A%E4%B9%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">3.3å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„</a>
<ul>
<li><a href="#331%E5%AE%9A%E4%B9%89%E5%8F%98%E9%87%8F">3.3.1å®šä¹‰å˜é‡</a></li>
<li><a href="#332-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0">3.3.2 åˆå§‹åŒ–æ¨¡å‹å‚æ•°</a></li>
<li><a href="#333-%E5%BE%AA%E7%8E%AF">3.3.3 å¾ªç¯</a></li>
<li><a href="#334-%E5%9C%A8nn_model%E4%B8%AD%E9%9B%86%E6%88%90331-332%E5%92%8C333%E9%83%A8%E5%88%86%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0">3.3.4- åœ¨nn_modelï¼ˆï¼‰ä¸­é›†æˆ3.3.1ã€3.3.2å’Œ3.3.3éƒ¨åˆ†ä¸­çš„å‡½æ•°</a></li>
</ul>
</li>
<li><a href="#45-%E9%A2%84%E6%B5%8B">4.5- é¢„æµ‹</a></li>
</ul>
</li>
<li><a href="#4-%E8%B0%83%E6%95%B4%E9%9A%90%E8%97%8F%E5%B1%82%E5%A4%A7%E5%B0%8F%E5%8F%AF%E9%80%89%E7%BB%83%E4%B9%A0">4 è°ƒæ•´éšè—å±‚å¤§å°ï¼ˆå¯é€‰ç»ƒä¹ ï¼‰</a></li>
</ul>
</li>
</ul>

    </div>
    </div>
    </div>
    <div class="toggleContainer">
        <div class="toggle">
            <i class="fas fa-angle-double-up"></i>
        </div>
    </div>
    <div id="bg">
    </div>
    <div id="bgchoice" style="display: none">link</div>
    
    <div id="bgurl" style="display:none">https://pic2.zhimg.com/80/v2-bcbb1a4f932ab78c198b0a99af266d4e_720w.jpg?source=1940ef5c</div>
       
    </div>
    <!-- å“åº”å¼å¸ƒå±€ï¼Œé’ˆå¯¹æ‰‹æœºç«¯å†…å®¹æ˜¾ç¤º -->
    <div class="nav-small">
        <head>
  <!-- å¼•å…¥Bootstrapæ ¸å¿ƒæ ·å¼æ–‡ä»¶ -->
  <link rel="stylesheet" href="https://s-hmily.github.io/media/css/bootstrap.min.css">
</head>

<body>
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container-fluid">
      <!-- Brand and toggle get grouped for better mobile display -->
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
          data-target="#barmenu" aria-expanded="false" id="barbutton">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="https://s-hmily.github.io">å®å®å½“&nbsp;&nbsp;|</a>
      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="barmenu">
        <ul class="nav navbar-nav">
          
          
          <li>
            <a href="https://s-hmily.github.io">
              é¦–é¡µ
            </a>
          </li>
          
          
          
          <li>
            <a href="/archives">
              å½’æ¡£
            </a>
          </li>
          
          
          
          <li>
            <a href="https://s-hmily.github.io/tags">
              æ ‡ç­¾
            </a>
          </li>
          
          
          
            <li><a href="https://s-hmily.github.io/talk">è¯´è¯´</a></li>
            
          
          <li><a href="https://s-hmily.github.io/friends">å‹é“¾</a></li>

          
        </ul>
      </div><!-- /.navbar-collapse -->
    </div><!-- /.container-fluid -->
  </nav>


  <!-- å¼•å…¥jQueryæ ¸å¿ƒjsæ–‡ä»¶ -->
  <script src="http://cdn.static.runoob.com/libs/jquery/2.1.1/jquery.min.js"></script>
  <script>
  var btstate = false;
  var bt = $("#barbutton");
  var bm = $("#barmenu");
  bt.click(function(){
    dropdown();
  })
  function dropdown(){
    console.log(btstate);
    //ä¸‹æ‹‰
    if(btstate==false){
      bt.removeClass("collapsed");
      bt.attr("aria-expanded","true");
      bm.attr("aria-expanded","true")
      bm.fadeIn(700);
      btstate = true;
    }
    else{
      bt.addClass("collapsed");
      bt.attr("aria-expanded","false");
      bm.removeClass("in");
      bm.hide();
      bm.attr("aria-expanded","false");
      btstate = false;
    }
  }
  </script> 
</body>
    <div style="margin-top:30px"></div>
    <link rel="stylesheet" href="https://s-hmily.github.io/media/css/font-awesome.css">
<style>

</style>

<body>
    <div class="allcontent" id="postdetail">
        <div class="postshow">
            
            <div class="postdetailimg" style="width:100%;overflow: hidden;display: none">
                <img src="https://s-hmily.github.io/post-images/qian-ceng-shen-jing-wang-luo-gai-shu.jpg" class="postimage" style="cursor:auto">
            </div>
            <div class="postinfo-detail">
                <div class="postdate"><i class="fa fa-calendar"></i>2020-07-18</div>
                <div class="poststatus postdate"><i class="fa fa-clock-o"></i> 33 min read</div>
                <div class="posttag">
                    
                    <a href="https://s-hmily.github.io/tag/lMYeCI_cf/" class="postlink">
                        <i class="fa fa-tag"></i> æœºå™¨å­¦ä¹ 
                    </a>
                    
                </div>
            </div>
            
        <div id="texttitle" style="text-align: center">
            <h2>æµ…å±‚ç¥ç»ç½‘ç»œæ¦‚è¿°å’Œåº”ç”¨</h2>
            <!-- id å°†ä½œä¸ºæŸ¥è¯¢æ¡ä»¶ -->
            <div id="pl" style="display:none">https://s-hmily.github.io/post/qian-ceng-shen-jing-wang-luo-gai-shu/</div>
            <div id="rootaddr" style="display:none">https://s-hmily.github.io</div>
            <span id="hotnum" class="leancloud_visitors" data-flag-title="æµ…å±‚ç¥ç»ç½‘ç»œæ¦‚è¿°å’Œåº”ç”¨">
                <h4 class="readercount">çƒ­åº¦ğŸ”¥: <i class="leancloud-visitors-count">loading...</i></h4>
            </span>
        </div>
        <div class="text ">
            <h2 id="1å‰è¨€">1.å‰è¨€</h2>
<p>ä¼—æ‰€å‘¨çŸ¥ï¼Œæˆ‘ä»¬ç”Ÿæ´»ä¸­é‡åˆ°çš„å¾ˆå¤šé—®é¢˜éƒ½æ˜¯éçº¿æ€§çš„é—®é¢˜ï¼Œä¾‹å¦‚é¢„æµ‹èŠ±çš„ç”Ÿé•¿æƒ…å†µã€‚é‚£ä¹ˆå¦‚æœæˆ‘ä»¬é‡åˆ°äº†ä¸€äº›éçº¿æ€§é—®é¢˜ï¼Œæˆ‘ä»¬æƒ³è¦å¯¹è¿™ä¸ªéçº¿æ€§é—®é¢˜è¿›è¡Œé¢„æµ‹ä¼°è®¡çš„æ—¶å€™æˆ‘ä»¬éœ€è¦æ€ä¹ˆåŠå‘¢ï¼Ÿ</p>
<p>è¿™é‡Œå°±éœ€è¦å¼•å…¥ç¥ç»ç½‘ç»œçš„æ¦‚å¿µäº†ã€‚</p>
<h2 id="2ç¥ç»ç½‘ç»œçš„æ¦‚è¿°">2.ç¥ç»ç½‘ç»œçš„æ¦‚è¿°</h2>
<p>æˆ‘ä»¬çŸ¥é“ï¼Œæˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­å¾ˆå¤šé—®é¢˜ï¼Œç”šè‡³è¯´å¤§å¤šæ•°é—®é¢˜éƒ½ä¸æ˜¯çº¿æ€§å¯åˆ†é—®é¢˜ï¼Œé‚£æˆ‘ä»¬è¦è§£å†³éçº¿æ€§å¯åˆ†é—®é¢˜è¯¥æ€æ ·å¤„ç†å‘¢ï¼Ÿè¿™å°±æ˜¯è¿™éƒ¨åˆ†æˆ‘ä»¬è¦å¼•å‡ºçš„â€œå¤šå±‚â€çš„æ¦‚å¿µã€‚æ—¢ç„¶å•å±‚æ„ŸçŸ¥æœºè§£å†³ä¸äº†éçº¿æ€§é—®é¢˜ï¼Œé‚£æˆ‘ä»¬å°±é‡‡ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼Œä¸‹å›¾å°±æ˜¯ä¸€ä¸ªä¸¤å±‚æ„ŸçŸ¥æœºè§£å†³å¼‚æˆ–é—®é¢˜çš„ç¤ºæ„å›¾ï¼š</p>
<figure data-type="image" tabindex="1"><img src="https://s-hmily.github.io/post-images/1595296483863.png" alt="" loading="lazy"></figure>
<p>æ„å»ºå¥½ä¸Šè¿°ç½‘ç»œä»¥åï¼Œé€šè¿‡è®­ç»ƒå¾—åˆ°æœ€åçš„åˆ†ç±»é¢å¦‚ä¸‹ï¼š</p>
<figure data-type="image" tabindex="2"><img src="https://s-hmily.github.io/post-images/1595296506152.png" alt="" loading="lazy"></figure>
<p>ç”±æ­¤å¯è§ï¼Œå¤šå±‚æ„ŸçŸ¥æœºå¯ä»¥å¾ˆå¥½çš„è§£å†³éçº¿æ€§å¯åˆ†é—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸å°†å¤šå±‚æ„ŸçŸ¥æœºè¿™æ ·çš„å¤šå±‚ç»“æ„ç§°ä¹‹ä¸ºæ˜¯ç¥ç»ç½‘ç»œã€‚ä½†æ˜¯ï¼Œå¤šå±‚æ„ŸçŸ¥æœºè™½ç„¶å¯ä»¥åœ¨ç†è®ºä¸Šå¯ä»¥è§£å†³éçº¿æ€§é—®é¢˜ï¼Œä½†æ˜¯å®é™…ç”Ÿæ´»ä¸­é—®é¢˜çš„å¤æ‚æ€§è¦è¿œä¸æ­¢å¼‚æˆ–é—®é¢˜è¿™ä¹ˆç®€å•ï¼Œæ‰€ä»¥æˆ‘ä»¬å¾€å¾€è¦æ„å»ºå¤šå±‚ç½‘ç»œï¼Œè€Œå¯¹äºå¤šå±‚ç¥ç»ç½‘ç»œé‡‡ç”¨ä»€ä¹ˆæ ·çš„å­¦ä¹ ç®—æ³•åˆæ˜¯ä¸€é¡¹å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºçš„å…·æœ‰4å±‚éšå«å±‚çš„ç½‘ç»œç»“æ„ä¸­è‡³å°‘æœ‰33ä¸ªå‚æ•°ï¼ˆä¸è®¡åç½®biaså‚æ•°ï¼‰ï¼Œæˆ‘ä»¬åº”è¯¥å¦‚ä½•å»ç¡®å®šå‘¢ï¼Ÿ</p>
<figure data-type="image" tabindex="3"><img src="https://s-hmily.github.io/post-images/1595296520486.png" alt="" loading="lazy"></figure>
<p>æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸ªå›¾ï¼š</p>
<figure data-type="image" tabindex="4"><img src="https://s-hmily.github.io/post-images/1595296534309.png" alt="" loading="lazy"></figure>
<p>è¿™æ˜¯å…¸å‹çš„ä¸‰å±‚ç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»„æˆï¼ŒLayer L1æ˜¯è¾“å…¥å±‚ï¼ŒLayer L2æ˜¯éšå«å±‚ï¼ŒLayer L3æ˜¯éšå«å±‚ï¼Œæˆ‘ä»¬ç°åœ¨æ‰‹é‡Œæœ‰ä¸€å †æ•°æ®{x1,x2,x3,...,xn},è¾“å‡ºä¹Ÿæ˜¯ä¸€å †æ•°æ®{y1,y2,y3,...,yn},ç°åœ¨è¦ä»–ä»¬åœ¨éšå«å±‚åšæŸç§å˜æ¢ï¼Œè®©ä½ æŠŠæ•°æ®çŒè¿›å»åå¾—åˆ°ä½ æœŸæœ›çš„è¾“å‡ºã€‚å¦‚æœä½ å¸Œæœ›ä½ çš„è¾“å‡ºå’ŒåŸå§‹è¾“å…¥ä¸€æ ·ï¼Œé‚£ä¹ˆå°±æ˜¯æœ€å¸¸è§çš„è‡ªç¼–ç æ¨¡å‹ï¼ˆAuto-Encoderï¼‰ã€‚å¯èƒ½æœ‰äººä¼šé—®ï¼Œä¸ºä»€ä¹ˆè¦è¾“å…¥è¾“å‡ºéƒ½ä¸€æ ·å‘¢ï¼Ÿæœ‰ä»€ä¹ˆç”¨å•Šï¼Ÿå…¶å®åº”ç”¨æŒºå¹¿çš„ï¼Œåœ¨å›¾åƒè¯†åˆ«ï¼Œæ–‡æœ¬åˆ†ç±»ç­‰ç­‰éƒ½ä¼šç”¨åˆ°ï¼ŒåŒ…æ‹¬ä¸€äº›å˜ç§ä¹‹ç±»çš„ã€‚å¦‚æœä½ çš„è¾“å‡ºå’ŒåŸå§‹è¾“å…¥ä¸ä¸€æ ·ï¼Œé‚£ä¹ˆå°±æ˜¯å¾ˆå¸¸è§çš„äººå·¥ç¥ç»ç½‘ç»œäº†ï¼Œç›¸å½“äºè®©åŸå§‹æ•°æ®é€šè¿‡ä¸€ä¸ªæ˜ å°„æ¥å¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„è¾“å‡ºæ•°æ®ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬ä»Šå¤©è¦è®²çš„è¯é¢˜ã€‚</p>
<h2 id="3ç¥ç»ç½‘ç»œçš„å®ç°è¿‡ç¨‹åˆ©ç”¨ç¥ç»ç½‘ç»œåˆ†ç±»äºŒç»´æ•°æ®é›†">3.ç¥ç»ç½‘ç»œçš„å®ç°è¿‡ç¨‹ï¼ˆåˆ©ç”¨ç¥ç»ç½‘ç»œåˆ†ç±»äºŒç»´æ•°æ®é›†ï¼‰</h2>
<p><strong>æç¤º</strong>ï¼šå»ºç«‹ç¥ç»ç½‘ç»œçš„ä¸€èˆ¬æ–¹æ³•ï¼š</p>
<p>1.å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„ï¼ˆè¾“å…¥å•å…ƒæ•°ï¼Œéšè—å•å…ƒæ•°ç­‰ï¼‰.</p>
<p>2.åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°</p>
<p>3.å¾ªç¯ï¼š</p>
<ul>
<li>
<p>å®ç°å‰å‘ä¼ æ’­</p>
</li>
<li>
<p>è®¡ç®—æŸå¤±</p>
</li>
<li>
<p>åå‘ä¼ æ’­ä»¥è·å¾—æ¢¯åº¦</p>
</li>
<li>
<p>æ›´æ–°å‚æ•°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰</p>
</li>
<li>
<p>æˆ‘ä»¬é€šå¸¸ä¼šæ„å»ºè¾…åŠ©å‡½æ•°æ¥è®¡ç®—ç¬¬1-3æ­¥ï¼Œç„¶åå°†å®ƒä»¬åˆå¹¶ä¸º<code>nn_model()</code>å‡½æ•°ã€‚ä¸€æ—¦æ„å»ºäº†<code>nn_model()</code>å¹¶å­¦ä¹ äº†æ­£ç¡®çš„å‚æ•°ï¼Œå°±å¯ä»¥å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚</p>
<h3 id="31å®‰è£…åŒ…">3.1å®‰è£…åŒ…</h3>
<p>è®©æˆ‘ä»¬é¦–å…ˆå¯¼å…¥åœ¨ä½œä¸šè¿‡ç¨‹ä¸­éœ€è¦çš„æ‰€æœ‰è½¯ä»¶åŒ…ã€‚</p>
<ul>
<li>
<p><a href="https://www.kesci.com/api/notebooks/5e85d6bf95b029002ca7e7e6/www.numpy.org">numpy</a>æ˜¯Pythonç§‘å­¦è®¡ç®—çš„åŸºæœ¬åŒ…ã€‚</p>
</li>
<li>
<p><a href="http://scikit-learn.org/stable/">sklearn</a>æä¾›äº†ç”¨äºæ•°æ®æŒ–æ˜å’Œåˆ†æçš„ç®€å•æœ‰æ•ˆçš„å·¥å…·ã€‚</p>
</li>
<li>
<p><a href="http://matplotlib.org/">matplotlib</a> æ˜¯åœ¨Pythonä¸­å¸¸ç”¨çš„ç»˜åˆ¶å›¾å½¢çš„åº“ã€‚</p>
</li>
<li>
<p>testCasesæä¾›äº†ä¸€äº›æµ‹è¯•ç¤ºä¾‹ç”¨ä»¥è¯„ä¼°å‡½æ•°çš„æ­£ç¡®æ€§</p>
</li>
<li>
<p>planar_utilsæä¾›äº†æ­¤ä½œä¸šä¸­ä½¿ç”¨çš„å„ç§å‡½æ•°</p>
<pre><code># Package imports
import numpy as np
import matplotlib.pyplot as plt
from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

%matplotlib inline

np.random.seed(1) # set a seed so that the results are consistent
</code></pre>
</li>
</ul>
<h3 id="32-æ•°æ®é›†">3.2 æ•°æ®é›†</h3>
<pre><code>X, Y = load_planar_dataset() 
# Visualize the data:
plt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0,:].shape), s=40, cmap=plt.cm.Spectral)
</code></pre>
<p>å…¶ä¸­Xæ˜¯èŠ±ç“£çš„ç‚¹ï¼Œè¡¨ç¤ºç‰¹å¾çŸ©é˜µï¼ˆX1,X2ï¼‰è¿™ä¸ªXæ•°æ®é›†å¯ä»¥åœ¨åè¾¹ç”¨X.shapeå¾—åˆ°è¿™ä¸ªXæ˜¯2è¡Œ400åˆ—çš„</p>
<p>Yæ•°æ®é›†æ˜¯1è¡Œ400åˆ—çš„ï¼Œè¡¨ç¤ºæ ‡ç­¾(çº¢è‰²ï¼š0ï¼Œè“è‰²ï¼š1)</p>
<p>è·å¾—numpyæ•°ç»„çš„shapeç»´åº¦ä»£ç </p>
<pre><code>### START CODE HERE ### (â‰ˆ 3 lines of code)
shape_X = X.shape
shape_Y = Y.shape

m = shape_X[1]  # training set size
### END CODE HERE ###

print ('The shape of X is: ' + str(shape_X))
print ('The shape of Y is: ' + str(shape_Y))
print ('I have m = %d training examples!' % (m))
</code></pre>
<p>ç»“æœï¼š</p>
<p>The shape of X is: (2, 400)</p>
<p>The shape of Y is: (1, 400)</p>
<p>I have m = 400 training examples!</p>
<h3 id="33å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„">3.3å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„</h3>
<h4 id="331å®šä¹‰å˜é‡">3.3.1å®šä¹‰å˜é‡</h4>
<p><strong>ç›®æ ‡</strong>ï¼šå®šä¹‰ä¸‰ä¸ªå˜é‡     n_x:è¾“å…¥å±‚å¤§å°  n_hï¼šéšè—å±‚å¤§å°(è¿™é‡Œè®¾ç½®æˆ4)   n_y:è¾“å‡ºå±‚å¤§å°</p>
<p><strong>æç¤º</strong>ï¼šä½¿ç”¨shapeå‡½æ•°æ¥æ‰¾åˆ°n_xå’Œn_yã€‚å¦å¤– å°†éšè—å±‚å¤§å°ç¡¬ç¼–ç ä¸º4</p>
<pre><code># GRADED FUNCTION: layer_sizes

def layer_sizes(X, Y):
    &quot;&quot;&quot;
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    &quot;&quot;&quot;
    ### START CODE HERE ### (â‰ˆ 3 lines of code)
    n_x = X.shape[0] # size of input layer
    n_h = 4
    n_y = Y.shape[0] # size of output layer
    ### END CODE HERE ###
    return (n_x, n_h, n_y)
</code></pre>
<p>â€‹    å¯¼å…¥ä¸€ç»„æ•°æ®è¿›è¡Œæµ‹è¯•ï¼š</p>
<pre><code>X_assess, Y_assess = layer_sizes_test_case()
(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)
print(&quot;The size of the input layer is: n_x = &quot; + str(n_x))
print(&quot;The size of the hidden layer is: n_h = &quot; + str(n_h))
print(&quot;The size of the output layer is: n_y = &quot; + str(n_y))
</code></pre>
<p>ç»“æœï¼š</p>
<p>The size of the input layer is: n_x = 5</p>
<p>The size of the hidden layer is: n_h = 4</p>
<p>The size of the output layer is: n_y = 2</p>
</li>
</ul>
<p><strong>é¢„æœŸè¾“å‡º</strong>ï¼ˆä»…ç”¨äºè¯„ä¼°åˆšåˆšç¼–ç çš„å‡½æ•°ï¼Œå¹¶ä¸ä»£è¡¨å®é™…ç½‘ç»œå¤§å°ï¼‰ã€‚ è¾“å…¥å±‚çš„å¤§å°ä¸ºï¼šn_x = 5 éšè—å±‚çš„å¤§å°ä¸ºï¼šn_h = 4 è¾“å‡ºå±‚çš„å¤§å°ä¸ºï¼šn_y = 2</p>
<p>å¯¹äºæˆ‘ä»¬è¿™ä¸ªæ•´ä¸ªé¢˜æ¥è®²ï¼Œåº”è¯¥æ˜¯n_x = 2 n_h = 4 n_y = 21</p>
<h4 id="332-åˆå§‹åŒ–æ¨¡å‹å‚æ•°">3.3.2 åˆå§‹åŒ–æ¨¡å‹å‚æ•°</h4>
<p><strong>ç›®æ ‡</strong>ï¼šå®ç°å‡½æ•°Â <code>initialize_parameters()</code>ã€‚</p>
<p><strong>æç¤ºï¼š</strong></p>
<ul>
<li>è¯·ç¡®ä¿å‚æ•°å¤§å°æ­£ç¡®ã€‚ å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯å‚è€ƒä¸Šé¢çš„ç¥ç»ç½‘ç»œå›¾ã€‚</li>
<li>ä½¿ç”¨éšæœºå€¼åˆå§‹åŒ–æƒé‡çŸ©é˜µã€‚      - ä½¿ç”¨ï¼š<code>np.random.randnï¼ˆaï¼Œbï¼‰* 0.01</code>éšæœºåˆå§‹åŒ–ç»´åº¦ä¸ºï¼ˆaï¼Œbï¼‰çš„çŸ©é˜µã€‚</li>
<li>å°†åå·®å‘é‡åˆå§‹åŒ–ä¸ºé›¶ã€‚      - ä½¿ç”¨ï¼š<code>np.zeros((a,b))</code> åˆå§‹åŒ–ç»´åº¦ä¸ºï¼ˆaï¼Œbï¼‰é›¶çš„çŸ©é˜µã€‚</li>
</ul>
<pre><code>#åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°
def initialize_parameters(n_x,n_h,n_y):
    '''
    å‚æ•°ï¼š
    n_x:è¾“å…¥å±‚ç»“ç‚¹çš„æ•°é‡
    n_h:éšè—å±‚ç»“ç‚¹çš„æ•°é‡
    n_y:è¾“å‡ºå±‚ç»“ç‚¹çš„æ•°é‡
    è¿”å›ï¼š
    paraneters:åŒ…å«å‚æ•°çš„å­—å…¸
    W1:æƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º(n_h,n_x)
    b1:åå‘é‡ï¼Œç»´åº¦ä¸º(n_h,1)
    W2ï¼šæƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º(n_y,n_h)
    b2:åå‘é‡ï¼Œç»´åº¦ä¸º(n_y,1)
    '''
    #é€šè¿‡æœ¬å‡½æ•°å¯ä»¥è¿”å›ä¸€ä¸ªæˆ–ä¸€ç»„æœä»â€œ0~1â€å‡åŒ€åˆ†å¸ƒçš„éšæœºæ ·æœ¬å€¼ã€‚éšæœºæ ·æœ¬å–å€¼èŒƒå›´æ˜¯[0,1)ï¼Œä¸åŒ…æ‹¬1ã€‚
    np.random.seed(2)#æŒ‡å®šä¸€ä¸ªéšæœºç§å­
    W1=np.random.rand(n_h,n_x)*0.01
    #zeros ç”Ÿæˆ0çŸ©é˜µ
    b1=np.zeros(shape=(n_h,1))
    W2=np.random.rand(n_y,n_h)
    b2=np.random.rand(n_y,1)
    #ä½¿ç”¨æ–­è¨€ç¡®ä¿æˆ‘çš„æ•°æ®æ ¼å¼æ˜¯æ­£ç¡®çš„
    #assertæ˜¯ç”¨äºå¯¹ç¨‹åºè¿›è¡Œè°ƒè¯•çš„ï¼Œå¯¹äºæ‰§è¡Œç»“æ„çš„åˆ¤æ–­ï¼Œè€Œä¸æ˜¯å¯¹äºä¸šåŠ¡æµç¨‹çš„åˆ¤æ–­ã€‚ï¼ˆç›¸å½“äºä¸€ä¸ªif ()è¯­å¥ï¼Œå¦‚æœæ»¡è¶³æ–­è¨€çš„æ‰§è¡Œç¨‹åºï¼Œå¦‚æœä¸æ»¡è¶³åˆ™æŠ›é”™è¯¯
    assert(W1.shape==(n_h,n_x))
    assert(b1.shape==(n_h,1))
    assert(W2.shape==(n_y,n_h))
    assert(b2.shape==(n_y,1))
    parameters={
        &quot;W1&quot;:W1,
        &quot;b1&quot;:b1,
        &quot;W2&quot;:W2,
        &quot;b2&quot;:b2
    }
    return parameters
</code></pre>
<p>æµ‹è¯•ä»£ç ï¼š</p>
<pre><code>n_x, n_h, n_y = initialize_parameters_test_case()

parameters = initialize_parameters(n_x, n_h, n_y)
print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>ç»“æœï¼šW1 = [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]
b2 = [[0.]]
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: W1 = [[-0.00416758 -0.00056267][-0.02136196 0.01640271] [-0.01793436 -0.00841747][ 0.00502881 -0.01245288]] b1 = [[0.][0.] [0.][0.]] W2 = [[-0.01057952 -0.00909008 0.00551454 0.02292208]] b2 = [[0.]]</p>
<h4 id="333-å¾ªç¯">3.3.3 å¾ªç¯</h4>
<p><strong>ç›®æ ‡</strong>ï¼šå®ç°<code>forward_propagationï¼ˆï¼‰</code>ï¼ˆå®ç°æ­£å‘ä¼ æ’­ï¼‰ã€‚</p>
<p>æç¤ºï¼š</p>
<ul>
<li>
<p>åœ¨ä¸Šæ–¹æŸ¥çœ‹åˆ†ç±»å™¨çš„æ•°å­¦è¡¨ç¤ºå½¢å¼ã€‚</p>
</li>
<li>
<p>ä½ å¯ä»¥ä½¿ç”¨å†…ç½®åœ¨ç¬”è®°æœ¬ä¸­çš„<code>sigmoid()</code>å‡½æ•°ã€‚</p>
</li>
<li>
<p>ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨numpyåº“ä¸­çš„<code>np.tanhï¼ˆï¼‰</code>å‡½æ•°ã€‚</p>
</li>
<li>
<p>å¿…é¡»æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š      1.ä½¿ç”¨<code>parameters [â€œ ..â€]</code>ä»å­—å…¸â€œ parametersâ€ï¼ˆè¿™æ˜¯<code>initialize_parametersï¼ˆï¼‰</code>çš„è¾“å‡ºï¼‰ä¸­æ£€ç´¢å‡ºæ¯ä¸ªå‚æ•°ã€‚      2.å®ç°æ­£å‘ä¼ æ’­ï¼Œè®¡ç®—Z[1],A[1],Z[2] å’Œ A[2] ï¼ˆæ‰€æœ‰è®­ç»ƒæ•°æ®çš„é¢„æµ‹ç»“æœå‘é‡ï¼‰ã€‚</p>
</li>
<li>
<p>å‘åä¼ æ’­æ‰€éœ€çš„å€¼å­˜å‚¨åœ¨<code>cache</code>ä¸­ï¼Œ <code>cache</code>å°†ä½œä¸ºåå‘ä¼ æ’­å‡½æ•°çš„è¾“å…¥ã€‚</p>
<p>ä»£ç ï¼š</p>
<pre><code># GRADED FUNCTION: forward_propagation

def forward_propagation(X, parameters):
    &quot;&quot;&quot;
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)
    
    Returns:
    A2 -- The sigmoid output of the second activation
    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;
    &quot;&quot;&quot;
    # Retrieve each parameter from the dictionary &quot;parameters&quot;
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Implement Forward Propagation to calculate A2 (probabilities)
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    Z1 = np.dot(W1,X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2,A1) + b2
    A2 = sigmoid(Z2)
    ### END CODE HERE ###
    
    assert(A2.shape == (1, X.shape[1]))
    
    cache = {&quot;Z1&quot;: Z1,
             &quot;A1&quot;: A1,
             &quot;Z2&quot;: Z2,
             &quot;A2&quot;: A2}
    
    return A2, cache
</code></pre>
</li>
</ul>
<p>æµ‹è¯•ä»£ç ï¼š</p>
<pre><code>X_assess, parameters = forward_propagation_test_case()

A2, cache = forward_propagation(X_assess, parameters)

# Note: we use the mean here just to make sure that your output matches ours. 
print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
</code></pre>
<pre><code>-0.0004997557777419913 -0.000496963353231779 0.00043818745095914653 0.500109546852431
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: -0.0004997557777419913  -0.000496963353231779 0.00043818745095914653        0.500109546852431</p>
<p>ç°åœ¨ï¼Œä½ å·²ç»è®¡ç®—äº†åŒ…å«æ¯ä¸ªç¤ºä¾‹çš„a<a href="i">2</a>Â çš„Â A[2]Â ï¼ˆåœ¨Pythonå˜é‡â€œ<code>A2</code>â€ä¸­ï¼‰ï¼Œå…¶ä¸­ï¼Œä½ å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•° å¦‚ä¸‹ï¼š</p>
<figure data-type="image" tabindex="5"><img src="https://s-hmily.github.io/post-images/1595296602244.png" alt="" loading="lazy"></figure>
<p><strong>ç›®çš„</strong>ï¼šå®ç°<code>compute_costï¼ˆï¼‰</code>ä»¥è®¡ç®—æŸå¤±Jçš„å€¼ã€‚</p>
<p>æœ‰å¾ˆå¤šç§æ–¹æ³•å¯ä»¥å®ç°äº¤å‰ç†µæŸå¤±ã€‚ æˆ‘ä»¬ä¸ºä½ æä¾›äº†å®ç°æ–¹æ³• ï¼š</p>
<figure data-type="image" tabindex="6"><img src="https://s-hmily.github.io/post-images/1595296616699.png" alt="" loading="lazy"></figure>
<pre><code>logprobs = np.multiply(np.log(A2),Y)
cost = - np.sum(logprobs)   
</code></pre>
<p>ï¼ˆä½ ä¹Ÿå¯ä»¥ä½¿ç”¨np.multiply()ç„¶åä½¿ç”¨np.sum()æˆ–ç›´æ¥ä½¿ç”¨np.dot()ï¼‰ã€‚</p>
<pre><code># GRADED FUNCTION: compute_cost

def compute_cost(A2, Y, parameters):
    &quot;&quot;&quot;
    Computes the cross-entropy cost given in equation (13)
    
    Arguments:
    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    Y -- &quot;true&quot; labels vector of shape (1, number of examples)
    parameters -- python dictionary containing your parameters W1, b1, W2 and b2
    
    Returns:
    cost -- cross-entropy cost given equation (13)
    &quot;&quot;&quot;
    
    m = Y.shape[1] # number of example

    # Compute the cross-entropy cost
     ### START CODE HERE ### (â‰ˆ 2 lines of code)
    logprobs = Y*np.log(A2) + (1-Y)* np.log(1-A2)
    cost = -1/m * np.sum(logprobs)
    ### END CODE HERE ###
    
    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. 
                                # E.g., turns [[17]] into 17 
    assert(isinstance(cost, float))
    
    return cost
</code></pre>
<p><strong>æµ‹è¯•</strong>ï¼š</p>
<pre><code>A2, Y_assess, parameters = compute_cost_test_case()

print(&quot;cost = &quot; + str(compute_cost(A2, Y_assess, parameters)))
</code></pre>
<pre><code>cost = 0.6929198937761265
</code></pre>
<p>ç°åœ¨ï¼Œé€šè¿‡ä½¿ç”¨åœ¨æ­£å‘ä¼ æ’­æœŸé—´è®¡ç®—çš„ç¼“å­˜ï¼Œä½ å¯ä»¥å®ç°åå‘ä¼ æ’­ã€‚</p>
<p><strong>é—®é¢˜</strong>ï¼šå®ç°å‡½æ•°<code>backward_propagationï¼ˆï¼‰</code>ã€‚</p>
<p><strong>è¯´æ˜</strong>ï¼š åå‘ä¼ æ’­é€šå¸¸æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€éš¾ï¼ˆæœ€æ•°å­¦ï¼‰çš„éƒ¨åˆ†ã€‚ä¸ºäº†å¸®åŠ©ä½ æ›´å¥½åœ°äº†è§£ï¼Œæˆ‘ä»¬æä¾›äº†åå‘ä¼ æ’­è¯¾ç¨‹çš„å¹»ç¯ç‰‡ã€‚ä½ å°†è¦ä½¿ç”¨æ­¤å¹»ç¯ç‰‡å³ä¾§çš„å…­ä¸ªæ–¹ç¨‹å¼ä»¥æ„å»ºå‘é‡åŒ–å®ç°ã€‚</p>
<figure data-type="image" tabindex="7"><img src="https://cdn.kesci.com/upload/image/q17hcd4yra.png?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<figure data-type="image" tabindex="8"><img src="https://s-hmily.github.io/post-images/1595296633924.png" alt="" loading="lazy"></figure>
<pre><code># GRADED FUNCTION: backward_propagation

def backward_propagation(parameters, cache, X, Y):
    &quot;&quot;&quot;
    Implement the backward propagation using the instructions above.
    
    Arguments:
    parameters -- python dictionary containing our parameters 
    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;.
    X -- input data of shape (2, number of examples)
    Y -- &quot;true&quot; labels vector of shape (1, number of examples)
    
    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    &quot;&quot;&quot;
    m = X.shape[1]
    
    # First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    W1 = parameters[&quot;W1&quot;]
    W2 = parameters[&quot;W2&quot;]
    ### END CODE HERE ###
        
    # Retrieve also A1 and A2 from dictionary &quot;cache&quot;.
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    A1 = cache[&quot;A1&quot;]
    A2 = cache[&quot;A2&quot;]
    ### END CODE HERE ###
    
    # Backward propagation: calculate dW1, db1, dW2, db2. 
    ### START CODE HERE ### (â‰ˆ 6 lines of code, corresponding to 6 equations on slide above)
    dZ2= A2 - Y
    dW2 = 1 / m * np.dot(dZ2,A1.T)
    db2 = 1 / m * np.sum(dZ2,axis=1,keepdims=True)
    dZ1 = np.dot(W2.T,dZ2) * (1-np.power(A1,2))
    dW1 = 1 / m * np.dot(dZ1,X.T)
    db1 = 1 / m * np.sum(dZ1,axis=1,keepdims=True)
    ### END CODE HERE ###
    
    grads = {&quot;dW1&quot;: dW1,
             &quot;db1&quot;: db1,
             &quot;dW2&quot;: dW2,
             &quot;db2&quot;: db2}
    
    return grads
</code></pre>
<p>In [16]:</p>
<pre><code>parameters, cache, X_assess, Y_assess = backward_propagation_test_case()

grads = backward_propagation(parameters, cache, X_assess, Y_assess)
print (&quot;dW1 = &quot;+ str(grads[&quot;dW1&quot;]))
print (&quot;db1 = &quot;+ str(grads[&quot;db1&quot;]))
print (&quot;dW2 = &quot;+ str(grads[&quot;dW2&quot;]))
print (&quot;db2 = &quot;+ str(grads[&quot;db2&quot;]))
</code></pre>
<pre><code>dW1 = [[ 0.01018708 -0.00708701]
 [ 0.00873447 -0.0060768 ]
 [-0.00530847  0.00369379]
 [-0.02206365  0.01535126]]
db1 = [[-0.00069728]
 [-0.00060606]
 [ 0.000364  ]
 [ 0.00151207]]
dW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]
db2 = [[0.06589489]]
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: dW1 = [[ 0.01018708 -0.00708701] [ 0.00873447 -0.0060768 ] [-0.00530847 0.00369379] [-0.02206365 0.01535126]] db1 = [[-0.00069728] [-0.00060606] [ 0.000364 ] [ 0.00151207]] dW2 = [[ 0.00363613 0.03153604 0.01162914 -0.01318316]] db2 = [[0.06589489]]</p>
<p><strong>é—®é¢˜</strong>ï¼šå®ç°å‚æ•°æ›´æ–°ã€‚ ä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼Œä½ å¿…é¡»ä½¿ç”¨ï¼ˆdW1ï¼Œdb1ï¼ŒdW2ï¼Œdb2ï¼‰æ‰èƒ½æ›´æ–°ï¼ˆW1ï¼Œb1ï¼ŒW2ï¼Œb2ï¼‰ã€‚</p>
<p><strong>ä¸€èˆ¬çš„æ¢¯åº¦ä¸‹é™è§„åˆ™</strong>ï¼šÎ¸=Î¸âˆ’Î±âˆ‚Jâˆ‚Î¸å…¶ä¸­Î±æ˜¯å­¦ä¹ ç‡ï¼Œè€ŒÎ¸ ä»£è¡¨ä¸€ä¸ªå‚æ•°ã€‚</p>
<p><strong>å›¾ç¤º</strong>ï¼šå…·æœ‰è‰¯å¥½çš„å­¦ä¹ é€Ÿç‡ï¼ˆæ”¶æ•›ï¼‰å’Œè¾ƒå·®çš„å­¦ä¹ é€Ÿç‡ï¼ˆå‘æ•£ï¼‰çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚ å›¾ç‰‡ç”±Adam Harleyæä¾›ã€‚</p>
<figure data-type="image" tabindex="9"><img src="https://cdn.kesci.com/upload/image/q17hh4otzu.gif?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<figure data-type="image" tabindex="10"><img src="https://cdn.kesci.com/upload/image/q17hharbth.gif?imageView2/0/w/960/h/960" alt="Image Name" loading="lazy"></figure>
<pre><code># GRADED FUNCTION: update_parameters

def update_parameters(parameters, grads, learning_rate = 1.2):
    &quot;&quot;&quot;
    Updates parameters using the gradient descent update rule given above
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients 
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    &quot;&quot;&quot;
    # Retrieve each parameter from the dictionary &quot;parameters&quot;
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Retrieve each gradient from the dictionary &quot;grads&quot;
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    dW1 = grads[&quot;dW1&quot;]
    db1 = grads[&quot;db1&quot;]
    dW2 = grads[&quot;dW2&quot;]
    db2 = grads[&quot;db2&quot;]
    ## END CODE HERE ###
    
    # Update rule for each parameter
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    ### END CODE HERE ###
    
    parameters = {&quot;W1&quot;: W1,
                  &quot;b1&quot;: b1,
                  &quot;W2&quot;: W2,
                  &quot;b2&quot;: b2}
    
    return parameters
</code></pre>
<p>æµ‹è¯•ï¼š</p>
<pre><code>parameters, grads = update_parameters_test_case()
parameters = update_parameters(parameters, grads)

print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>ç»“æœï¼š
W1 = [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]
b1 = [[-1.02420756e-06]
 [ 1.27373948e-05]
 [ 8.32996807e-07]
 [-3.20136836e-06]]
W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]
b2 = [[0.00010457]]
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: dW1 = [[ 0.01018708 -0.00708701] [ 0.00873447 -0.0060768 ] [-0.00530847 0.00369379] [-0.02206365 0.01535126]] db1 = [[-0.00069728] [-0.00060606] [ 0.000364 ] [ 0.00151207]] dW2 = [[ 0.00363613 0.03153604 0.01162914 -0.01318316]] db2 = [[0.06589489]]</p>
<h4 id="334-åœ¨nn_modelä¸­é›†æˆ331-332å’Œ333éƒ¨åˆ†ä¸­çš„å‡½æ•°">3.3.4- åœ¨nn_modelï¼ˆï¼‰ä¸­é›†æˆ3.3.1ã€3.3.2å’Œ3.3.3éƒ¨åˆ†ä¸­çš„å‡½æ•°</h4>
<p><strong>é—®é¢˜</strong>ï¼šåœ¨nn_modelï¼ˆï¼‰ä¸­å»ºç«‹ä½ çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚</p>
<p><strong>è¯´æ˜</strong>ï¼šç¥ç»ç½‘ç»œæ¨¡å‹å¿…é¡»ä»¥æ­£ç¡®çš„é¡ºåºç»„åˆå…ˆå‰æ„å»ºçš„å‡½æ•°ã€‚</p>
<p>In [19]:</p>
<pre><code># GRADED FUNCTION: nn_model

def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    &quot;&quot;&quot;
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    &quot;&quot;&quot;
    
    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.
    ### START CODE HERE ### (â‰ˆ 5 lines of code)
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters[&quot;W1&quot;]
    b1 = parameters[&quot;b1&quot;]
    W2 = parameters[&quot;W2&quot;]
    b2 = parameters[&quot;b2&quot;]
    ### END CODE HERE ###
    
    # Loop (gradient descent)

    for i in range(0, num_iterations):
         
        ### START CODE HERE ### (â‰ˆ 4 lines of code)
        # Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.
        A2, cache = forward_propagation(X, parameters)
        
        # Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.
        cost = compute_cost(A2, Y, parameters)
 
        # Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.
        grads = backward_propagation(parameters, cache, X, Y)
 
        # Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.
        parameters = update_parameters(parameters, grads)
        
        ### END CODE HERE ###
        
        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))

    return parameters
</code></pre>
<p>æµ‹è¯•</p>
<pre><code>X_assess, Y_assess = nn_model_test_case()

parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=False)
print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))
print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))
print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))
print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))
</code></pre>
<pre><code>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log
/home/kesci/work/planar_utils.py:34: RuntimeWarning: overflow encountered in exp
  s = 1/(1+np.exp(-x))
</code></pre>
<pre><code>ç»“æœï¼š
W1 = [[-4.18503197  5.33214315]
 [-7.52988635  1.24306559]
 [-4.19302427  5.32627154]
 [ 7.52984762 -1.24308746]]
b1 = [[ 2.32926944]
 [ 3.79460252]
 [ 2.33002498]
 [-3.79466751]]
W2 = [[-6033.83668723 -6008.12983227 -6033.10091631  6008.06624417]]
b2 = [[-52.66610924]]
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: W1 = [[-4.18503197 5.33214315] [-7.52988635 1.24306559] [-4.19302427 5.32627154] [ 7.52984762 -1.24308746]] b1 = [[ 2.32926944] [ 3.79460252] [ 2.33002498] [-3.79466751]] W2 = [[-6033.83668723 -6008.12983227 -6033.10091631 6008.06624417]] b2 = [[-52.66610924]]</p>
<h3 id="45-é¢„æµ‹">4.5- é¢„æµ‹</h3>
<p><strong>é—®é¢˜</strong>ï¼šä½¿ç”¨ä½ çš„æ¨¡å‹é€šè¿‡æ„å»ºpredict()å‡½æ•°è¿›è¡Œé¢„æµ‹ã€‚ ä½¿ç”¨æ­£å‘ä¼ æ’­æ¥é¢„æµ‹ç»“æœã€‚</p>
<p><strong>æç¤º</strong>ï¼špredictions = yprediction=1{activation &gt; 0.5}={1if activation&gt;0.50otherwise<br>
ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³åŸºäºé˜ˆå€¼å°†çŸ©é˜µXè®¾ä¸º0å’Œ1ï¼Œåˆ™å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š <code>X_new = (X &gt; threshold)</code></p>
<pre><code># GRADED FUNCTION: predict

def predict(parameters, X):
    &quot;&quot;&quot;
    Using the learned parameters, predicts a class for each example in X
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    X -- input data of size (n_x, m)
    
    Returns
    predictions -- vector of predictions of our model (red: 0 / blue: 1)
    &quot;&quot;&quot;
    
    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.
  ### START CODE HERE ### (â‰ˆ 2 lines of code)
    A2, cache = forward_propagation(X, parameters)
    predictions = np.round(A2)
    ### END CODE HERE ###
    
    return predictions
</code></pre>
<p>æµ‹è¯•ï¼š</p>
<pre><code>parameters, X_assess = predict_test_case()

predictions = predict(parameters, X_assess)
print(&quot;predictions mean = &quot; + str(np.mean(predictions)))
</code></pre>
<pre><code>predictions mean = 0.6666666666666666
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: predictions mean = 0.6666666666666666</p>
<p>ç°åœ¨è¿è¡Œæ¨¡å‹ä»¥æŸ¥çœ‹å…¶å¦‚ä½•åœ¨äºŒç»´æ•°æ®é›†ä¸Šè¿è¡Œã€‚ è¿è¡Œä»¥ä¸‹ä»£ç ä»¥ä½¿ç”¨å«æœ‰nhéšè—å•å…ƒçš„å•ä¸ªéšè—å±‚æµ‹è¯•æ¨¡å‹ã€‚</p>
<pre><code># Build a model with a n_h-dimensional hidden layer
parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)

# Plot the decision boundary
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4))
</code></pre>
<pre><code>ç»“æœï¼š
Cost after iteration 0: 0.693048
Cost after iteration 1000: 0.288083
Cost after iteration 2000: 0.254385
Cost after iteration 3000: 0.233864
Cost after iteration 4000: 0.226792
Cost after iteration 5000: 0.222644
Cost after iteration 6000: 0.219731
Cost after iteration 7000: 0.217504
Cost after iteration 8000: 0.219467
Cost after iteration 9000: 0.218561
</code></pre>
<pre><code>Text(0.5, 1.0, 'Decision Boundary for hidden layer size 4')
</code></pre>
<figure data-type="image" tabindex="11"><img src="https://cdn.kesci.com/rt_upload/CB068684F93C4A2A8AE816EB492CDCBE/q17hj9pr80.png" alt="img" loading="lazy"></figure>
<p><strong>é¢„æœŸè¾“å‡º</strong>: Cost after iteration 9000: 0.218561</p>
<pre><code># Print accuracy
predictions = predict(parameters, X)
print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')
</code></pre>
<pre><code>Accuracy: 90%
</code></pre>
<p><strong>é¢„æœŸè¾“å‡º</strong>: Accuracy: 90%</p>
<p>ä¸Logisticå›å½’ç›¸æ¯”ï¼Œå‡†ç¡®æ€§ç¡®å®æ›´é«˜ã€‚ è¯¥æ¨¡å‹å­¦ä¹ äº†flowerçš„å¶å­å›¾æ¡ˆï¼ ä¸é€»è¾‘å›å½’ä¸åŒï¼Œç¥ç»ç½‘ç»œç”šè‡³èƒ½å¤Ÿå­¦ä¹ éçº¿æ€§çš„å†³ç­–è¾¹ç•Œã€‚</p>
<p>ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°è¯•å‡ ç§ä¸åŒçš„éšè—å±‚å¤§å°ã€‚</p>
<h2 id="4-è°ƒæ•´éšè—å±‚å¤§å°å¯é€‰ç»ƒä¹ ">4 è°ƒæ•´éšè—å±‚å¤§å°ï¼ˆå¯é€‰ç»ƒä¹ ï¼‰</h2>
<p>è¿è¡Œä»¥ä¸‹ä»£ç ï¼ˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼‰ï¼Œ ä½ å°†è§‚å¯Ÿåˆ°ä¸åŒå¤§å°éšè—å±‚çš„æ¨¡å‹çš„ä¸åŒè¡¨ç°ã€‚</p>
<p>In [25]:</p>
<pre><code># This may take about 2 minutes to run

plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 10, 20]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print (&quot;Accuracy for {} hidden units: {} %&quot;.format(n_h, accuracy))
</code></pre>
<pre><code>Accuracy for 1 hidden units: 67.5 %
Accuracy for 2 hidden units: 67.25 %
Accuracy for 3 hidden units: 90.75 %
Accuracy for 4 hidden units: 90.5 %
Accuracy for 5 hidden units: 91.25 %
Accuracy for 10 hidden units: 90.25 %
Accuracy for 20 hidden units: 90.5 %
</code></pre>
<figure data-type="image" tabindex="12"><img src="https://cdn.kesci.com/rt_upload/F70D5A23097642688F6245327ACE9DD7/q17hkk98sl.png" alt="img" loading="lazy"></figure>
<p>å®Œæ•´ä»£ç </p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary,sigmoid,load_planar_dataset,load_extra_datasets
#è®¾ç½®ä¸€ä¸ªå›ºå®šçš„éšæœºç§å­
np.random.seed(1)
X,Y=load_planar_dataset()
#ç”¨matplotlibå¯è§†åŒ–æ•°æ®
plt.scatter(X[0,:],X[1,:],c=np.squeeze(Y),s=40,cmap=plt.cm.Spectral)
plt.show()
shape_X=X.shape
shape_Y=Y.shape
m=Y.shape[1] #è®­ç»ƒé›†é‡Œé¢çš„æ•°æ®
print(&quot;Xçš„ç»´åº¦ä¸ºï¼š&quot;+str(shape_X))
print(&quot;Yçš„ç»´åº¦ä¸ºï¼š&quot;+str(shape_Y))
print(&quot;æ•°æ®é›†é‡Œé¢çš„æ•°æ®æœ‰ï¼š&quot;+str(m)+&quot;ä¸ª&quot;)
def layer_sizes(X,Y):
    '''å‚æ•°ï¼š
    Xï¼šè¾“å…¥æ•°æ®é›†ï¼Œç»´åº¦æ˜¯ï¼ˆè¾“å…¥çš„æ•°é‡ï¼Œè®­ç»ƒ/æµ‹è¯•çš„æ•°é‡ï¼‰
    Yï¼šæ ‡ç­¾
    è¿”å›ï¼š
    n_x:è¾“å…¥å±‚çš„æ•°é‡
    n_h:éšè—å±‚çš„æ•°é‡
    n_y:è¾“å‡ºå±‚çš„æ•°é‡'''
    n_x=X.shape[0]#è¾“å…¥å±‚
    n_h=4#éšè—å±‚ï¼Œç¡¬ç¼–ç ä¸º4
    n_y=Y.shape[0]
    return (n_x,n_h,n_y)
#æµ‹è¯•layer_sizes
print(&quot;=========================æµ‹è¯•layer_sizes=========================&quot;)
X_asses , Y_asses = layer_sizes_test_case()
(n_x,n_h,n_y) =  layer_sizes(X_asses,Y_asses)
print(&quot;è¾“å…¥å±‚çš„èŠ‚ç‚¹æ•°é‡ä¸º: n_x = &quot; + str(n_x))
print(&quot;éšè—å±‚çš„èŠ‚ç‚¹æ•°é‡ä¸º: n_h = &quot; + str(n_h))
print(&quot;è¾“å‡ºå±‚çš„èŠ‚ç‚¹æ•°é‡ä¸º: n_y = &quot; + str(n_y))
#åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°
def initialize_parameters(n_x,n_h,n_y):
    '''
    å‚æ•°ï¼š
    n_x:è¾“å…¥å±‚ç»“ç‚¹çš„æ•°é‡
    n_h:éšè—å±‚ç»“ç‚¹çš„æ•°é‡
    n_y:è¾“å‡ºå±‚ç»“ç‚¹çš„æ•°é‡
    è¿”å›ï¼š
    paraneters:åŒ…å«å‚æ•°çš„å­—å…¸
    W1:æƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º(n_h,n_x)
    b1:åå‘é‡ï¼Œç»´åº¦ä¸º(n_h,1)
    W2ï¼šæƒé‡çŸ©é˜µï¼Œç»´åº¦ä¸º(n_y,n_h)
    b2:åå‘é‡ï¼Œç»´åº¦ä¸º(n_y,1)
    '''
    #é€šè¿‡æœ¬å‡½æ•°å¯ä»¥è¿”å›ä¸€ä¸ªæˆ–ä¸€ç»„æœä»â€œ0~1â€å‡åŒ€åˆ†å¸ƒçš„éšæœºæ ·æœ¬å€¼ã€‚éšæœºæ ·æœ¬å–å€¼èŒƒå›´æ˜¯[0,1)ï¼Œä¸åŒ…æ‹¬1ã€‚
    np.random.seed(2)#æŒ‡å®šä¸€ä¸ªéšæœºç§å­
    W1=np.random.rand(n_h,n_x)*0.01
    #zeros ç”Ÿæˆ0çŸ©é˜µ
    b1=np.zeros(shape=(n_h,1))
    W2=np.random.rand(n_y,n_h)
    b2=np.random.rand(n_y,1)
    #ä½¿ç”¨æ–­è¨€ç¡®ä¿æˆ‘çš„æ•°æ®æ ¼å¼æ˜¯æ­£ç¡®çš„
    #assertæ˜¯ç”¨äºå¯¹ç¨‹åºè¿›è¡Œè°ƒè¯•çš„ï¼Œå¯¹äºæ‰§è¡Œç»“æ„çš„åˆ¤æ–­ï¼Œè€Œä¸æ˜¯å¯¹äºä¸šåŠ¡æµç¨‹çš„åˆ¤æ–­ã€‚ï¼ˆç›¸å½“äºä¸€ä¸ªif ()è¯­å¥ï¼Œå¦‚æœæ»¡è¶³æ–­è¨€çš„æ‰§è¡Œç¨‹åºï¼Œå¦‚æœä¸æ»¡è¶³åˆ™æŠ›é”™è¯¯
    assert(W1.shape==(n_h,n_x))
    assert(b1.shape==(n_h,1))
    assert(W2.shape==(n_y,n_h))
    assert(b2.shape==(n_y,1))
    parameters={
        &quot;W1&quot;:W1,
        &quot;b1&quot;:b1,
        &quot;W2&quot;:W2,
        &quot;b2&quot;:b2
    }
    return parameters
#æµ‹è¯•initialize_parameters
n_x,n_h,n_y=initialize_parameters_test_case()
parameters=initialize_parameters(n_x,n_h,n_y)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters[&quot;b2&quot;]))
#å®ç°å¾ªç¯
'''æˆ‘ä»¬ç°åœ¨è¦å®ç°å‰å‘ä¼ æ’­å‡½æ•°forward_propagation()ã€‚ 
æˆ‘ä»¬å¯ä»¥ä½¿ç”¨sigmoid()å‡½æ•°ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨np.tanh()å‡½æ•°ã€‚  
æ­¥éª¤å¦‚ä¸‹ï¼š
ä½¿ç”¨å­—å…¸ç±»å‹çš„parametersï¼ˆå®ƒæ˜¯initialize_parameters() çš„è¾“å‡ºï¼‰æ£€ç´¢æ¯ä¸ªå‚æ•°ã€‚
å®ç°å‘å‰ä¼ æ’­, è®¡ç®—Z[1],A[1],Z[2]Z[1],A[1],Z[2] å’Œ A[2]A[2]ï¼ˆ è®­ç»ƒé›†é‡Œé¢æ‰€æœ‰ä¾‹å­çš„é¢„æµ‹å‘é‡ï¼‰ã€‚
åå‘ä¼ æ’­æ‰€éœ€çš„å€¼å­˜å‚¨åœ¨â€œcacheâ€ä¸­ï¼Œcacheå°†ä½œä¸ºåå‘ä¼ æ’­å‡½æ•°çš„è¾“å…¥ã€‚
'''
def forward_propagation(X,parameters):
    '''
    å‚æ•°ï¼š
    X-ç»´åº¦ä¸º(n_x,m)çš„è¾“å…¥æ•°æ®
    parameters:åˆå§‹åŒ–å‡½æ•°(initialize_parameters)çš„è¾“å‡º
    è¿”å›:
    A2:ä½¿ç”¨æ¿€æ´»å‡½æ•°åçš„æ¿€æ´»å€¼
    cache-åŒ…å«â€œZ1&quot;,&quot;A1&quot;,&quot;Z2&quot;å’Œâ€œA2â€çš„å­—å…¸å‹å˜é‡
    '''
    W1=parameters[&quot;W1&quot;]
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    #å‘å‰ä¼ æ’­è®¡ç®—A2
    #dot è®¡ç®—çŸ©é˜µ
    Z1=np.dot(W1,X)+b1
    A1=np.tanh(Z1)
    Z2=np.dot(W2,A1)+b2
    A2=sigmoid(Z2)
    #ä½¿ç”¨æ–­è¨€ä¿è¯æˆ‘çš„æ•°æ®æ ¼å¼æ˜¯æ­£ç¡®çš„
    assert(A2.shape==(1,X.shape[1]))
    cache={&quot;Z1&quot;:Z1,
           &quot;A1&quot;:A1,
           &quot;Z2&quot;:Z2,
           &quot;A2&quot;:A2
    }
    return (A2,cache)
#æµ‹è¯•æ¨¡å‹
X_assess,parameters=forward_propagation_test_case()
A2,cache=forward_propagation(X_assess,parameters)
print(np.mean(cache[&quot;Z1&quot;]),np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
#è®¡ç®—æˆæœ¬å‡½æ•°ä¸æŸå¤±å‡½æ•°
 
def compute_cost(A2,Y,parameters):
    '''è®¡ç®—äº¤å‰ç†µ
    å‚æ•°ï¼š
    A2ï¼šä½¿ç”¨sigmoidå‡½æ•°è®¡ç®—çš„ç¬¬äºŒæ¬¡æ¿€æ´»åçš„å‡½æ•°å€¼
    Y:Trueæ ‡ç­¾å‘é‡ï¼Œç»´åº¦ä¸º(1,æ•°é‡)
    parameters:ä¸€ä¸ªåŒ…å«W1ï¼ŒW2ï¼Œb1,b2çš„å­—å…¸ç±»å‹çš„å˜é‡
    è¿”å›ï¼šæˆæœ¬äº¤å‰ç†µå‡½æ•°ï¼Œç»™å‡ºæ–¹ç¨‹
    '''
    m=Y.shape[1]
    W1=parameters['W1']
    W2=parameters['W2']
    #è®¡ç®—æˆæœ¬
    logprobs=logprobs=np.multiply(np.log(A2),Y)+np.multiply((1-Y),np.log(1-A2))
    cost=-np.sum(logprobs)/m
    cost=float(np.squeeze(cost))
    assert(isinstance(cost,float))
    return cost
#é¢„æµ‹compute_cost
A2,Y_assess,parameters=compute_cost_test_case()
print(&quot;cost=&quot;+str(compute_cost(A2,Y_assess,parameters)))
#æ­å»ºåå‘ä¼ æ’­å‡½æ•°
def backward_propagation(parameters,cache,X,Y):
    '''å‚æ•°ï¼š
    parameters:åŒ…å«å‚æ•°çš„ä¸€ä¸ªå­—å…¸å˜é‡
    cache:åŒ…å«Z1,A1,Z2,A2çš„å­—å…¸ç±»å‹çš„å˜é‡
    X:è¾“å…¥æ•°æ®ï¼Œç»´åº¦ä¸º(2,æ•°é‡)
    Y:è¾“å‡ºæ•°æ®ï¼Œå”¯ç‹¬ä¸º(1,æ•°é‡)
    è¿”å›:
    grads:åŒ…å«Wå’Œbçš„å¯¼æ•°çš„ä¸€ä¸ªå­—å…¸å˜é‡'''
    m=X.shape[1]
    W1=parameters['W1']
    W2=parameters['W2']
    A1=cache['A1']
    A2=cache['A2']
    dZ2=A2-Y
    dW2=(1/m)*np.dot(dZ2,A1.T)
    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)
    dZ1=np.multiply(np.dot(W2.T,dZ2),1-np.power(A1,2))
    dW1 = (1 / m) * np.dot(dZ1, X.T)
    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)
    grads={'dW1':dW1,
           'db1':db1,
           'dW2':dW2,
           'db2':db2}
    return grads
#æ›´æ–°å‚æ•°
def update_parameters(parameters,grads,learning_rate=1.2):
    '''å‚æ•°ï¼š
    parameters:åŒ…å«å‚æ•°çš„å­—å…¸ç±»å‹æ•°æ®çš„å˜é‡
    grads:åŒ…å«å¯¼æ•°å€¼å¾—å­—å…¸ç±»å‹å˜é‡
    learning_rate:å­¦ä¹ é€Ÿç‡
    è¿”å›ï¼š
    parameters:åŒ…å«æ›´æ–°å‚æ•°çš„å­—å…¸ç±»å‹å’Œå˜é‡'''
    W1,W2=parameters['W1'],parameters['W2']
    b1,b2=parameters['b1'],parameters['b2']
    dW1,dW2=grads['dW1'],grads['dW2']
    db1,db2=grads['db1'],grads['db2']
    W1=W1-learning_rate*dW1
    b1=b1-learning_rate*db1
    W2=W2-learning_rate*dW2
    b2=b2-learning_rate*db2
    parameters={&quot;W1&quot;:W1,
                &quot;b1&quot;:b1,
                &quot;W2&quot;:W2,
                &quot;b2&quot;:b2}
    return parameters
#æµ‹è¯•update_parameters
parameters,grads=update_parameters_test_case()
parameters=update_parameters(parameters,grads)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters['b2']))
#æŠŠä¸Šè¿°å†…å®¹æ•´åˆåˆ°nn_model()ä¸­ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹å¿…é¡»ä»¥æ­£ç¡®çš„é¡ºåºä½¿ç”¨å…ˆå‰çš„åŠŸèƒ½
def nn_model(X,Y,n_h,num_iterations,print_cost=False):
    '''å‚æ•°ï¼š
    X-æ•°æ®é›†ï¼Œç»´åº¦ä¸º(2,ç¤ºä¾‹æ•°)
    Y-æ ‡ç­¾,ç»´åº¦ä¸ºï¼ˆ1ï¼Œç¤ºä¾‹æ•°ï¼‰
    n_h:éšè—å±‚çš„æ•°é‡
    num_iterations:æ¢¯åº¦ä¸‹é™å¾ªç¯ä¸­çš„è¿­ä»£æ¬¡æ•°
    print_cost:å¦‚æœä¸ºTrue,åˆ™æ¯100æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡æˆæœ¬æ•°å€¼
    è¿”å›:
    parameters:æ¨¡å‹å­¦ä¹ çš„å‚æ•°ï¼Œå®ƒä»¬å¯ä»¥ç”¨æ¥é¢„æµ‹
    '''
    np.random.seed(3)#æŒ‡å®šéšæœºç§å­
    n_x=layer_sizes(X,Y)[0]
    n_y=layer_sizes(X,Y)[2]
    parameters=initialize_parameters(n_x,n_h,n_y)
    W1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    for i in range(num_iterations):
        A2,cache=forward_propagation(X,parameters)
        cost=compute_cost(A2,Y,parameters)
        grads=backward_propagation(parameters,cache,X,Y)
        parameters=update_parameters(parameters,grads,learning_rate=0.5)
        if print_cost:
            if i%1000==0:
                print(&quot;ç¬¬&quot;,i,&quot;æ¬¡å¾ªç¯&quot;,&quot;æˆæœ¬ä¸º:&quot;+str(cost))
    return parameters
#æµ‹è¯•nn_model
X_assess,Y_assess=nn_model_test_case()
parameters=nn_model(X_assess,Y_assess,4,num_iterations=10000,print_cost=False)
print(&quot;W1=&quot;+str(parameters[&quot;W1&quot;]))
print(&quot;b1=&quot;+str(parameters[&quot;b1&quot;]))
print(&quot;W2=&quot;+str(parameters[&quot;W2&quot;]))
print(&quot;b2=&quot;+str(parameters[&quot;b2&quot;]))
#predictæ¨¡å‹
'''æ¿€æ´»å€¼å¤§äº0.5ï¼Œé¢„æµ‹å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0'''
def predict(parameters,X):
    '''ä½¿ç”¨å­¦ä¹ çš„å‚æ•°ä¸ºXåˆ†ç±»
    å‚æ•°ï¼š
    parameters:åŒ…å«å‚æ•°çš„å­—å…¸ç±»å‹çš„å˜é‡
    X:è¾“å…¥æ•°æ®
    è¿”å›ï¼š
    predictions:æˆ‘ä»¬æ¨¡å‹é¢„æµ‹çš„å‘é‡ï¼ˆçº¢è‰²ï¼š0/è“è‰²ï¼š1ï¼‰
    '''
    A2,cache=forward_propagation(X,parameters)
    predictions=np.round(A2)
    return predictions
#æµ‹è¯•predict
parameters,X_assess=predict_test_case()
predictions=predict(parameters,X_assess)
print(&quot;é¢„æµ‹çš„å¹³å‡å€¼=&quot;+str(np.mean(predictions)))
parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)
 
#ç»˜åˆ¶è¾¹ç•Œ
'''æ³¨ï¼šæ•°ç»„çš„å¯è§†åŒ–
é€šå¸¸ç®—æ³•çš„ç»“æœæ˜¯å¯ä»¥è¡¨ç¤ºå‘é‡çš„æ•°ç»„ï¼Œç›´æ¥åˆ©ç”¨æ•°ç»„ç”»å›¾æ—¶ç•Œé¢ä¸ºç©º
åˆ©ç”¨sequeeze()å‡½æ•°è½¬åŒ–ä¸ºç§©ä¸º1çš„æ•°ç»„å¯ä»¥æ­£å¸¸ç”»å›¾'''
plot_decision_boundary(lambda x: predict(parameters, x.T), X, np.squeeze(Y))
plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4))
plt.show()
predictions = predict(parameters, X)
print ('å‡†ç¡®ç‡: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')
#æ›´æ”¹éšè—ç»“ç‚¹çš„æ•°é‡
plt.figure(figsize=(16,32))
hidden_layer_sizes=[1,2,3,4,5,20,50]#éšè—å±‚æ•°é‡
for i,n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5,2,i+1)
    plt.title('Hidden Layer of Size%d'% n_h)
    parameters=nn_model(X,Y,n_h,num_iterations=5000)
    plot_decision_boundary(lambda x:predict(parameters,x.T),X,np.squeeze(Y))
    predictions=predict(parameters,X)
    accuracy=float((np.dot(Y,predictions.T)+np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print(&quot;éšè—ç»“ç‚¹çš„æ•°é‡ï¼š{}ï¼Œå‡†ç¡®ç‡ï¼š{}%&quot;.format(n_h,accuracy))
</code></pre>
<p><strong>è¯´æ˜</strong>ï¼š</p>
<ul>
<li>
<p>è¾ƒå¤§çš„æ¨¡å‹ï¼ˆå…·æœ‰æ›´å¤šéšè—çš„å•å…ƒï¼‰èƒ½å¤Ÿæ›´å¥½åœ°æ‹Ÿåˆè®­ç»ƒé›†ï¼Œç›´åˆ°æœ€ç»ˆæœ€å¤§çš„æ¨¡å‹è¿‡æ‹Ÿåˆæ•°æ®ä¸ºæ­¢ã€‚</p>
</li>
<li>
<p>éšè—å±‚çš„æœ€ä½³å¤§å°ä¼¼ä¹åœ¨n_h = 5å·¦å³ã€‚çš„ç¡®ï¼Œæ­¤å€¼ä¼¼ä¹å¾ˆå¥½åœ°æ‹Ÿåˆäº†æ•°æ®ï¼Œè€Œåˆä¸ä¼šå¼•èµ·æ˜æ˜¾çš„è¿‡åº¦æ‹Ÿåˆã€‚</p>
</li>
<li>
<p>ç¨åä½ è¿˜å°†å­¦ä¹ æ­£åˆ™åŒ–ï¼Œå¸®åŠ©æ„å»ºæ›´å¤§çš„æ¨¡å‹ï¼ˆä¾‹å¦‚n_h = 50ï¼‰è€Œä¸ä¼šè¿‡åº¦æ‹Ÿåˆã€‚</p>
<p>å‚è€ƒæ–‡ç« ï¼š</p>
<p><a href="https://www.kesci.com/home/project/5dd3946900b0b900365f3a48/code">æ–‡ç« ä¸€</a></p>
<p><a href="https://www.cnblogs.com/charlotte77/p/5629865.html">æ–‡ç« äºŒ</a></p>
<p><a href="https://www.cnblogs.com/maybe2030/p/5597716.html">æ–‡ç« ä¸‰</a></p>
<p>æ•°æ®åŒ…åœ¨æ–‡ç« ä¸€ä¸­ å¯ä»¥ç›´æ¥ä¸‹è½½ä½¿ç”¨</p>
</li>
</ul>

        </div>
        
        <div class="prev-post">
            ä¸Šä¸€ç¯‡
            <a href="https://s-hmily.github.io/post/python-chang-yong-han-shu-jie-xi/">
                pythonå¸¸ç”¨å‡½æ•°è§£æ
            </a>
        </div>
        
        
        <div class="next-post">
            ä¸‹ä¸€ç¯‡
            <a href="https://s-hmily.github.io/post/sigmoid-han-shu/">
                sigmoidå‡½æ•°
            </a>
        </div>
        
    </div>
    </div>
</body>
<script>
    var t_img; // å®šæ—¶ï¿½?
    var isLoad = true; // æ§åˆ¶å˜é‡
    isImgLoad(function () {
        // åŠ è½½å®Œæˆ
        $('.postdetailimg').css("display", "block");
    });
    // åˆ¤æ–­å›¾ç‰‡åŠ è½½çš„å‡½ï¿½?
    function isImgLoad(callback) {
        // æ³¨æ„æˆ‘çš„å›¾ç‰‡ç±»åéƒ½æ˜¯coverï¼Œå› ä¸ºæˆ‘ï¿½?éœ€è¦ï¿½?ï¿½ç†coverã€‚å…¶å®ƒå›¾ç‰‡å¯ä»¥ä¸ç®¡ï¿½?
        // æŸ¥æ‰¾æ‰€æœ‰å°é¢å›¾ï¼Œè¿­ä»£ï¿½?ï¿½ç†
        $('.postdetailimg').each(function () {
            // æ‰¾åˆ°ï¿½?0å°±å°†isLoadè®¾ä¸ºfalseï¼Œå¹¶é€€å‡ºeach
            if (this.height === 0) {
                isLoad = false;
                return false;
            }
        });
        // ä¸ºtrueï¼Œæ²¡æœ‰å‘ç°ä¸º0çš„ã€‚åŠ è½½å®Œï¿½?
        if (isLoad) {
            clearTimeout(t_img); // æ¸…é™¤å®šæ—¶ï¿½?
            // å›è°ƒå‡½æ•°
            callback();
            // ä¸ºfalseï¼Œå› ä¸ºæ‰¾åˆ°äº†æ²¡æœ‰åŠ è½½å®Œæˆçš„å›¾ï¼Œå°†è°ƒç”¨å®šæ—¶å™¨é€’å½’
        } else {
            isLoad = true;
            t_img = setTimeout(function () {
                isImgLoad(callback); // é€’å½’ï¿½?ï¿½?
            }, 500); // æˆ‘è¿™é‡Œï¿½?ï¿½ç½®çš„æ˜¯500ï¿½?ç§’å°±ï¿½?æä¸€æ¬¡ï¼Œï¿½?ä»¥è‡ªå·±è°ƒï¿½?
        }
    }

    //æ–‡ç« é˜…è¯»çƒ­åº¦
    var pl = $("#pl").html();
    var rootaddr = $("#rootaddr").html();
    pl = pl.replace(rootaddr, "");
    $("#hotnum").attr('id', pl);
</script>
    <div name="comment" style="background: white;margin-top:100px">
        <div class="commentcontainer">
            
            <p>è¯·åˆ°å®¢æˆ·ç«¯â€œä¸»é¢˜--è‡ªå®šä¹‰é…ç½®--valineâ€ä¸­å¡«å…¥IDå’ŒKEY</p>
            
        </div>
    </div>
    </div>
    <div id="landlord">
        <div class="message" style="opacity:0"></div>
        <canvas id="live2d" width="240" height="250" class="live2d"></canvas>
    </div>
    <div id="codeCopyText" style="display: none">ä»£ç å¤åˆ¶æˆåŠŸäº†å“¦</div>
    <div id="domainname" style="display:none">https://s-hmily.github.io</div>
    </body>
    <script src="https://s-hmily.github.io/media/js/post.js"></script>
    <script>
        //å¯»æ‰¾æ‰€æœ‰codeæ ‡ç­¾ï¼ŒåŠ å¤åˆ¶æŒ‰é’®é¸­ï¼(è¡Œå†…ä»£ç é™¤å¤–)
        var codes = document.getElementsByTagName('code');
        if (codes.length) {
            for (var i = 0; i < codes.length; i++) {
                //é«˜åº¦/è¡Œé«˜=æ–‡æœ¬è¡Œæ•°
                // var rowNum=Math.round(codes[i].height()/parseFloat(codes[i].css('line-height')));
                // console.log("å½“å‰æœ‰"+rowNum+"è¡Œ");
                var code_id = "code_id_" + i;
                codes[i].setAttribute("id", code_id);
                var ci = "#" + code_id;
                var codedot = $(ci);
                var rowNum = Math.round(codedot.height() / parseFloat(codedot.css('line-height')));
                if (rowNum <= 1) continue;
                var btn = document.createElement("button");
                btn.setAttribute("class", "copybt");
                btn.setAttribute("data-clipboard-target", "#" + code_id);
                btn.innerHTML = 'å¤åˆ¶ä»£ç ';
                codes[i].parentNode.insertBefore(btn, codes[i]);
            }
        };
        var cop = new ClipboardJS('.copybt');
        var codeCopyText = $("#codeCopyText").html();
        cop.on('success', function (e) {
            alert(codeCopyText);
            e.clearSelection();
        });
        cop.on('error', function (e) {
            alert("çŸ®æ²¹ï¼Œå¤åˆ¶å¤±è´¥äº†...æ‰‹åŠ¨å¤åˆ¶å§å‹‡å£«ï¼");
            e.clearSelection();
        });
    </script>
    
    <script type="text/javascript">
        var message_Path = '/live2d/'
        var home_Path = document.getElementById("domainname").innerHTML+"/"; //æ­¤å¤„ä¿®æ”¹ä¸ºä½ çš„åŸŸåï¼Œå¿…é¡»å¸¦æ–œæ 
    </script>
    <script type="text/javascript" src="https://s-hmily.github.io/media/live2d/js/live2d.js"></script>
    <script type="text/javascript" src="https://s-hmily.github.io/media/live2d/js/message.js"></script>
    <script type="text/javascript">
        loadlive2d("live2d", "https://s-hmily.github.io/media/live2d/assets/tororo.model.json");
    </script>
    
<script>
$(function () {
    $('.toggleContainer').click(function(){$('html,body').animate({scrollTop: '0px'}, 800);});
	$(window).scroll(function() {
        var st = $(window).scrollTop();
        if(st > 30){
            $(".toggleContainer").fadeIn(400);
        }else{
            $(".toggleContainer").fadeOut(100);
        }
	});
});
</script>

<script>
        var bgchoice=$('#bgchoice').html();
        var bg = $('#bg');
        var bgurl = document.getElementById("bgurl").innerHTML;
        if(bgchoice=='default')
            for (var i = 0; i < 3; i++)
                bgurl = bgurl.replace("\\", "/");
        bg.css("background", "url('" + bgurl + "')");
</script>
